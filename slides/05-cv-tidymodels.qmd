---
title: "Chapter 5 and tidymodels"
format: 
  kakashi-revealjs:
    theme: [custom.scss]
    slide-number: true
    chalkboard: true
---

```{r child = "setup.Rmd"}
```

## Setup

```{r}
#| label: load-packages
#| echo: true
library(tidyverse)
library(ISLR)
library(tidymodels)
library(gridExtra)
```

## Cross validation
### `r emo::ji("bulb")` Big idea

::: nonincremental
* We have determined that it is sensible to use a _test_ set to calculate metrics like prediction error
:::

## Cross validation
### `r emo::ji("bulb")` Big idea

::: nonincremental
* We have determined that it is sensible to use a _test_ set to calculate metrics like prediction error
:::

::: question
Why?
:::


## Cross validation 
### `r emo::ji("bulb")` Big idea

::: nonincremental
* We have determined that it is sensible to use a _test_ set to calculate metrics like prediction error
:::

::: question
How could we do this?
:::



## Cross validation
### `r emo::ji("bulb")` Big idea

::: nonincremental
* We have determined that it is sensible to use a _test_ set to calculate metrics like prediction error
* What if we don't have a separate data set to test our model on?
* `r emo::ji("tada")` We can use **resampling** methods to **estimate** the test-set prediction error
:::


## Training error versus test error {.small}

::: question
What is the difference? Which is typically larger?
:::

 * The **training error** is calculated by using the same observations used to fit the statistical learning model
 * The **test error** is calculated by using a statistical learning method to predict the response of **new** observations
 * The **training error rate** typically _underestimates_ the true prediction error rate

##

![](img/05/model-complexity.png)

## Estimating prediction error {.small}

 * Best case scenario: We have a large data set to test our model on 
 * This is not always the case!

. . .

 `r emo::ji("bulb")` Let's instead find a way to estimate the test error by holding out a subset of the training observations from the model fitting process, and then applying the statistical learning method to those held out observations



## Approach #1: Validation set  {.small}

 * Randomly divide the available set up samples into two parts: a **training set** and a **validation set**
 * Fit the model on the **training set**, calculate the prediction error on the **validation set**

. . .

::: question
If we have a **quantitative predictor** what metric would we use to calculate this test error?
:::


* Often we use Mean Squared Error (MSE)


## Approach #1: Validation set {.small}

::: nonincremental
* Randomly divide the available set up samples into two parts: a **training set** and a **validation set**
* Fit the model on the **training set**, calculate the prediction error on the **validation set**

:::

::: question
If we have a **qualitative predictor** what metric would we use to calculate this test error?
:::


* Often we use misclassification rate



## Approach #1: Validation set {.small}

```{r, echo = FALSE}
plot_split <- function(data = sample(c(rep("Training", 10), rep("Testing", 10)))) {
    s_plot <-
        tibble(
          Row = 1:20,
          Data = data
        ) |> 
        ggplot(aes(x = Row, y = 1, fill = Data)) + 
        geom_tile(color = "white",
                  size = 1) + 
        scale_fill_manual(values = c("orange", "cornflower blue"),
                          guide = FALSE) +
        theme_void() +
        coord_equal()
    
    s_plot
}
```

```{r echo=FALSE, fig.width = 10, fig.height = .5, fig.asp = 0.1}
plot_split()
```

. . .

$$\Large\color{orange}{MSE_{\texttt{test-split}} = \textrm{Ave}_{i\in\texttt{test-split}}[y_i-\hat{f}(x_i)]^2}$$

. . .

$$\Large\color{orange}{Err_{\texttt{test-split}} = \textrm{Ave}_{i\in\texttt{test-split}}I[y_i\neq \mathcal{\hat{C}}(x_i)]}$$



## Approach #1: Validation set {.small}

Auto example:  

::: nonincremental
* We have 392 observations. 
* Trying to predict `mpg` from `horsepower`. 
* We can split the data in half and use 196 to fit the model and 196 to test 
:::

```{r, fig.height = 2}
data(Auto)
set.seed(3)
f <- function() {
  samp <- sample(1:392, 196)
  train <- Auto[samp, ]
  test <- Auto[-samp, ]
  tibble(x = 1:15,
         y = map_dbl(1:15, mse, test = test, train = train)
  )
}
mse <- function(p = 1, test = test, train = train) {
  mean((test$mpg - predict(lm(mpg ~ poly(horsepower, degree = p), data = train), newdata = test))^2)
}
ggplot(f(), aes(x = x, y = y)) + 
  geom_line() +
  geom_point() + 
  labs(x = "degree polynomial", y = "MSE") + 
  theme_classic() + 
  ylim(c(15, 30))
```



## Approach #1: Validation set {.small}


```{r echo=FALSE, fig.width = 10, fig.height = .5, fig.asp = 0.05}
plot_split()
```


$\color{orange}{MSE_{\texttt{test-split}}}$

. . .

```{r echo=FALSE, fig.width = 10, fig.height = .5, fig.asp = 0.05}
plot_split()
```



$\color{orange}{MSE_{\texttt{test-split}}}$

. . .

```{r echo=FALSE, fig.width = 10, fig.height = .5, fig.asp = 0.05}
plot_split()
```


$\color{orange}{MSE_{\texttt{test-split}}}$


. . .

```{r echo=FALSE, fig.width = 10, fig.height = .5, fig.asp = 0.05}
plot_split()
```


$\color{orange}{MSE_{\texttt{test-split}}}$



## Approach #1: Validation set {.small}

Auto example: 

::: nonincremental

* We have 392 observations. 
* Trying to predict `mpg` from `horsepower`. 
* We can split the data in half and use 196 to fit the model and 196 to test - **what if we did this many times?**

:::

```{r, fig.height = 2}
set.seed(1)
d <- map_df(1:10, ~ f())
d$z <- rep(LETTERS[1:10], each = 15)
ggplot(d, aes(x = x, y = y, group = z, color = z)) + 
  geom_line() +
  labs(x = "degree polynomial", y = "MSE") + 
  theme_classic() + 
  theme(legend.position = "none") +
  coord_cartesian(ylim = c(15, 30))
```



## Approach #1: Validation set (Drawbacks) {.small}

* the validation estimate of the test error can be highly variable, depending on which observations are included in the training set and which observations are included in the validation set
* In the validation approach, only a subset of the observations (those that are included in the training set rather than in the validation set) are used to fit the model
* Therefore, the validation set error may tend to **overestimate** the test error for the model fit on the entire data set



## Approach #2: K-fold cross validation {.small}

`r emo::ji("bulb")` The idea is to do the following:

*  Randomly divide the data into $K$ equal-sized parts
*  Leave out part $k$, fit the model to the other $K - 1$ parts (combined)
*  Obtain predictions for the left-out $k$th part
*  Do this for each part $k = 1, 2,\dots K$, and then combine the result



## K-fold cross validation {.smaller}

```{r echo=FALSE, fig.width = 10, fig.height = .3, fig.asp = 0.05}
plot_split(data = c(rep("Train", 15), rep("Test", 5)))
```
$\color{orange}{MSE_{\texttt{test-split-1}}}$


. . .

```{r echo=FALSE, fig.width = 10, fig.height = .3, fig.asp = 0.05}
plot_split(data = c(rep("Train", 10), rep("Test", 5), rep("Train", 5)))
```

$\color{orange}{MSE_{\texttt{test-split-2}}}$

. . .

```{r echo=FALSE, fig.width = 10, fig.height = .3, fig.asp = 0.05}
plot_split(data = c(rep("Train", 5), rep("Test", 5), rep("Train", 10)))
```

$\color{orange}{MSE_{\texttt{test-split-3}}}$


. . .


```{r echo=FALSE, fig.width = 10, fig.height = .3, fig.asp = 0.05}
plot_split(data = c(rep("Test", 5), rep("Train", 15)))
```

$\color{orange}{MSE_{\texttt{test-split-4}}}$


**Take the mean of the $k$ MSE values**

## {{< fa laptop >}} `Application Exercise` 

Create a new R project, then a new `quarto` file with `cv` in its name in that project. Answer the questions in that file. 

If we use 10 folds:

::: nonincremental
1. What percentage of the training data is used in each analysis for each fold?
2. What percentage of the training data is used in the assessment for each fold?
:::

```{r}
#| code-fold: false
countdown::countdown(2)
```


## Estimating prediction error (quantitative outcome) {.smaller}

* Split the data into K parts, where $C_1, C_2, \dots, C_k$ indicate the indices of observations in part $k$
* $CV_{(K)} = \sum_{k=1}^K\frac{n_k}{n}MSE_k$
* $MSE_k = \sum_{i \in C_k} (y_i - \hat{y}_i)^2/n_k$
* $n_k$ is the number of observations in group $k$
* $\hat{y}_i$ is the fit for observation $i$ obtained from the data with the part $k$ removed
* If we set $K = n$, we'd have $n-fold$ cross validation which is the same as **leave-one-out cross validation** (LOOCV)

## Estimating prediction error (quantitative outcome) {.smaller}

::: nonincremental
* Split the data into K parts, where $C_1, C_2, \dots, C_k$ indicate the indices of observations in part $k$
* $CV_{(K)} = \sum_{k=1}^K\frac{n_k}{n}MSE_k$
* $MSE_k = \sum_{i \in C_k} (y_i - \hat{y}_i)^2/n_k$
* $n_k$ is the number of observations in group $k$
* $\hat{y}_i$ is the fit for observation $i$ obtained from the data with the part $k$ removed
* If we set $K = n$, we'd have $n-fold$ cross validation which is the same as **leave-one-out cross validation** (LOOCV)
:::
 
## Leave-one-out cross validation {.small}

```{r echo=FALSE, fig.width = 10, fig.height = .5, fig.asp = 0.1}
plot_split(data = c(rep("Train", 19), rep("Test", 1)))
```

. . .

```{r echo=FALSE, fig.width = 10, fig.height = .5, fig.asp = 0.1}
plot_split(data = c(rep("Train", 18), rep("Test", 1), "Train"))
```

. . .

```{r echo=FALSE, fig.width = 10, fig.height = .5, fig.asp = 0.1}
plot_split(data = c(rep("Train", 17), rep("Test", 1), rep("Train", 2)))
```


```{r echo=FALSE, fig.width = 10, fig.height = .5, fig.asp = 0.1}
plot_split(data = c(rep("Train", 16), rep("Test", 1), rep("Train", 3)))
```

$$\dots$$


```{r echo=FALSE, fig.width = 10, fig.height = .5, fig.asp = 0.1}
plot_split(data = c(rep("Test", 1), rep("Train", 19)))
```



## Special Case!

* With _linear_ regression, you can actually calculate the LOOCV error without having to iterate!
* $CV_{(n)} = \frac{1}{n}\sum_{i=1}^n\left(\frac{y_i-\hat{y}_i}{1-h_i}\right)^2$
* $\hat{y}_i$ is the $i$th fitted value from the linear model
* $h_i$ is the diagonal of the "hat" matrix (remember that! `r emo::ji("hat")`)


## Picking $K$ {.small}

* $K$ can vary from 2 (splitting the data in half each time) to $n$ (LOOCV)
* LOOCV is sometimes useful but usually the estimates from each fold are very correlated, so their average can have a **high variance**
* A better choice tends to be $K=5$ or $K=10$



## Bias variance trade-off

* Since each training set is only $(K - 1)/K$ as big as the original training set, the estimates of prediction error will typically be **biased** upward
* This bias is minimized when $K = n$ (LOOCV), but this estimate has a **high variance**
* $K =5$ or $K=10$ provides a nice compromise for the bias-variance trade-off


## Approach #2: K-fold Cross Validation {.small}

Auto example: 

::: nonincremental

* We have 392 observations. 
* Trying to predict `mpg` from `horsepower`
:::

```{r fig1, cache = TRUE, fig.height = 2}
ggplot(d, aes(x = x, y = y, group = z, color = z)) + 
  geom_line() +
  labs(x = "degree polynomial", y = "MSE", title = "50-50 Validation set") + 
  theme_classic() + 
  theme(legend.position = "none") +
  coord_cartesian(ylim = c(15, 30)) -> p1

f2 <- function() {
  Auto_ <- Auto |>
    slice(sample(1:nrow(Auto))) |>
    mutate(k = rep(1:10, length.out = nrow(Auto)))
  
  purrr::map2_df(rep(1:10, each = 15),
                 rep(1:15, 10),
                 ~ get_mse(.x, .y, data = Auto_)) |>
    group_by(x) |>
    summarise(cv_10 = sum(n / sum(n) * mse))
}

get_mse <- function(K, p, data = Auto_) {
  Auto_k <- data |>
    filter(k != K)
  model_k <- lm(mpg ~ poly(horsepower, p), data = Auto_k)
  data |>
    filter(k == K) %>%
    mutate(pred = predict(model_k, newdata = .)) |>
    summarise(mse = mean((mpg - pred)^2),
              n = n(),
              x = p)
}
d2 <- map_df(1:10, ~f2())
d2$group <- rep(LETTERS[1:10], each = 15)
ggplot(d2, aes(x = x, y = cv_10, group = group, color = group)) +
  geom_line() + 
  coord_cartesian(ylim = c(15, 30)) + 
  labs(y = "MSE", x = "degree polynomial", title = "10-fold CV") + 
  theme_classic() + 
  theme(legend.position = "none") -> p2

grid.arrange(p1, p2, ncol = 2)
```



## Estimating prediction error (qualitative outcome) {.small}

* The premise is the same as cross valiation for quantitative outcomes
* Split the data into K parts, where $C_1, C_2, \dots, C_k$ indicate the indices of observations in part $k$
* $CV_K = \sum_{k=1}^K\frac{n_k}{n}Err_k$
* $Err_k = \sum_{i\in C_k}I(y_i\neq\hat{y}_i)/n_k$ (misclassification rate)
* $n_k$ is the number of observations in group $k$
* $\hat{y}_i$ is the fit for observation $i$ obtained from the data with the part $k$ removed


## Estimating prediction error (qualitative outcome) {.small}

::: nonincremental
* The premise is the same as cross valiation for quantitative outcomes
* Split the data into K parts, where $C_1, C_2, \dots, C_k$ indicate the indices of observations in part $k$
* $CV_K = \sum_{k=1}^K\frac{n_k}{n}Err_k$
* $Err_k = \sum_{i\in C_k}I(y_i\neq\hat{y}_i)/n_k$ (misclassification rate)
* $n_k$ is the number of observations in group $k$
* $\hat{y}_i$ is the fit for observation $i$ obtained from the data with the part $k$ removed
:::








# tidymodels

## {{< fa laptop >}} `Application Exercise` {.small}


::: nonincremental
1. Create a new `quarto` file in your project and add `tidymodels` in the name. 

2. Load the packages by running the top chunk of R code
:::

```{r}
#| echo: true
#| label: load-packeges-example
library(tidymodels)
library(broom)
library(ISLR)
library(countdown)
```

## tidymodels {.small}

:::: columns

::: column
![](img/02/tidymodels.png)
:::

::: column

[tidymodels.org](https://www.tidymodels.org/)


- tidymodels is an opinionated collection of R packages designed for modeling and statistical analysis.
- All packages share an underlying philosophy and a common grammar.
:::

::::


## Step 1: Specify the model

* Pick the **model**
* Set the **engine**



## Specify the model


```{r}
#| label: lr_engine
#| echo: true
#| eval: false
linear_reg() |>
  set_engine("lm")
```



## Specify the model


```{r}
#| label: glm_engine
#| echo: true
#| eval: false
linear_reg() |>
  set_engine("glmnet")
```



## Specify the model


```{r}
#| label: spark_engine
#| echo: true
#| eval: false
linear_reg() |>
  set_engine("spark")
```



## Specify the model


```{r}
#| label: rpart_engine
#| echo: true
#| eval: false
decision_tree() |>
  set_engine("rpart")
```


## Specify the model

::: nonincremental

* All available models:

[tidymodels.org](https://www.tidymodels.org/find/parsnip/)

:::




## {{< fa laptop >}} `Application Exercise` 

::: nonincremental

1. Write a pipe that creates a model that uses `lm()` to fit a linear regression using tidymodels. Save it as `lm_spec` and look at the object. What does it return?

:::

_Hint: you'll need  https://www.tidymodels.org_

```{r}
#| echo: false
countdown::countdown(minutes = 5)
```

## Answer

```{r}
#| label: lr_exa_answer
#| echo: true
lm_spec <- 
  linear_reg() |> # Pick linear regression
  set_engine(engine = "lm") # set engine
lm_spec
```



## Fit the data

::: nonincremental

- You can train your model using the `fit()` function

```{r}
#| label: fit_exam
#| echo: true
fit(lm_spec,
    mpg ~ horsepower,
    data = Auto)
```

:::

## `r fontawesome::fa("laptop")` `Application Exercise` {.small}

::: nonincremental

1. Fit the model:

```{r}
#| label: example2_fit
#| echo: true
#| eval: false
library(ISLR)
lm_fit <- fit(lm_spec,
              mpg ~ horsepower,
              data = Auto)
lm_fit
```


Does this give the same results as

```{r, eval = FALSE}
#| label: lm-vs-tidymodels-lm
#| echo: true
#| eval: false
lm(mpg ~ horsepower, data = Auto)
```

```{r}
#| echo: false
countdown::countdown(3)
```

:::

## Answer {.small}

```{r}
#| label: lm-tm-answer
#| echo: true
lm_fit <- fit(lm_spec,
              mpg ~ horsepower,
              data = Auto)
lm_fit
lm(mpg ~ horsepower, data = Auto)
```


## Get predictions

```{r}
#| label: predict_ex
#| echo: true
#| eval: false
lm_fit |>
  predict(new_data = Auto)
```


* Uses the `predict()` function
* `r emo::ji("double_exclamation_mark")` `new_data` has an underscore
* `r emo::ji("smile")` This automagically creates a data frame


## Get predictions {.small}

```{r}
#| label: pred_w_bind
#| echo: true
lm_fit |>
  predict(new_data = Auto) |>
  bind_cols(Auto)
```

. . .

::: question
What does `bind_cols` do?
:::


## Get predictions {.small}

```{r}
#| label: pred_w_bind_ex2
#| echo: true

lm_fit |>
  predict(new_data = Auto) |>
  bind_cols(Auto)
```

::: question
Which column has the predicted values?
:::

## `r fontawesome::fa("laptop")` `Application Exercise` 

```{r}
#| echo: false
countdown::countdown(minutes = 3)
```

::: nonincremental
1. Edit the code below to add the original data to the predicted data.
:::

```{r, eval = FALSE}
#| label: pred_lm_auto
#| echo: true
mpg_pred <- lm_fit |> 
  predict(new_data = Auto) |> 
  ---
```



## Get predictions {.small}

```{r}
#| label: pred_lm_ae_answer
#| echo: true
mpg_pred <- lm_fit |>
  predict(new_data = Auto) |>
  bind_cols(Auto)

mpg_pred
```


## Calculate the error

::: nonincremental
* Root mean square error
:::

```{r}
#| label: get-rmse
#| echo: true
mpg_pred |>
  rmse(truth = mpg, estimate = .pred)
```

. . .

::: question
What is this estimate? (training error? testing error?)
:::



## Validation set approach

```{r}
#| label: valid-set-1
#| echo: true
Auto_split <- initial_split(Auto, prop = 0.5)
Auto_split
```

. . .

::: question
How many observations are in the training set?
:::

## Validation set approach

```{r}
#| label: valid-set-2
#| echo: true
Auto_split <- initial_split(Auto, prop = 0.5)
Auto_split
```


::: question
How many observations are in the test set?
:::

## Validation set approach

```{r}
#| label: valid-set-3
#| echo: true
Auto_split <- initial_split(Auto, prop = 0.5)
Auto_split
```


::: question
How many observations are there in total?
:::


## Validation set approach

```{r}
#| label: valid-set-4
#| echo: true
Auto_split <- initial_split(Auto, prop = 0.5)
Auto_split
```

::: nonincremental
* Extract the training and testing data
:::

```{r, eval = FALSE}
#| label: train-test
#| echo: true
training(Auto_split)
testing(Auto_split)
```



## Validation set approach {.small}

```{r}
#| label: set_train
#| echo: true
Auto_train <- training(Auto_split)
```

```{r}
#| label: print_train
#| echo: true
#| eval: false
Auto_train
```


```{r}
#| label: print_train-2
#| echo: false
as_tibble(Auto_train)
```


## `r fontawesome::fa("laptop")` `Application Exercise`  {.small}

::: nonincremental
1. Copy the code below, fill in the blanks to fit a model on the **training** data then calculate the **test** RMSE.

```{r, eval = FALSE}
#| label: ae-fill-blanks
#| echo: true
set.seed(100)
Auto_split  <- ________
Auto_train  <- ________
Auto_test   <- ________
lm_fit      <- fit(lm_spec, 
                   mpg ~ horsepower, 
                   data = ________)
mpg_pred  <- ________ |> 
  predict(new_data = ________) |> 
  bind_cols(________)
rmse(________, truth = ________, estimate = ________)
```

```{r}
#| echo: false
countdown::countdown(6)
```

:::

## A faster way! {.small}

* You can use `last_fit()` and specify the split
* This will automatically train the data on the `train` data from the split
* Instead of specifying which metric to calculate (with `rmse` as before) you can just use `collect_metrics()` and it will automatically calculate the metrics on the `test` data from the split

## A faster way! {.small}

```{r}
#| label: last_fit
#| echo: true
set.seed(100)

Auto_split <- initial_split(Auto, prop = 0.5)
lm_fit <- last_fit(lm_spec,
                   mpg ~ horsepower,
                   split = Auto_split) 

lm_fit |>
  collect_metrics()
```

## A faster way! {.small}

```{r}
#| code-line-numbers: "6|9"
#| label: last_fit-2
#| echo: true
set.seed(100)

Auto_split <- initial_split(Auto, prop = 0.5)
lm_fit <- last_fit(lm_spec,
                   mpg ~ horsepower,
                   split = Auto_split) 

lm_fit |>
  collect_metrics()
```



## What about cross validation?

```{r}
#| label: vfold-1
#| echo: true
Auto_cv <- vfold_cv(Auto, v = 5)
Auto_cv
```


## What about cross validation?

* Instead of `fit` we will use `fit_resamples` 

. . .

```{r, eval = FALSE}
#| label: fit_resample-2
#| echo: true
#| code-line-numbers: "1|2|3"
fit_resamples(lm_spec, 
              mpg ~ horsepower,
              resamples = Auto_cv) 
```




## What about cross validation?

::: question
How do we get the metrics out? With `collect_metrics()` again!
:::

. . .


```{r}
#| label: collect-metrics
#| echo: true
results <- fit_resamples(lm_spec,
                         mpg ~ horsepower,
                         resamples = Auto_cv)

results |>
  collect_metrics()
```


## `r fontawesome::fa("laptop")` `Application Exercise`  {.small}

::: nonincremental

```{r} 
#| echo: false
countdown::countdown(minutes = 5)
```

1. Edit the code below to get the 5-fold cross validation error rate for the following model:

$mpg = \beta_0 + \beta_1 horsepower + \beta_2 horsepower^2+ \epsilon$

```{r, eval = FALSE}
#| label: vfold-example
#| echo: true
Auto_cv <- vfold_cv(Auto, v = 5)

results <- fit_resamples(lm_spec,
                         ----,
                         resamples = ---)

results |>
  collect_metrics()
```

* What do you think `rsq` is?

:::

## Answer

```{r}
#| label: vfold-example-sol
#| echo: true
Auto_cv <- vfold_cv(Auto, v = 5)

results <- fit_resamples(lm_spec,
                         mpg ~ horsepower + I(horsepower^2),
                         resamples = Auto_cv)

results |>
  collect_metrics()
```

## `r fontawesome::fa("laptop")` `Application Exercise`  {.small}

::: nonincremental
1. Fit 3 models on the data using 5 fold cross validation:

   ::: smaller
     $mpg = \beta_0 + \beta_1 horsepower + \epsilon$ 
     
     $mpg = \beta_0 + \beta_1 horsepower + \beta_2 horsepower^2+ \epsilon$
     
     $mpg = \beta_0 + \beta_1 horsepower + \beta_2 horsepower^2+ \beta_3 horsepower^3 +\epsilon$
   :::

2. Collect the metrics from each model, saving the results as `results_1`, `results_2`, `results_3`

3. Which model is "best"?

:::
```{r}
#| echo: false
countdown(8)
```

