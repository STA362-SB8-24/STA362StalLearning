{
  "hash": "81a62cb445c6360391916f62a8457d34",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 6 Part 2\"\nsubtitle: \"Shrinkage\"\nformat: \n  revealjs:\n    output-file: \"06-2-shrinkage.html\"\n    slide-number: true\n  html:\n    output-file: \"06-2-shrinkage_o.html\"\nlogo: \"img/icon.png\"\n---\n\n\n\n## Setup\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR2)\nlibrary(leaps)\n```\n:::\n\n\n\n## Shrinkage Methods {.smaller}\n\nRidge regression and Lasso\n- The subset selection methods use least squares to fit a linear model that contains a subset of the predictors.\n\n- As an alternative, we can fit a model containing all p predictors using a technique that constrains or regularizes the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero.\n\n- It may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficient estimates can significantly reduce their variance.\n\n\n## Another Reason\n\n* Sometimes we can't solve for $\\hat\\beta$\n\n::: question\n* Why?\n:::\n\n* We have more variables than observations ( $p > n$ )\n* The variables are linear combinations of one another\n* The variance can blow up\n\n# What can we do about this?\n\n## Ridge Regression {.small}\n\n* What if we add an additional _penalty_ to keep the $\\hat\\beta$ coefficients small (this will keep the variance from blowing up!)\n* Instead of minimizing $RSS$, like we do with linear regression, let's minimize $RSS$ PLUS some penalty function\n* $RSS + \\underbrace{\\lambda\\sum_{j=1}^p\\beta^2_j}_{\\textrm{shrinkage penalty}}$\n\n\n::: question\n* What happens when $\\lambda=0$? What happens as $\\lambda\\rightarrow\\infty$?\n:::\n\n\n\n## Ridge Regression {.small}\n\n::: {.panel-tabset}\n\n### LR\n- Recall, the least squares fitting procedure estimates $\\beta_0,...,\\beta_p$ using the values that  minimize $$RSS = \\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right)^2$$\n\n\n### Ridge\n- Ridge regression coefficient estimates, $\\hat{\\beta}^R$ are the values that minimize \n\n$$\\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right)^2+\\lambda\\sum_{j=1}^p\\beta_j^2$$\n\n$$ = RSS + \\lambda\\sum_{j=1}^p\\beta_j^2$$\n\nwhere $\\lambda\\geq 0$ is a tuning parameter, to be determined separately\n\n:::\n\n## More on Ridge\n\n- Like least squares, ridge regression seeks coefficient estimates that fit the data well by making the RSS small. \n\n- The second term $\\lambda\\sum_j\\beta_j^2$ is called a _shrinkage penalty_, is small when $\\beta_1,...\\beta_p$ are close to 0, and so it has the effect of _shrinking_ the estimates of $\\beta_j$ toward 0. \n\n## Ridge Shrinkage {.smaller}\n\n::::{.columns}\n\n:::{.column}\n\n![](img/ridge_shrink.png)\n:::\n\n:::{.column} \n\nEach curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of $\\lambda$.\n:::\n::::\n\n## Ridge Shinkage Coeff {.smaller}\n\n::::{.columns}\n:::{.column}\n![](img/ridge_shrink_2.png)\n\n:::\n\n:::{.column}\n\n\n- This displays the same ridge coefficient estimates as the previous graphs, but instead of displaying $\\lambda$ on the x-axis, we now display $||\\hat{\\beta}_\\lambda^R||_2/||\\hat{\\beta}||_2$, where $\\hat{\\beta}$ denotes the vector of the least squares coefficient estimates. \n\n- In statistics lingo, the ridge uses an $\\ell_2$ (pronounced \"ell 2\") penalty of the betas, written $||\\beta||_2$. \n\n:::\n::::\n\n## Ridge - Scalling Predictors {.smaller}\n\n- The standard least squares coefficient estimates are scale\nequivalent: multiplying $X_j$ by a constant c simply leads to\na scaling of the least squares coefficient estimates by a\nfactor of $1=c$. In other words, regardless of how the $j$th\npredictor is scaled, $X_j\\hat{\\beta}_j$ will remain the same.\n\n- In contrast, the ridge regression coefficient estimates can\nchange substantially when multiplying a given predictor by\na constant, due to the sum of squared coefficients term in\nthe penalty part of the ridge regression objective function.\n\n- Therefore, it is best to apply ridge regression after\nstandardizing the predictors, using the formula \n\n- $$\\tilde{x}_{ij} = \\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)^2}}$$\n\n## Ridge Regression\n\n* **IMPORTANT**: When doing ridge regression, it is important to standardize your variables (divide by the standard deviation)\n\n\n\n\n## Bias-variance tradeoff\n\n::: question\nHow do you think ridge regression fits into the bias-variance trade-off?\n:::\n\n\n* As $\\lambda$ ‚òùÔ∏è, bias ‚òùÔ∏è, variance üëá\n\n## Ridge Bias-Variance tradeoff {.smaller}\n\n\n\n![](img/RidgeBVTrade.png)\n\n- Simulated data with n = 50 observations, p = 45 predictors, all having\nnonzero coefficients. \n- Squared bias (black), variance (green), and test\nmean squared error (purple) for the ridge regression predictions on a\nsimulated data set, as a function of $\\lambda$ and $||\\hat{\\beta}_\\lambda^R||_2/||\\hat{\\beta}||_2$. The horizontal dashed lines indicate the minimum possible MSE. The purple crosses indicate the ridge regression models for which the MSE is smallest. \n\n\n\n## Lasso {.smaller}\n\n- Ridge regression does have one obvious disadvantage: unlike subset selection, which will generally select models that involve just a subset of the variables, ridge regression will include all p predictors in the final model\n\n- The Lasso is a relatively recent alternative to ridge regression that overcomes this disadvantage. The lasso coefficients, $\\hat{\\beta}_\\lambda^L$, minimize the quantity\n\n\n- $$\\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right)^2+\\lambda\\sum_{j=1}^p|\\beta_j|= RSS + \\lambda\\sum_{j=1}^p|\\beta_j|$$\n\n- where $\\lambda\\geq 0$ is a tuning parameter, to be determined separately\n\n- In statistics lingo, the lasso uses an $\\ell_1$ (pronounced \"ell 1\") penalty instead of an $\\ell_2$ penalty. The $\\ell_1$ norm of a coefficient vector $\\beta$ is given by $||\\beta||_1 = \\sum|\\beta_j|$\n\n\n## Lasso Continued {.smaller}\n\n- As with ridge regression, the lasso shrinks the coefficient\nestimates towards zero.\n- In the case of the lasso, the $\\ell_1$ penalty has the\neffect of forcing some of the coefficient estimates to be\nexactly equal to zero when the tuning parameter $\\lambda$ is\nsufficiently large.\n- Like best subset selection, the lasso performs\nvariable selection.\n- We say that the lasso yields sparse models - that is,\nmodels that involve only a subset of the variables.\n- As in ridge regression, selecting a good value of $\\lambda$ for the\nlasso is critical; cross-validation is again the method of\nchoice.\n\n## Lasso Coefficient Shrink\n\n![](img/lasso_shrink.png)\n\n## Lasso Coefficient Ratio\n\n![](img/lasso_shrink_2.png)\n\n\n## Lasso Regression\n\n* **IMPORTANT**: When doing lasso regression, it is important to standardize your variables (divide by the standard deviation)\n\n## Ridge vs Lasso\n\n:::{.question}\nWhy does lasso, unlike ridge, result in coefficient estimates that are exactly zero?\n:::\n\n## Ridge vs Lasso 2 {.smaller}\n\nThey each are a minimization problem\n\nLasso: $$\\text{minimize}_\\beta\\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\\text{ subject to }\\sum_{j=1}^p|\\beta_j|\\leq s$$\n\nRidge: $$\\text{minimize}_\\beta\\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\\text{ subject to }\\sum_{j=1}^p\\beta_j^2\\leq s$$\n\n## Ridge vs Lasso Graphs\n\n![](img/ridge_vs_lasso_graph.png)\n\n## Ridge vs Lasso $\\lambda$ vs MSE\n\n::::{.columns}\n:::{.column}\n\n![](img/ridge_vs_lasso_performance.png)\n\n:::\n\n:::{.column}\n\nPlots of squared bias (black), variance (green), and test\nMSE (purple) for the lasso on simulated data set.\n\n:::\n\n::::\n\n\n## Ridge vs Lasso $R^2$ vs MSE\n\n::::{.columns}\n:::{.column}\n\n![](img/ridge_vs_lasso_performance_2.png)\n\n:::\n\n:::{.column}\n\nComparison of squared bias, variance and test MSE between lasso (solid) and ridge (dashed). Both are plotted against their $R^2$ on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest.\n:::\n\n::::\n\n\n\n\n\n\n## Ridge vs Lasso $\\lambda$ vs MSE Ex 2\n\n::::{.columns}\n\n:::{.column}\n\n![](img/ridge_vs_lasso_performance_e2.png)\n\n:::\n\n:::{.column}\n\nPlots of squared bias (black), variance (green), and test\nMSE (purple) for the lasso. The simulated data is similar to\nthat before, except that now only two predictors are related\nto the response.\n\n:::\n\n::::\n\n\n## Ridge vs Lasso $R^2$ vs MSE Ex 2\n\n::::{.columns}\n:::{.column}\n\n![](img/ridge_vs_lasso_performance_e2_2.png)\n\n:::\n\n:::{.column}\n\nComparison of squared bias, variance\nand test MSE between lasso (solid) and ridge (dashed). Both\nare plotted against their $R^2$ on the training data, as a common\nform of indexing. The crosses in both plots indicate the lasso\nmodel for which the MSE is smallest.\n\n:::\n\n::::\n\n## Lasso vs Ridge Summary {.smaller}\n\n- These two examples illustrate that neither ridge regression\nnor the lasso will universally dominate the other.\n\n- In general, one might expect the lasso to perform better\nwhen the response is a function of only a relatively small\nnumber of predictors.\n\n- However, the number of predictors that is related to the\nresponse is never known a priori for real data sets.\n\n- A technique such as cross-validation can be used in order\nto determine which approach is better on a particular data\nset.\n\n## Choosing $\\lambda$ {.smaller}\n\n* $\\lambda$ is known as a **tuning parameter** and is selected using **cross validation**\n* For example, choose the $\\lambda$ that results in the smallest estimated test error\n* Afterwards we refit using all available observations (from training set)\n\n",
    "supporting": [
      "06-2-shrinkage_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}