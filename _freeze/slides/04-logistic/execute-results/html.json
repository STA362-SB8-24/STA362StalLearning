{
  "hash": "db66277c38338db61a5ad8aefcfe81cd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 4 Part 1\"\nsubtitle: \"Logistic Regression\"\nformat: \n  revealjs:\n    output-file: \"04-logistic.qmd\"\n    slide-number: true\n  html:\n    output-file: \"04-logistic_o.qmd\"\n---\n\n \n## Recap {.small}\n\n* We had a _linear regression_ refresher\n* Linear regression is a great tool when we have a continuous outcome\n* We are going to learn some fancy ways to do even better in the future\n\n*Setup:*\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR)\n```\n:::\n\n\n\n# Classification\n\n## Classification {.smaller}\n\n:::question\nWhat are some examples of classification problems?\n:::\n\n* Qualitative response variable in an _unordered set_, $\\mathcal{C}$\n* `eye color` $\\in$ `{blue, brown, green}`\n* `email` $\\in$ `{spam, not spam}`\n* Response, $Y$ takes on values in $\\mathcal{C}$\n* Predictors are a vector, $X$\n* The task: build a function $C(X)$ that takes $X$ and predicts $Y$, $C(X)\\in\\mathcal{C}$ \n* Many times we are actually more interested in the _probabilities_ that $X$ belongs to each category in $\\mathcal{C}$\n\n\n\n## Example: Credit card default\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(1)\nDefault |>\n  sample_frac(size = 0.25) |>\n  ggplot(aes(balance, income, color = default)) +\n  geom_point(pch = 4) +\n  scale_color_manual(values = c(\"cornflower blue\", \"red\")) +\n  theme_classic() +\n  theme(legend.position = \"top\") -> p1\n\np2 <- ggplot(Default, aes(x = default, y = balance, fill = default)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"cornflower blue\", \"red\")) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\np3 <- ggplot(Default, aes(x = default, y = income, fill = default)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"cornflower blue\", \"red\")) +\n  theme_classic() +\n  theme(legend.position = \"none\")\ngrid.arrange(p1, p2, p3, ncol = 3, widths = c(2, 1, 1))\n```\n\n::: {.cell-output-display}\n![](04-logistic_files/figure-revealjs/plot1-1.png){width=960}\n:::\n:::\n\n\n\n\n## Can we use linear regression? {.small}\n\nWe can code `Default` as\n\n$$Y = \\begin{cases} 0 & \\textrm{if }\\texttt{No}\\\\ 1&\\textrm{if }\\texttt{Yes}\\end{cases}$$\nCan we fit a linear regression of $Y$ on $X$ and classify as `Yes` if $\\hat{Y}> 0.5$?\n\n* In this case of a **binary** outcome, linear regression is okay (it is equivalent to **linear discriminant analysis**, you can read more about that in your book!)\n* $E[Y|X=x] = P(Y=1|X=x)$, so it seems like this is a pretty good idea!\n* **The problem**: Linear regression can produce probabilities less than 0 or greater than 1 😱\n\n## Can we use linear regression? {.small}\n\nWe can code `Default` as\n\n$$Y = \\begin{cases} 0 & \\textrm{if }\\texttt{No}\\\\ 1&\\textrm{if }\\texttt{Yes}\\end{cases}$$\nCan we fit a linear regression of $Y$ on $X$ and classify as `Yes` if $\\hat{Y}> 0.5$?\n\n::: question\nWhat may do a better job?\n:::\n\n* **Logistic regression!**\n\n\n\n## Linear versus logistic regression {.small}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nDefault <- Default |>\n  mutate(\n  p = glm(default ~ balance, data = Default, family = \"binomial\") |>\n  predict(type = \"response\"),\n  p2 = lm(I(default == \"Yes\") ~ balance, data = Default) |> predict(),\n  def = ifelse(default == \"Yes\", 1, 0)\n)\n\n\nDefault |>\n  sample_frac(0.25) |>\nggplot(aes(balance, p2)) +\n  geom_hline(yintercept = c(0, 1), lty = 2, size = 0.2) +\n  geom_line(color = \"cornflower blue\") +\n  geom_point(aes(balance, def), shape = \"|\", color = \"orange\") +\n  theme_classic() +\n  labs(y = \"probability of default\") -> p1\n\nDefault |>\n  sample_frac(0.25) |>\nggplot(aes(balance, p)) +\n  geom_hline(yintercept = c(0, 1), lty = 2, size = 0.2) +\n  geom_line(color = \"cornflower blue\") +\n  geom_point(aes(balance, def), shape = \"|\", color = \"orange\") +\n  theme_classic() +\n  labs(y = \"probability of default\") -> p2\n\ngrid.arrange(p1, p2, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](04-logistic_files/figure-revealjs/plot2-1.png){width=960}\n:::\n:::\n\n\n:::question\nWhich does a better job at predicting the probability of default?\n:::\n\n* The orange marks represent the response $Y\\in\\{0,1\\}$\n\n\n## Linear Regression {.small}\n\nWhat if we have $>2$ possible outcomes? For example, someone comes to the emergency room and we need to classify them according to their symptoms\n\n$$ \n\\begin{align}\nY = \\begin{cases} 1& \\textrm{if }\\texttt{stroke}\\\\2&\\textrm{if }\\texttt{drug overdose}\\\\3&\\textrm{if }\\texttt{epileptic seizure}\\end{cases}\n\\end{align}\n$$\n\n:::question\nWhat could go wrong here?\n:::\n\n\n* The coding implies an _ordering_\n* The coding implies _equal spacing_ (that is the difference between `stroke` and `drug overdose` is the same as `drug overdose` and `epileptic seizure`)\n\n\n## Linear Regression {.small}\n\nWhat if we have $>2$ possible outcomes? For example, someone comes to the emergency room and we need to classify them according to their symptoms\n\n$$ \n\\begin{align}\nY = \\begin{cases} 1& \\textrm{if }\\texttt{stroke}\\\\2&\\textrm{if }\\texttt{drug overdose}\\\\3&\\textrm{if }\\texttt{epileptic seizure}\\end{cases}\n\\end{align}\n$$\n\n* Linear regression is **not** appropriate here\n* **Mutliclass logistic regression** or **discriminant analysis** are more appropriate\n\n\n\n## Logistic Regression {.small}\n\n$$ \np(X) = \\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}\n$$\n\n* Note: $p(X)$ is shorthand for $P(Y=1|X)$\n* No matter what values $\\beta_0$, $\\beta_1$, or $X$ take $p(X)$ will always be between 0 and 1\n\n## Logistic Regression {.small}\n\n$$ \np(X) = \\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}\n$$\n\nWe can rearrange this into the following form:\n\n$$\n\\log\\left(\\frac{p(X)}{1-p(X)}\\right) = \\beta_0 + \\beta_1 X\n$$\n\n:::question\nWhat is this transformation called?\n:::\n\n* This is a **log odds** or **logit** transformation of $p(X)$\n\n\n## Linear versus logistic regression\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-logistic_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\nLogistic regression ensures that our estimates for $p(X)$ are between 0 and 1 🎉\n\n\n\n## Maximum Likelihood\n\n:::question\nRefresher: How did we estimate $\\hat\\beta$ in linear regression?\n:::\n\n\n\n## Maximum Likelihood {.small}\n\n:::question\nRefresher: How did we estimate $\\hat\\beta$ in linear regression?\n:::\n\nIn logistic regression, we use **maximum likelihood** to estimate the parameters\n\n$$\\mathcal{l}(\\beta_0,\\beta_1)=\\prod_{i:y_i=1}p(x_i)\\prod_{i:y_i=0}(1-p(x_i))$$\n\n\n* This **likelihood** give the probability of the observed ones and zeros in the data\n* We pick $\\beta_0$ and $\\beta_1$ to maximize the likelihood\n* _We'll let `R` do the heavy lifting here_\n\n\n## Let's see it in R {.small}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(default ~ balance, \n      data = Default) |>\n  tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term         estimate std.error statistic   p.value\n  <chr>           <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept) -10.7      0.361        -29.5 3.62e-191\n2 balance       0.00550  0.000220      25.0 1.98e-137\n```\n\n\n:::\n:::\n\n\n\n* Use the `logistic_reg()` function in R with the `glm` engine\n\n\n\n## Making predictions\n\n:::question\nWhat is our estimated probability of default for someone with a balance of $1000?\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> -10.6513306 </td>\n   <td style=\"text-align:right;\"> 0.3611574 </td>\n   <td style=\"text-align:right;\"> -29.49221 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> balance </td>\n   <td style=\"text-align:right;\"> 0.0054989 </td>\n   <td style=\"text-align:right;\"> 0.0002204 </td>\n   <td style=\"text-align:right;\"> 24.95309 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n. . .\n\n$$\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}{1+e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}=\\frac{e^{-10.65+0.0055\\times 1000}}{1+e^{-10.65+0.0055\\times 1000}}=0.006\n$$\n\n\n\n## Making predictions\n\n:::question\nWhat is our estimated probability of default for someone with a balance of $2000?\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> -10.6513306 </td>\n   <td style=\"text-align:right;\"> 0.3611574 </td>\n   <td style=\"text-align:right;\"> -29.49221 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> balance </td>\n   <td style=\"text-align:right;\"> 0.0054989 </td>\n   <td style=\"text-align:right;\"> 0.0002204 </td>\n   <td style=\"text-align:right;\"> 24.95309 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n. . .\n\n$$\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}{1+e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}=\\frac{e^{-10.65+0.0055\\times 2000}}{1+e^{-10.65+0.0055\\times 2000}}=0.586\n$$\n\n\n \n## Logistic regression example {.small}\n\nLet's refit the model to predict the probability of default given the customer is a `student`\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> -3.5041278 </td>\n   <td style=\"text-align:right;\"> 0.0707130 </td>\n   <td style=\"text-align:right;\"> -49.554219 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> studentYes </td>\n   <td style=\"text-align:right;\"> 0.4048871 </td>\n   <td style=\"text-align:right;\"> 0.1150188 </td>\n   <td style=\"text-align:right;\"> 3.520181 </td>\n   <td style=\"text-align:right;\"> 0.0004313 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n$$P(\\texttt{default = Yes}|\\texttt{student = Yes}) = \\frac{e^{-3.5041+0.4049\\times1}}{1+e^{-3.5041+0.4049\\times1}}=0.0431$$\n\n. . .\n\n:::question\nHow will this change if student = No?\n:::\n\n. . .\n\n$$P(\\texttt{default = Yes}|\\texttt{student = No}) = \\frac{e^{-3.5041+0.4049\\times0}}{1+e^{-3.5041+0.4049\\times0}}=0.0292$$\n\n\n\n\n\n\n## Potential Confounding {.small}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-logistic_files/figure-revealjs/plot3-1.png){width=960}\n:::\n:::\n\n\n:::question\nWhat is going on here?\n:::\n\n\n## Confounding {.small}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-logistic_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n* Students tend to have higher balances than non-students\n* Their **marginal** default rate is higher\n* For each level of balance, students default less \n* Their **conditional** default rate is lower\n\n## A bit about \"odds\" {.small}\n\n* The \"odds\" tell you how likely an event is\n* 🌂 Let's say there is a 60% chance of rain today * What is the probability that it will rain?\n* $p = 0.6$\n* What is the probability that it **won't** rain?\n* $1-p = 0.4$\n* What are the **odds** that it will rain? \n* 3 to 2, 3:2, $\\frac{0.6}{0.4} = 1.5$\n\n\n## Transforming logs {.small}\n\n* How do you \"undo\" a $\\log$ base $e$?\n* Use $e$! For example:\n* $e^{\\log(10)} = 10$ \n* $e^{\\log(1283)} = 1283$\n* $e^{\\log(x)} = x$\n  \n\n## Transforming logs {.small}\n\n::: question\nHow would you get the odds from the log(odds)?\n:::\n\n::: nonincremental\n* How do you \"undo\" a $\\log$ base $e$?\n* Use $e$! For example:\n* $e^{\\log(10)} = 10$ \n* $e^{\\log(1283)} = 1283$\n* $e^{\\log(x)} = x$\n:::\n* $e^{\\log(odds)}$ = odds\n\n\n\n## Transforming odds {.small}\n\n* odds = $\\frac{\\pi}{1-\\pi}$\n* Solving for $\\pi$\n* $\\pi = \\frac{\\textrm{odds}}{1+\\textrm{odds}}$\n* Plugging in $e^{\\log(odds)}$ = odds\n* $\\pi = \\frac{e^{\\log(odds)}}{1+e^{\\log(odds)}}$\n* Plugging in $\\log(odds) = \\beta_0 + \\beta_1x$\n* $\\pi = \\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}$\n\n\n## The logistic model\n\n* ✌️ forms\n\nForm | Model\n-----|------\nLogit form | $\\log\\left(\\frac{\\pi}{1-\\pi}\\right) = \\beta_0 + \\beta_1x$\nProbability form | $\\Large\\pi = \\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}$\n\n\n\n## The logistic model\n\nprobability | odds | log(odds)\n--|--|--\n$\\pi$ | $\\frac{\\pi}{1-\\pi}$ | $\\log\\left(\\frac{\\pi}{1-\\pi}\\right)=l$\n\n\n⬅️\n\n\nlog(odds) | odds | probability\n--|--|--\n$l$ | $e^l$ | $\\frac{e^l}{1+e^l} = \\pi$\n\n\n\n## The logistic model {.small}\n\n* ✌️ forms\n* **log(odds)**: $l = \\beta_0 + \\beta_1x$\n* **P(Outcome = Yes)**: $\\Large\\pi =\\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}$\n\n\n\n\n## Odds ratios {.small}\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person’s head\nmight be an effective treatment for migraine headaches.\n\n* Researchers recruited 200 subjects who suffered from migraines \n* randomly assigned them to receive either the TMS (transcranial magnetic\nstimulation) treatment or a placebo treatment \n* Subjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later. (either **Pain-free** or **Not pain-free**)\n\n\n\n## Odds ratios {.small}\n\n::: question\nWhat is the explanatory variable?\n:::\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person’s head might be an effective treatment for migraine headaches.\n\n::: nonincremental\n* Researchers recruited 200 subjects who suffered from migraines \n* randomly assigned them to receive either the TMS (transcranial magnetic\nstimulation) treatment or a placebo treatment \n* Subjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either **Pain-free** or **Not pain-free**)\n:::\n\n## Odds ratios {.small}\n\n::: question\nWhat type of variable is this?\n:::\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person’s head might be an effective treatment for migraine headaches.\n\n::: nonincremental\n* Researchers recruited 200 subjects who suffered from migraines \n* randomly assigned them to receive either the TMS (transcranial magnetic\nstimulation) treatment or a placebo treatment \n* Subjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either **Pain-free** or **Not pain-free**)\n:::\n\n## Odds ratios {.small}\n\n::: question\nWhat is the outcome variable?\n:::\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person’s head might be an effective treatment for migraine headaches.\n\n::: nonincremental\n* Researchers recruited 200 subjects who suffered from migraines \n* randomly assigned them to receive either the TMS (transcranial magnetic\nstimulation) treatment or a placebo treatment \n* Subjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either **Pain-free** or **Not pain-free**)\n:::\n\n## Odds ratios {.small}\n\n::: question\nWhat type of variable is this?\n:::\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person’s head might be an effective treatment for migraine headaches.\n\n::: nonincremental\n* Researchers recruited 200 subjects who suffered from migraines \n* randomly assigned them to receive either the TMS (transcranial magnetic\nstimulation) treatment or a placebo treatment \n* Subjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either **Pain-free** or **Not pain-free**)\n:::\n\n## Odds ratios {.smaller}\n\nTreatment |TMS | Placebo| Total|\n|---|---|----|---|\n|Pain-free two hours later |39| 22 |61|\n|Not pain-free two hours later |61 |78| 139|\n|Total| 100| 100 |200|\n\n* We can compare the results using **odds**\n* What are the odds of being pain-free for the placebo group?\n* $(22/100)/(78/100) = 22/78 = 0.282$\n* What are the odds of being pain-free for the treatment group?\n* $39/61 = 0.639$\n* Comparing the **odds** what can we conclude?\n* TMS increases the likelihood of success\n\n\n## Odds ratios {.small}\n\nTreatment |TMS | Placebo| Total\n---|---|----|---\nPain-free two hours later |39| 22 |61\nNot pain-free two hours later |61 |78| 139\nTotal| 100| 100 |200\n\n* We can summarize this relationship with an **odds ratio**: the ratio of the two odds\n* $\\Large OR = \\frac{39/61}{22/78} = \\frac{0.639}{0.282} = 2.27$\n* _\"the odds of being pain free were 2.27 times higher with TMS than with the placebo\"_\n\n\n\n## Odds ratios {.small}\n\n::: question\nWhat if we wanted to calculate this in terms of Not pain-free (with pain-free) as the referent?\n:::\n\nTreatment |TMS | Placebo| Total\n---|---|----|---\nPain-free two hours later |39| 22 |61\nNot pain-free two hours later |61 |78| 139\nTotal| 100| 100 |200\n\n* $\\Large OR = \\frac{61/39}{78/22} = \\frac{1.564}{3.545} = 0.441$\n* _the odds for still being in pain for the TMS group are 0.441 times the odds of being in pain for the placebo group_\n \n\n\n## Odds ratios {.small}\n\n::: question\nWhat changed here?\n:::\n\nTreatment |TMS | Placebo| Total\n---|---|----|---\nPain-free two hours later |39| 22 |61\nNot pain-free two hours later |61 |78| 139\nTotal| 100| 100 |200\n\n\n* $\\Large OR = \\frac{78/22}{61/39} = \\frac{3.545}{1.564} = 2.27$\n* _the odds for still being in pain for the placebo group are 2.27 times the odds of being in pain for the TMS group_\n \n\n\n\n## Odds ratios {.small}\n\nIn general, it's more natural to interpret odds ratios > 1, you can flip the referent to do so\n\nTreatment |TMS | Placebo| Total\n---|---|----|---\nPain-free two hours later |39| 22 |61\nNot pain-free two hours later |61 |78| 139\nTotal| 100| 100 |200\n\n\n$\\Large OR = \\frac{78/22}{61/39} = \\frac{3.545}{1.564} = 2.27$\n\n\n_the odds for still being in pain for the placebo group are 2.27 times the odds of being in pain for the TMS group_\n\n## Odds ratios {.small}\n\nLet's look at some Titanic data. We are interested in whether the  passenger reported being female is related to whether they survived.\n\n\n::: {.cell}\n\n:::\n\n\n||Female | Male | Total\n---------|-------|-----|-----\nSurvived| 308 | 142 | 450\nDied | 154 | 709| 863\nTotal | 462 | 851 | 1313\n\n\n## Odds ratios {.small}\n\n::: question\nWhat are the odds of surviving for females versus males?\n:::\n\n\n||Female | Male | Total\n---------|-------|-----|-----\nSurvived| 308 | 142 | 450\nDied | 154 | 709| 863\nTotal | 462 | 851 | 1313\n\n\n$$\\Large OR = \\frac{308/154}{142/709} = \\frac{2}{0.2} = 9.99$$\n\n\n## Odds ratios {.small}\n\n::: question\nHow do you interpret this?\n:::\n\n\n||Female | Male | Total\n---------|-------|-----|-----\nSurvived| 308 | 142 | 450\nDied | 154 | 709| 863\nTotal | 462 | 851 | 1313\n\n\n$$\\Large OR = \\frac{308/154}{142/709} = \\frac{2}{0.2} = 9.99$$\n_the odds of surviving for the female passengers was 9.99 times the odds of surviving for the male passengers_\n\n\n\n## Odds ratios {.small}\n\n::: question\nWhat if we wanted to fit a model? What would the equation be?\n:::\n\n\n||Female | Male | Total\n---------|-------|-----|-----\nSurvived| 308 | 142 | 450\nDied | 154 | 709| 863\nTotal | 462 | 851 | 1313\n\n. . .\n\n$$\\Large \\log(\\textrm{odds of survival}) = \\beta_0 + \\beta_1 \\textrm{Female}$$\n\n## Odds ratios {.small}\n\n$$\\Large \\log(\\textrm{odds of survival}) = \\beta_0 + \\beta_1 \\textrm{Female}$$\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(Survived ~ Sex, data = Titanic) |>\n  tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    -1.61    0.0919     -17.5 1.70e-68\n2 Sexfemale       2.30    0.135       17.1 2.91e-65\n```\n\n\n:::\n:::\n\n \n## Odds Ratios {.small}\n\n::: question\nHow do you interpret this result?\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(Survived ~ Sex, data = Titanic) |>\n  tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    -1.61    0.0919     -17.5 1.70e-68\n2 Sexfemale       2.30    0.135       17.1 2.91e-65\n```\n\n\n:::\n:::\n\n\n\n## Odds Ratios {.small}\n\n::: question\nHow do you interpret this result?\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"4\"}\nlogistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(Survived ~ Sex, data = Titanic) |>\n  tidy(exponentiate = TRUE) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    0.200    0.0919     -17.5 1.70e-68\n2 Sexfemale      9.99     0.135       17.1 2.91e-65\n```\n\n\n:::\n\n```{.r .cell-code  code-line-numbers=\"4\"}\nexp(2.301176)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 9.99\n```\n\n\n:::\n:::\n\n\n## Odds Ratios {.small}\n\n::: question\nHow do you interpret this result?\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(Survived ~ Sex, data = Titanic) |>\n  tidy(exponentiate = TRUE) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    0.200    0.0919     -17.5 1.70e-68\n2 Sexfemale      9.99     0.135       17.1 2.91e-65\n```\n\n\n:::\n\n```{.r .cell-code}\nexp(2.301176)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 9.99\n```\n\n\n:::\n:::\n\n\n\n_the odds of surviving for the female passengers was 9.99 times the odds of surviving for the male passengers_\n\n\n\n## Odds ratios {.small}\n\nWhat if the explanatory variable is continuous?\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(Acceptance ~ GPA, data = MedGPA) |>\n  tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   -19.2       5.63     -3.41 0.000644\n2 GPA             5.45      1.58      3.45 0.000553\n```\n\n\n:::\n:::\n\n\n\n_A one unit increase in GPA yields a 5.45 increase in the log odds of acceptance_\n\n\n\n## Odds ratios {.small}\n\nWhat if the explanatory variable is continuous?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(Acceptance ~ GPA, data = MedGPA) |>\n  tidy(exponentiate = TRUE) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  4.56e-9      5.63     -3.41 0.000644\n2 GPA          2.34e+2      1.58      3.45 0.000553\n```\n\n\n:::\n:::\n\n\n_A one unit increase in GPA yields a 234-fold increase in the odds of acceptance_\n\n* 😱 that seems huge! **Remember:** the interpretation of these coefficients depends on your units (the same as in ordinary linear regression).\n\n\n\n## Odds ratios {.small}\n\n::: question\nHow could we get the odds associated with increasing GPA by 0.1?\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(Acceptance ~ GPA, data = MedGPA) |>\n  tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   -19.2       5.63     -3.41 0.000644\n2 GPA             5.45      1.58      3.45 0.000553\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(5.454) ## a one unit increase in GPA\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 234\n```\n\n\n:::\n\n```{.r .cell-code}\nexp(5.454 * 0.1) ## a 0.1 increase in GPA\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.73\n```\n\n\n:::\n:::\n\n\n\n_A one-tenth unit increase in GPA yields a 1.73-fold increase in the odds of acceptance_\n\n\n\n## Odds ratios {.small}\n\n::: question\nHow could we get the odds associated with increasing GPA by 0.1?\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMedGPA <- MedGPA |>\n  mutate(GPA_10 = GPA * 10)\n\nlogistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(Acceptance ~ GPA_10, data = MedGPA) |>\n  tidy(exponentiate = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term             estimate std.error statistic  p.value\n  <chr>               <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept) 0.00000000456     5.63      -3.41 0.000644\n2 GPA_10      1.73              0.158      3.45 0.000553\n```\n\n\n:::\n:::\n\n\n\n_A one-tenth unit increase in GPA yields a 1.73-fold increase in the odds of acceptance_\n\n## `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 640 512\" style=\"height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M128 32C92.7 32 64 60.7 64 96V352h64V96H512V352h64V96c0-35.3-28.7-64-64-64H128zM19.2 384C8.6 384 0 392.6 0 403.2C0 445.6 34.4 480 76.8 480H563.2c42.4 0 76.8-34.4 76.8-76.8c0-10.6-8.6-19.2-19.2-19.2H19.2z\"/></svg>`{=html} `Application Exercise`\n\nUsing the `Default` data from the `ISLR` package. Fit two logistic regression models that predict whether a customer `defaults`\n\n- One model with `student` as a predictor. Interpret the coefficient of `studentYes`. \n- Another model with `balance` as a predictor. Interpret the coefficient of `balance`. \n\nHere is some code to get you started:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ISLR)\ndata(\"Default\")\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}