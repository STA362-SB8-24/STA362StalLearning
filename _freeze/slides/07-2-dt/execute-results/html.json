{
  "hash": "f6c1db4b3d6152f2ef424963c677ce3c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 8 Part 2\"\nsubtitle: \"Decision Trees\"\nformat: \n  revealjs:\n    output-file: \"07-2-dt.html\"\n    slide-number: true\n  html:\n    output-file: \"07-2-dt_o.html\"\neditor_options: \n  chunk_output_type: console\nlogo: \"img/icon.png\"\n---\n\n\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ISLR2)\nlibrary(rpart.plot)\nlibrary(partykit)\nlibrary(rattle)\n#install.packages('ranger')\n```\n:::\n\n\n\n\n# Graphs\n\n## Plotting decision trees\n\nThere are several R packages that assist with tree plotting\n\n* `rpart.plot`\n* `partykit`\n* `rattle`\n\n\n\n## Where to find out more about packages {.smaller}\n\n1. Vignettes\n2. Journal Article (R Journal is great)\n3. Rstudio Community\n4. StackOverflow\n5. Twitter\n\n## `rpart.plot`\n\nWe're going to focus on `rpart.plot`, but feel free to try the others!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"rpart.plot\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rpart.plot)\n```\n:::\n\n\n\n\n## `rpart.plot` {.smaller}\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_spec <- decision_tree(\n  cost_complexity = 0.1,\n  tree_depth = 10,\n  mode = \"regression\") |>\n  set_engine(\"rpart\")\n\nwf <- workflow() |>\n  add_recipe(\n    recipe(Salary ~ Hits + Years + PutOuts + RBI + Walks + Runs,\n                   data = baseball)\n  ) |>\n  add_model(tree_spec)\n\nmodel <- fit(wf, baseball)\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|1|2|\"}\nrpart.plot(model$fit$fit$fit,\n           roundint = FALSE)\n```\n:::\n\n\n\n\n## `rpart.plot`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrpart.plot(model$fit$fit$fit, \n           roundint = FALSE)\n```\n\n::: {.cell-output-display}\n![](07-2-dt_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n\n\n# Classification Trees\n\n## Classification Trees\n\n* Very similar to **regression trees** except it is used to predict a **qualitative response** rather than a **quantitative** one\n\n* We predict that each observation belongs to the **most commonly occuring class** of the training observations in a given region\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Fitting classification trees {.smaller}\n\n* We use **recursive binary splitting** to grow the tree\n* Instead of RSS, we can use:\n* **Gini index**: $G = \\sum_{k=1}^K \\hat{p}_{mk}(1-\\hat{p}_{mk})$\n\n* This is a measure of total variance across the $K$ classes. If all of the $\\hat{p}_{mk}$ values are close to zero or one, this will be small\n\n\n* The Gini index is a measure of node **purity** small values indicate that node contains predominantly observations from a single class\n\n\n* In `R`, this can be estimated using the `gain_capture()` function. \n\n\n## Classification tree - Heart Disease Example {.smaller}\n\n\n::: {.cell}\n\n:::\n\n\n* Classifying whether 303 patients have heart disease based on 13 predictors (`Age`, `Sex`, `Chol`, etc)\n\n\n\n## 1. Split the data into a cross-validation set\n\n\n::: {.cell}\n\n```{.r .cell-code}\nheart_cv <- vfold_cv(heart, v = 5)\n```\n:::\n\n\n\n\n::: question\nHow many folds do I have?\n:::\n\n\n## 2. Create a model specification that tunes based on complexity, $\\alpha$ {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|4|\"}\ntree_spec <- decision_tree(\n  cost_complexity = tune(), \n  tree_depth = 10,\n  mode = \"classification\") %>% \n  set_engine(\"rpart\")\n\nwf <- workflow() |>\n  add_recipe(\n    recipe(HD ~ Age + Sex + ChestPain + RestBP + Chol + Fbs + \n                     RestECG + MaxHR + ExAng + Oldpeak + Slope + Ca,\n    data = heart\n    )\n  ) |>\n  add_model(tree_spec)\n```\n:::\n\n\n\n\n## 3. Fit the model on the cross validation set {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|5|\"}\ngrid <- expand_grid(cost_complexity = seq(0.01, 0.05, by = 0.01))\nmodel <- tune_grid(wf,\n                   grid = grid,\n                   resamples = heart_cv,\n                   metrics = metric_set(gain_capture, accuracy)) \n```\n:::\n\n\n. . .\n\n::: question\nWhat $\\alpha$s am I trying?\n:::\n\n\n\n## 5. Choose $\\alpha$ that minimizes the Gini Index {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest <- model %>%\n  select_best(metric = \"gain_capture\")\n```\n:::\n\n\n\n\n## 6. Fit the final model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_wf <- wf |>\n  finalize_workflow(best)\n\nfinal_model <- fit(final_wf, data = heart)\n```\n:::\n\n\n\n\n## 7. Examine how the final model does on the full sample {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_model %>%\n  predict(new_data = heart) %>%\n  bind_cols(heart) %>%\n  conf_mat(truth = HD, estimate = .pred_class) %>%\n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](07-2-dt_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n\n\n## Decision trees\n\n:::: columns\n\n::: column\n### Pros\n\n* simple\n* easy to interpret\n:::\n\n\n\n::: column\n\n### Cons\n\n* not often competitive in terms of predictive accuracy\n* Next we will discuss how to combine _multiple_ trees to improve accuracy\n:::\n\n::::\n\n\n## <i class=\"fas fa-edit\"></i> `Try Classification Trees`\n\n- Fit a classification tree to predict species in the `penguins` data from the `palmerpenguins` packages. \n\n\n# Bagging\n\n## Bagging {.smaller}\n\n* **bagging** is a general-purpose procedure for reducing the variance of a statistical learning method (outside of just trees)\n\n* It is particularly useful and frequently used in the context of decision trees\n\n* Also called **bootstrap aggregation**\n\n\n## Bagging {.smaller}\n\n* Mathematically, why does this work? Let's go back to intro to stat!\n\n* If you have a set of $n$ independent observations: $Z_1, \\dots, Z_n$, each with a variance of $\\sigma^2$, what would the variance of the _mean_, $\\bar{Z}$ be?\n\n* The variance of $\\bar{Z}$ is $\\sigma^2/n$\n\n* In other words, **averaging a set of observations reduces the variance**.\n\n* This is generally not practical because we generally do not have multiple training sets\n\n\n## Bagging {.smaller}\n\n**Averaging a set of observations reduces the variance**. This is generally not practical because we generally do not have multiple training sets.\n\n::: question\nWhat can we do?\n:::\n\n\n* Bootstrap! We can take repeated samples from the single training data set.\n\n\n\n## Bagging process {.smaller}\n\n* generate $B$ different bootstrapped training sets\n\n\n* Train our method on the $b$th bootstrapped training set to get $\\hat{f}^{*b}(x)$, the prediction at point $x$\n\n\n* Average all predictions to get: $\\hat{f}_{bag}(x)=\\frac{1}{B}\\sum_{b=1}^B\\hat{f}^{*b}(x)$\n\n\n* This is **bagging**!\n\n\n\n## Bagging regression trees {.smaller}\n\n* generate $B$ different bootstrapped training sets\n* Fit a regression tree on the $b$th bootstrapped training set to get $\\hat{f}^{*b}(x)$, the prediction at point $x$\n* Average all predictions to get: $\\hat{f}_{bag}(x)=\\frac{1}{B}\\sum_{b=1}^B\\hat{f}^{*b}(x)$\n\n\n\n## Bagging classification trees {.smaller}\n\n* for each test observation,  record the class predicted by the $B$ trees\n\n\n* Take a **majority** vote - the overall prediction is the most commonly occuring class among the $B$ predictions\n\n\n\n## Out-of-bag Error Estimation {.smaller}\n\n* You can estimate the **test error** of a bagged model\n\n\n* The key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations\n\n\n* On average, each bagged tree makes use of about 2/3 of the observations (you can prove this if you'd like!, not required for this course though)\n\n\n* The remaining 1/3 of observations _not_ used to fit a given bagged tree are the **out-of-bag** (OOB) observations\n\n\n\n## Out-of-bag Error Estimation {.smaller}\n\n\nYou can predict the response for the $i$th observation using each of the trees in which that observation was OOB\n\n:::question\nHow many predictions do you think this will yield for the $i$th observation?\n:::\n\n\n* This will yield $B/3$ predictions for the $i$th observations. We can _average_ this!\n\n\n\n* This estimate is essentially the LOOCV error for bagging as long as $B$ is large üéâ\n\n## Bagging (vs Boosting) Video\n\n<https://www.youtube.com/watch?v=tjy0yL1rRRU&t=4s>\n\n## <i class=\"fas fa-edit\"></i> `Describing Bagging`\n\nSee if you can _draw a diagram_ to describe the bagging process to someone who has never heard of this before.\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"countdown\" id=\"timer_df1842a6\" data-update-every=\"1\" tabindex=\"0\" style=\"right:0;bottom:0;\">\n<div class=\"countdown-controls\"><button class=\"countdown-bump-down\">&minus;</button><button class=\"countdown-bump-up\">&plus;</button></div>\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n\n:::\n:::\n\n\n# Random Forests {.smaller}\n\n\n_Do you_ ‚ù§Ô∏è _all of the tree puns?_\n\n::: question\nIf we are using bootstrap samples, how similar do you think the trees will be?\n::: \n. . .\n\n* Random forests provide an improvement over bagged trees\nby way of a small tweak that _decorrelates_ the trees\n\n\n* By _decorrelating_ the trees, this reduces the variance even more when we average the trees!\n\n\n\n## Random Forest process {.smaller}\n\n* Like bagging, build a number of decision trees on\nbootstrapped training samples\n\n\n* Each time the tree is split, instead of considering _all predictors_ (like bagging), **a random selection of** $m$ **predictors** is chosen as split candidates from the full set of $p$ predictors\n* The split is allowed to use only one of those $m$ predictors\n\n\n* A fresh selection of $m$ predictors is taken at each split \n\n\n* typically we choose $m \\approx \\sqrt{p}$\n\n\n\n\n## <i class=\"fas fa-edit\"></i> `Choosing m for Random Forest`\n\nLet's say you have a dataset with 100 observations and 9 variables, if you were fitting a random forest, what would a good $m$ be?\n\n\n# The heart disease example \n\n_We are predicting whether a patient has heart disease from 13 predictors_\n\n\n## 1. Randomly divide the data in half, 149 training observations, 148 testing {.smaller}\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(77)\nheart_split <- initial_split(heart, prop = 0.5)\nheart_train <- training(heart_split)\n```\n:::\n\n\n\n\n## 2. Create model specification {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_spec <- rand_forest(\n  mode = \"classification\",\n  mtry = ---\n) |> \n  set_engine(\"ranger\")\n```\n:::\n\n\n. . .\n\n::: question\n_mtry_ here is _m_. If we are doing _bagging_ what do you think we set this to? \n:::\n\n\n\n## 2. Create bagging specification\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbagging_spec <- rand_forest(\n  mode = \"classification\",\n  mtry = 13 #<<\n) |> \n  set_engine(\"ranger\")\n```\n:::\n\n\n\n\n::: question\nWhat would we change _mtry_ to if we are doing a random forest?\n:::\n\n\n\n## 2. Create Random Forest specification\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_spec <- rand_forest(\n  mode = \"classification\",\n  mtry = 3 #<<\n) |> \n  set_engine(\"ranger\")\n```\n:::\n\n\n\n\n* The default for `rand_forest` is `floor(sqrt(# predictors))` (so 3 in this case)\n\n## 3. Create the workflow {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf <- workflow() |>\n  add_recipe(\n    recipe(\n      HD ~ Age + Sex + ChestPain + RestBP + Chol + Fbs + \n               RestECG + MaxHR + ExAng + Oldpeak + Slope + Ca + Thal,\n             data = heart_train\n    )\n  ) |>\n  add_model(rf_spec)\n```\n:::\n\n\n\n## 4. Fit the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- fit(wf, data = heart_train)\n```\n:::\n\n\n\n\n## 5. Examine how it looks in the test data {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nheart_test <- testing(heart_split)\nmodel |>\n  predict(new_data = heart_test) |>\n  bind_cols(heart_test) |>\n  conf_mat(truth = HD, estimate = .pred_class) |>\n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](07-2-dt_files/figure-revealjs/unnamed-chunk-25-1.png){width=960}\n:::\n:::\n\n\n\n## Trade Off {.smaller}\n\n::: question\nWhat is our final tree? \n:::\n\n. . . \n\n- With both bagging and random forests, we have traded interpretability with performance. \n\n- These approaches will predict better but we no longer have a single represenation fo the tree. \n\n- Even if we wanted to pick the best performing tree, it may have a different subset of variables than other similar trees. \n\n## <i class=\"fas fa-laptop\"></i> `Application Exercise`\n\n* Open your last application exercise\n* Refit your model as a _bagged tree_ and a _random forest_\n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/countdown-0.4.0/countdown.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/countdown-0.4.0/countdown.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}