{
  "hash": "119bbf895acc97a32072763b7e0e8970",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 4 Part 2\"\nsubtitle: \"Multinomial Logisitc, LDA, QDA\"\nformat: \n  revealjs:\n    slide-number: true\n    chalkboard: true\n---\n\n \n## Recap {.small}\n\n* We had a _logistic regression_ refresher\n\n### Now...\n\n* What if our response has more than two levels? \n* What if logistic regression is a poor fit? \n\n## Setup \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR)\n```\n:::\n\n\n\n## Multinomial Logistic\n\n- So far we have discussed logistic regression with two classes. \n\n- It is easily generalized to more than two classes. \n\n\n## Confounding {.small}\n\nRecall our defaults data with variable `default`, `student`, and `balance`\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04_2-lda_qda_files/figure-revealjs/plot3-1.png){width=960}\n:::\n:::\n\n\n:::question\nWhat is going on here?\n:::\n\n\n## Confounding {.small}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04_2-lda_qda_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n* Students tend to have higher balances than non-students\n* Their **marginal** default rate is higher\n* For each level of balance, students default less \n* Their **conditional** default rate is lower\n\n\n\n## Logistic regression for more than two classes {.small}\n\n$$P(Y=k|X) = \\frac{e ^{\\beta_{0k}+\\beta_{1k}X_1+\\dots+\\beta_{pk}X_p}}{\\sum_{l=1}^Ke^{\\beta_{0l}+\\beta_{1l}X_1+\\dots+\\beta_{pl}X_p}}$$\n\n* We generalize this to situations with **multiple** classes\n* Here we have a linear function for **each** of the $K$ classes\n* This is known as **multinomial logistic regression**\n\n\n## Multiple logistic regression {.small}\n\n$$\\log\\left(\\frac{p(X)}{1-p(X)}\\right)=\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p$$\n$$p(X) = \\frac{e^{\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p}}{1+e^{\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p}}$$\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> -10.8690452 </td>\n   <td style=\"text-align:right;\"> 0.4922555 </td>\n   <td style=\"text-align:right;\"> -22.080088 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> balance </td>\n   <td style=\"text-align:right;\"> 0.0057365 </td>\n   <td style=\"text-align:right;\"> 0.0002319 </td>\n   <td style=\"text-align:right;\"> 24.737563 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> income </td>\n   <td style=\"text-align:right;\"> 0.0000030 </td>\n   <td style=\"text-align:right;\"> 0.0000082 </td>\n   <td style=\"text-align:right;\"> 0.369815 </td>\n   <td style=\"text-align:right;\"> 0.7115203 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> studentYes </td>\n   <td style=\"text-align:right;\"> -0.6467758 </td>\n   <td style=\"text-align:right;\"> 0.2362525 </td>\n   <td style=\"text-align:right;\"> -2.737646 </td>\n   <td style=\"text-align:right;\"> 0.0061881 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n* Why is the coefficient for `student` negative now when it was positive before?\n\n## LDA Warmup\n\nTo give us a general overview, we are going to watch the StatQuest video on the topic: <https://www.youtube.com/watch?v=azXCzI57Yfc>\n\n## Discriminant Analysis\n\n- Here the approach is to model the distribution of X in each of the classes separately, and then use Bayes theorem to flip things around and obtain $P(Y jX)$. \n\n- When we use normal (Gaussian) distributions for each class, this leads to linear or quadratic discriminant analysis.\n\n- However, this approach is quite general, and other distributions can be used as well. We will focus on normal distributions.\n\n## Why? \n\n- When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.\n\n- If n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\n\n- Linear discriminant analysis is popular when we have more than two response classes, because it also provides low-dimensional views of the data.\n\n## Bayes Theorem (classification)\n\nThomas Bayes was a famous mathematician whose name represents a big subfield of statistical and probabilistic modeling. Here we focus on a simple result, known as Bayes theorem:\n\n$$P(Y=k|X=x) = \\frac{P(X=x|Y=k)\\cdot P(Y=k)}{P(X=x)}$$\n\n\n## Bayes for Discriminant Analysis\n\n\n$$P(Y=k|X=x) = \\frac{\\pi_kf_k(x)}{\\sum_{l=1}^K\\pi_lf_l(x)}$$, where\n\n- $f_k(x)=P(X=x|Y=k)$ is the *density* for $X$ in class $k$. Here we use normal's but they could be other distributions (such as $\\Chi^2$)\n\n- $\\pi_k = P(Y=k)$ is the marginal or prior probability for class $k$. \n\n## Classify to the highest density\n\n:::: {.columns}\n\n::: {.column}\n$$\\pi_1=.5, \\pi_2=.5$$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04_2-lda_qda_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n\n:::\n\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04_2-lda_qda_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::::\n\n- We classify a new point according to which density is highest.\n\n- When the priors are different, we take them into account as well, and compare $\\pi_kf_k(x)$. \n\n- On the right, we favor the pink class - the decision boundary has shifted to the left.\n\n## Linear Descriminant Analysis (when $p=1$)\n\nThe Gaussian (normal) density has the form\n\n$$f_k(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k}e^{-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma_k})^2}$$\n\n- $\\mu_k$ is the mean, $\\sigma_k^2$ the variance (in class $k$)\n\n- For now, we assume $\\sigma_k=\\sigma$ for all groups (we will need to check this with real data)\n\n## Linear Descriminant Analysis (when $p=1$)\n\nWe plug this $f_k(x)$ into Bayes formula and after some simplifying we get:\n\n$$p_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma_k})^2}}{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma_k})^2}}$$\n\n## Discriminant Function (when $p=1$)\n\nTo classify at the value X = x, we need to see which of the $p_k(x)$ is largest. Taking logs, and discarding terms that do not depend on $k$, we see that this is equivalent to assigning x to the class with the largest *discriminant* score:\n\n$$\\delta_k(x) = x\\cdot \\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+log(\\pi_k)$$\n\n- Importantly, $\\delta_k(x)$ is a *linear* function of $x$.\n\n- If there are $K=2$ classes and $\\pi_1=\\pi_2=.5$, then the *decision boundry* is at \n\n$$x=\\frac{\\mu_1+\\mu_2}{2}$$\n\n\n\n## Example\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04_2-lda_qda_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}