{
  "hash": "9e4f55510281855ac9db43a96f87d4b1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 4 Part 3\"\nsubtitle: \"KMeans, Naive Bayes\"\nformat: \n  revealjs:\n    slide-number: true\n    chalkboard: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n## Setup \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR)\nlibrary(Stat2Data)\nlibrary(discrim)\n#install.packages(\"kknn\")\n#install.packages(\"klaR\")\n```\n:::\n\n\n# K-Nearest Neighbors\n\n## KNN {.smaller}\n\n- In theory we would always like to predict qualitative responses using the Bayes classifier. \n- For real data, we do not know the conditional distribution of Y given X, and so computing the Bayes classifier is impossible.\n- Many approaches attempt to estimate the conditional distribution of Y given X, and then classify a given observation to the class with highest estimated probability. One such method is the K-nearest neighbors (KNN) classifier.\n\n## KNN {.smaller}\n\n- Given positive integer $K$, and a test observation $x_0$, KNN identifies the $K$ points in the training data that are closest to $x_0$, represented by $\\mathcal{N}_0$. \n\n- KNN then estimates the conditional probabilities for each class $j$ as the fraction of the points in $\\mathcal{N}_0$ whose response values equal $j$:\n\n- $$P(Y=j|X=x_0)=\\frac{1}{K}\\sum_{i\\in \\mathcal{N}}I(y_i=j)$$\n\n- Lastly, KNN classifies $x_0$ into the class with the largest probability\n\n## KNN Example {.smaller}\n\nThe KNN approach, using $K = 3$, is illustrated in a situation with six blue observations and six orange observations.\n\n::::{.columns}\n\n:::{.column}\n\n![](img/KNNClass1.png)\n\n:::\n\n:::{.column}\nA test observation, $x_0$, at which a predicted class label is desired is shown as a black cross.\n\n\nThe three closest points to the test observation are identified, and it is predicted that the test observation belongs to the most commonly-occurring class, in this case blue.\n  \n:::\n::::\n\n## KNN Example {.smaller}\n\nThe KNN approach, using $K = 3$, is illustrated in a situation with six blue observations and six orange observations.\n\n::::{.columns}\n\n:::{.column}\n\n![](img/KNNClass1_2.png)\n\n:::\n\n:::{.column}\n  \nThe KNN decision boundary for this example is shown in black. The blue grid indicates the region in which a test observation will be assigned to the blue class, and the orange grid indicates the region in which it will be assigned to the orange class.\n\n:::\n::::\n\n## KNN Example KNN vs Bayes {.smaller}\n\n::::{.columns}\n\n:::{.column}\n\n- The black curve indicates the KNN decision boundary, using $K = 10$. The Bayes decision boundary is shown as a purple dashed line. \n\n- The KNN and Bayes decision boundaries are very similar.\n\n:::\n\n:::{.column}\n![](img/KNNClassK10.png)\n:::\n::::\n\n## KNN Low and High Flexibility {.smaller}\n\n- Comparison of KNN decision boundaries (solid black curves) obtained using $K = 1$ and $K = 100$. \n  -   The $K = 1$ decision boundary is overly flexible and the $K = 100$ boundary is not sufficiently flexible. \n- The Bayes decision boundary is shown as a purple dashed line.\n\n![](img/KNNClassK1vs100.png)\n\n## Statquest\n\n<https://www.youtube.com/watch?v=HVXime0nQeI&t=53s>\n\n\n## KMeans Classification Tidymodels\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"6|8\"}\nset.seed(14546)\ndef_splits <- initial_split(Default,.7)\n\ndefault_training <- training(def_splits)\n\nknn_fit_1 <- nearest_neighbor() |>\n  set_mode(\"classification\")|>\n  set_engine(\"kknn\")|> \n    fit(default ~ balance + student,\n      data = default_training)\n```\n:::\n\n\n## KMeans Classification Tidymodels\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_testing <- testing(def_splits)\n\nknn_fit_1 |>\n  augment(new_data = default_testing) |>\n  conf_mat(truth = default, estimate = .pred_class) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction   No  Yes\n       No  2865   69\n       Yes   26   40\n```\n\n\n:::\n\n```{.r .cell-code}\nknn_fit_1 |>\n  augment(new_data = default_testing) |>\n  accuracy(truth = default,estimate=.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.968\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## KMeans ROC Curve\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_fit_1 |>\n  augment(new_data = default_testing) |>\n  roc_curve(truth = default,.pred_No) |>\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](04-3-km-nb_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n## KMeans ROC AUC\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_fit_1 |>\n  augment(new_data = default_testing) |>\n  roc_auc(truth = default,.pred_No)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.844\n```\n\n\n:::\n:::\n\n\n\n# Naive Bayes\n\n## Naive Bayes {.smaller}\n\n- In LDA and QDA we assumed the predictors were Normal\n  - This allow us to find the density functions $f_k(x)$'s by optimizing a linear or quadratic function $\\delta_k(x)$. \n  - With LDA, we assume the predictors have the same covariance structure between classes\n  - With QDA, $X_j$s can have any covariance structure\n  - With both, the predictors must be quantitative\n\n## Naive Bayes {.smaller}\n\n- With Naive Bayes we drop the normality assumption but introduce a must stronger assumption\n  -   Within a class $k$, \n  -   The predictors are independent\n  \n## Naive Bayes New Assumptions\n\n- These two independence assumptions allows us to write, for $k=1,2,...,K$,\n\n- $$f_k(x) = f_{k1}(x)f_{k2}(x)\\cdot \\cdot \\cdot f_{kp}$$\n\n  -   where $f_{kj}$ is the density function for the $j$th predictor among observations in the $k$th class. \n\n## Naive Bayes Probability Function \n\nMaking these assumption, we now have:\n\n$$P(Y=k|X=x) = \\frac{\\pi_k\\cdot f_{k1}(x_1)\\cdot \\cdot \\cdot f_{kp}(x_p)}{\\sum_{l=1}^K\\pi_l\\cdot f_{l1}(x_1)\\cdot \\cdot \\cdot f_{lp}(x_p)}$$\nfor $k=1,2,...,K$\n\n## Naive Bayes - Why? {.smaller}\n\n- Each $f$ is one dimensional!\n- If $X_j$ is *quantitative* can still assume each $X_j|Y=k$ is univariate normal\n- If $X_j$ is *quantitative*, then another option is to use a non-parametric estimate for $f_{kj}$\n  -   Make a histogram for the observations of the $j$th predictor within each class.\n  -   Then we can estimate $f_{kj}(x_j)$ as the fraction of the training observations in the $k$th class that belong to the same histogram bin as $x_j$ . \n  -   We can use a kernel density estimator, which is kernel density estimator (essentially a smoothed version of a histogram)\n\n## Naive Bayes - Why? {.smaller}\n\n- If $X_j$ is *qualitative*, then we can simply count the proportion of training observations for the $j$th predictor corresponding to each class. \n  -   Suppose that $X_j \\in \\{1, 2, 3\\}$, and we have 100 observations in the $k$th class. \n  -   Suppose that the $j$th predictor takes on values of 1,2, and 3 in 32, 55, and 13 of those observations, respectively. Then we can estimate $f_{kj}$ as\n\n- $$\\hat{f}_{kj}(x_j) = \\begin{cases}\n  .32 \\quad \\text{if }x_j = 1\\\\\n  .55 \\quad \\text{if }x_j = 2\\\\\n  .13 \\quad \\text{if }x_j = 3\\\\\n  \\end{cases}$$\n\n## Naive Bayes - Statsquest {.smaller}\n\n<https://www.youtube.com/watch?v=O2L2Uv9pdDA>\n\n## Naive Bayes Tidymodels\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"1|3\"}\nnb_fit_1 <- naive_Bayes() |>\n  set_mode(\"classification\")|>\n  set_engine(\"klaR\")|> \n    fit(default ~ balance + student,\n      data = default_training)\n```\n:::\n\n\n## Naive Bayes Confusion Matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnb_fit_1 |>\n  augment(new_data = default_testing) |>\n  conf_mat(truth = default, estimate = .pred_class) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction   No  Yes\n       No  2880   76\n       Yes   11   33\n```\n\n\n:::\n:::\n\n\n## Naive Bayes Accuracy\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnb_fit_1 |>\n  augment(new_data = default_testing) |>\n  accuracy(truth = default,estimate=.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.971\n```\n\n\n:::\n:::\n\n\n## Naive Bayes ROC Curve\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnb_fit_1 |>\n  augment(new_data = default_testing) |>\n  roc_curve(truth = default,.pred_No) |>\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](04-3-km-nb_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n## Naive Bayes ROC AUC\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnb_fit_1 |>\n  augment(new_data = default_testing) |>\n  roc_auc(truth = default,.pred_No)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.952\n```\n\n\n:::\n:::\n\n\n## Naive Bayes Statquest\n\n<https://www.youtube.com/watch?v=O2L2Uv9pdDA&t=1s>\n\n## Comparing LDA, QDA, and Normal Naive Bayes {.smaller}\n\n<https://towardsdatascience.com/differences-of-lda-qda-and-gaussian-naive-bayes-classifiers-eaa4d1e999f6>\n\nNaive Bayes\n- Given Y, the predictors X are conditionally independent.\n\nLDA\n- LDA assumes that the covariance matrix across classes is the same.\n\nQDA\n- QDA does not assume constant covariance matrix across classes.",
    "supporting": [
      "04-3-km-nb_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}