{
  "hash": "59afbbed9d6b01e620c1e94db8d3aaae",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 3 - Linear Regression\"\nformat: \n  revealjs:\n    output-file: \"03-linear_regression.html\"\n    slide-number: true\n  html:\n    output-file: \"03-linear_regression_o.html\"\n---\n\n\n## {{< fa laptop >}} `Application Exercise` {.small}\n\n::: nonincremental\n\nCreate a new `quarto` file for this homework in your exercises R project. \n\n:::\n\n## Let's look at an example {.small}\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\nLet's look at a sample of 116 sparrows from Kent Island. We are interested in the relationship between `Weight` and `Wing Length`\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-linear-regression_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n* the **standard error** of $\\hat{\\beta_1}$ ( $SE_{\\hat{\\beta}_1}$ ) is how much we expect the sample slope to vary from one random sample to another.\n\n\n## Sparrows\n\n::: question\nHow can we quantify how much we'd expect the slope to differ from one random sample to another?\n:::\n\n\n\n::: {.cell highlight.output='5'}\n\n```{.r .cell-code  code-line-numbers=\"4\"}\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Weight ~ WingLength, data = Sparrows) |>\n  tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1\n2 WingLength     0.467    0.0347     13.5  2.62e-25\n```\n\n\n:::\n:::\n\n\n\n\n\n## Sparrows\n\n::: question\nHow do we interpret this?\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Weight ~ WingLength, data = Sparrows) |>\n  tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1\n2 WingLength     0.467    0.0347     13.5  2.62e-25\n```\n\n\n:::\n:::\n\n\n\n* \"the sample slope is more than 13 standard errors above a slope of zero\"\n\n\n## Sparrows {.small}\n\n::: question\nHow do we know what values of this statistic are worth paying attention to?\n:::\n\n. . .\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"4\"}\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Weight ~ WingLength, data = Sparrows) |>\n  tidy(conf.int = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1   -0.531     3.26 \n2 WingLength     0.467    0.0347     13.5  2.62e-25    0.399     0.536\n```\n\n\n:::\n:::\n\n\n* confidence intervals\n* p-values\n\n\n## `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 640 512\" style=\"height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M128 32C92.7 32 64 60.7 64 96V352h64V96H512V352h64V96c0-35.3-28.7-64-64-64H128zM19.2 384C8.6 384 0 392.6 0 403.2C0 445.6 34.4 480 76.8 480H563.2c42.4 0 76.8-34.4 76.8-76.8c0-10.6-8.6-19.2-19.2-19.2H19.2z\"/></svg>`{=html} `Application Exercise` {.small}\n\n::: nonincremental\n1. Fit a linear model using the `mtcars` data frame predicting miles per gallon (`mpg`) from weight and horsepower (`wt` and `hp`).\n2. Pull out the coefficients and confidence intervals using the `tidy()` function demonstrated. How do you interpret these?\n:::\n\n\n## Sparrows\n\n::: question\nHow are these statistics distributed under the null hypothesis?\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Weight ~ WingLength, data = Sparrows) |>\n  tidy() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1\n2 WingLength     0.467    0.0347     13.5  2.62e-25\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n## Sparrows\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-linear-regression_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n* I've generated some data under a null hypothesis where $n = 20$\n\n\n\n## Sparrows\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-linear-regression_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n* this is a **t-distribution** with **n-p-1** degrees of freedom.\n\n\n\n## Sparrows\n\nThe distribution of test statistics we would expect given the **null hypothesis is true**, $\\beta_1 = 0$, is **t-distribution** with **n-2** degrees of freedom.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-linear-regression_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n\n\n## Sparrows\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-linear-regression_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n\n\n## Sparrows\n\n::: question\nHow can we compare this line to the distribution under the null?\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-linear-regression_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n* p-value\n\n\n# p-value \n\nThe probability of getting a statistic as extreme or more extreme than the observed test statistic **given the null hypothesis is true**\n\n\n\n## Sparrows\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-linear-regression_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n\n::: small\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Weight ~ WingLength, data = Sparrows) |>\n  tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1\n2 WingLength     0.467    0.0347     13.5  2.62e-25\n```\n\n\n:::\n:::\n\n:::\n\n\n\n## Return to generated data, n = 20\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-linear-regression_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n* Let's say we get a statistic of **1.5** in a sample\n\n\n## Let's do it in R!\n\nThe proportion of area less than 1.5\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-linear-regression_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npt(1.5, df = 18)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9245248\n```\n\n\n:::\n:::\n\n\n\n\n## Let's do it in R!\n\nThe proportion of area **greater** than 1.5\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-linear-regression_files/figure-revealjs/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npt(1.5, df = 18, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.07547523\n```\n\n\n:::\n:::\n\n\n\n## Let's do it in R!\n\nThe proportion of area **greater** than 1.5 **or** **less** than -1.5.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-linear-regression_files/figure-revealjs/unnamed-chunk-23-1.png){width=960}\n:::\n:::\n\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\npt(1.5, df = 18, lower.tail = FALSE) * 2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1509505\n```\n\n\n:::\n:::\n\n\n\n\n\n\n# p-value {.center}\n\nThe probability of getting a statistic as extreme or more extreme than the observed test statistic **given the null hypothesis is true**\n\n\n\n## Hypothesis test\n\n* **null hypothesis** $H_0: \\beta_1 = 0$ \n* **alternative hypothesis** $H_A: \\beta_1 \\ne 0$\n* **p-value**: 0.15\n* Often, we have an $\\alpha$-level cutoff to compare this to, for example **0.05**. Since this is greater than **0.05**, we **fail to reject the null hypothesis**\n\n\n\n\n## `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 640 512\" style=\"height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M128 32C92.7 32 64 60.7 64 96V352h64V96H512V352h64V96c0-35.3-28.7-64-64-64H128zM19.2 384C8.6 384 0 392.6 0 403.2C0 445.6 34.4 480 76.8 480H563.2c42.4 0 76.8-34.4 76.8-76.8c0-10.6-8.6-19.2-19.2-19.2H19.2z\"/></svg>`{=html} `Application Exercise` {.small}\n\n::: nonincremental\n1. Using the linear model you fit previously (`mpg` from `wt` and `hp`) - calculate the p-value for the coefficient for weight\n2. Interpret this value. What is the null hypothesis? What is the alternative hypothesis? Do you reject the null?\n:::\n\n\n\n# confidence intervals {.center}\n\nIf we use the same sampling method to select different samples and computed an interval estimate for each sample, we would expect the true population parameter ( $\\beta_1$ ) to fall within the interval estimates 95% of the time.\n\n\n\n# Confidence interval\n\n\n$$\\Huge \\hat\\beta_1 \\pm t^∗ \\times SE_{\\hat\\beta_1}$$\n\n* $t^*$ is the critical value for the $t_{n−p-1}$ density curve to obtain the desired confidence level\n* Often we want a **95% confidence level**.  \n\n\n\n## Let's do it in R! {.small}\n\n\n\n::: {.cell highlight.output='5'}\n\n```{.r .cell-code}\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Weight ~ WingLength, data = Sparrows) |>\n  tidy(conf.int = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1   -0.531     3.26 \n2 WingLength     0.467    0.0347     13.5  2.62e-25    0.399     0.536\n```\n\n\n:::\n:::\n\n\n\n* $t^* = t_{n-p-1} = t_{114} = 1.98$\n* $LB = 0.47 - 1.98\\times 0.0347 = 0.399$\n* $UB = 0.47+1.98 \\times 0.0347 = 0.536$\n\n\n\n\n# confidence intervals \n\nIf we use the same sampling method to select different samples and computed an interval estimate for each sample, we would expect the true population parameter ( $\\beta_1$ ) to fall within the interval estimates 95% of the time.\n\n\n\n## Linear Regression Questions\n\n* ✔️ Is there a relationship between a response variable and predictors? \n* ✔️ How strong is the relationship? \n* ✔️ What is the uncertainty? \n* How accurately can we predict a future outcome?\n\n\n\n## Sparrows {.small}\n\n::: question\nUsing the information here, how could I predict a new sparrow's weight if I knew the wing length was 30?\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Weight ~ WingLength, data = Sparrows) |>\n  tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1\n2 WingLength     0.467    0.0347     13.5  2.62e-25\n```\n\n\n:::\n:::\n\n\n* $1.37 + 0.467 \\times 30 = 15.38$\n\n\n\n## Linear Regression Accuracy {.small}\n \n::: question\nWhat is the residual sum of squares again?\n:::\n\n* Note: In previous classes, this may have been referred to as SSE (sum of squares error), the book uses RSS, so we will stick with that!\n\n. . .\n\n$$RSS = \\sum(y_i - \\hat{y}_i)^2$$\n\n## Linear Regression Accuracy {.small}\n\n::: nonincremental\n* The **total sum of squares** represents the variability of the outcome, it is equivalent to the variability described by the **model** plus the remaining **residual sum of squares**\n\n$$TSS = \\sum(y_i - \\bar{y})^2$$\n:::\n\n## Linear Regression Accuracy {.small}\n\n* There are many ways \"model fit\" can be assessed. Two common ones are:\n  * Residual Standard Error (RSE)\n  * $R^2$ - the fraction of the variance explained\n* $RSE = \\sqrt{\\frac{1}{n-p-1}RSS}$\n* $R^2 = 1 - \\frac{RSS}{TSS}$\n\n\n\n## Let's do it in R!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit <- linear_reg() |> \n  set_engine(\"lm\") |>\n  fit(Weight ~ WingLength, data = Sparrows)\n\nlm_fit |>\n  predict(new_data = Sparrows) |>\n  bind_cols(Sparrows) |>\n  rsq(truth = Weight, estimate = .pred) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.614\n```\n\n\n:::\n:::\n\n\n. . .\n\n::: question\nIs this testing $R^2$ or training $R^2$?\n:::\n\n\n## `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 640 512\" style=\"height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M128 32C92.7 32 64 60.7 64 96V352h64V96H512V352h64V96c0-35.3-28.7-64-64-64H128zM19.2 384C8.6 384 0 392.6 0 403.2C0 445.6 34.4 480 76.8 480H563.2c42.4 0 76.8-34.4 76.8-76.8c0-10.6-8.6-19.2-19.2-19.2H19.2z\"/></svg>`{=html} `Application Exercise` {.small}\n\n::: nonincremental\n1. Fit a linear model using the `mtcars` data frame predicting miles per gallon (`mpg`) from weight and horsepower (`wt` and `hp`), using polynomials with 4 degrees of freedom for both.\n2. Estimate the training $R^2$ using the `rsq` function. \n3. Interpret this values.\n:::\n\n\n## `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 640 512\" style=\"height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M128 32C92.7 32 64 60.7 64 96V352h64V96H512V352h64V96c0-35.3-28.7-64-64-64H128zM19.2 384C8.6 384 0 392.6 0 403.2C0 445.6 34.4 480 76.8 480H563.2c42.4 0 76.8-34.4 76.8-76.8c0-10.6-8.6-19.2-19.2-19.2H19.2z\"/></svg>`{=html} `Application Exercise` {.small}\n\n::: nonincremental\n1. Create a cross validation object to do 5 fold cross validation using the `mtcars` data\n2. Refit the model on this object (using `fit_resamples`)\n3. Use `collect_metrics` to estimate the test $R^2$ - how does this compare to the training $R^2$ calculated in the previous exercise?\n\n:::\n\n\n## Additional Linear Regression Topics\n\n* Polynomial terms\n* Interactions\n* Outliers\n* Non-constant variance of error terms\n* High leverage points\n* Collinearity\n\n_Refer to Chapter 3 for more details on these topics if you need a refresher._\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}