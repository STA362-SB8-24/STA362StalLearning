{
  "hash": "2fe2c6c80a6e75e9807b5041e55426c6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 8 Part 3\"\nsubtitle: \"Boosting Trees\"\nformat: \n  revealjs:\n    output-file: \"07-3-boost.html\"\n    slide-number: true\n  html:\n    output-file: \"07-3-boost_o.html\"\neditor_options: \n  chunk_output_type: console\nlogo: \"img/icon.png\"\n---\n\n\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ISLR2)\nlibrary(rpart.plot)\n#install.packages('xgboost')\n```\n:::\n\n\n\n\n\n## Boosting {.smaller}\n\n* Like **bagging**, **boosting** is an approach that can be applied to many statistical learning methods\n\n\n* We will discuss how to use **boosting** for decision trees\n\n\n\n\n\n## Bagging {.smaller}\n\n* resampling from the original training data to make many bootstrapped training data sets \n* fitting a separate decision tree to each bootstrapped training data set\n* combining all trees to make one predictive model\n* ☝️ Note, each tree is built on a bootstrap dataset, independent of the other trees\n\n## Boosting  {.smaller}\n\n* **Boosting** is similar, except the trees are grown _sequentially_, using information from the previously grown trees\n\n\n\n## Boosting algorithm for regression trees {.smaller}\n\n### Step 1\n\n* Set $\\hat{f}(x)= 0$ and $r_i= y_i$ for all $i$ in the training set\n\n\n\n## Boosting algorithm for regression trees  {.smaller}\n\n### Step 2 For $b = 1, 2, \\dots, B$ repeat:\n\n* Fit a tree $\\hat{f}^b$ with $d$ splits ( $d$ + 1 terminal nodes) to the training data ( $X, r$ )\n* Update $\\hat{f}$ by adding in a shrunken version of the new tree: $\\hat{f}(x)\\leftarrow \\hat{f}(x)+\\lambda \\hat{f}^b(x)$\n\n* Update the residuals: $r_i \\leftarrow r_i - \\lambda \\hat{f}^b(x_i)$\n\n## Boosting algorithm for regression trees  {.smaller}\n\n### Step 3\n\n* Output the boosted model $\\hat{f}(x)=\\sum_{b = 1}^B\\lambda\\hat{f}^b(x)$\n\n\n## Big picture  {.smaller}\n\n* Given the current model, we are fitting a decision tree to the _residuals_\n\n\n* We then add this new decision tree into the fitted function to update the residuals\n\n\n* Each of these trees can be small (just a few terminal nodes), determined by $d$\n\n\n* Instead of fitting a single large decision tree, which could result in overfitting, boosting _learns slowly_\n\n\n\n## Big Picture  {.smaller}\n\n\n* By fitting small trees to the _residuals_ we _slowly_ improve $\\hat{f}$ in areas where it does not perform well\n\n\n* The shrinkage parameter $\\lambda$ slows the process down even more allowing more and different shaped trees to try to minimize those residuals\n\n\n\n## Boosting for classification  {.smaller}\n\n* Boosting for classification is similar, but a bit more complex\n\n\n* `tidymodels` will handle this for us, but if you are interested in learning more, you can check out [Chapter 10 of Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)\n\n\n\n\n## Tuning parameters  {.smaller}\n\n::: question\nWith **bagging** what could we tune?\n:::\n\n\n\n* $B$, the number of bootstrapped training samples (the number of decision trees fit) (`trees`)\n\n\n* It is more efficient to just pick something very large instead of tuning this\n* For $B$, you don't really risk overfitting if you pick something too big\n\n## Tuning parameters  {.smaller}\n\n\n::: question\nWith **random forest** what could we tune?\n:::\n\n\n* The depth of the tree, $B$, and `m` the number of predictors to try (`mtry`)\n\n\n* The default is $\\sqrt{p}$, and this does pretty well\n\n\n\n## Tuning parameters for boosting  {.smaller}\n\n\n* $B$ the number of bootstraps\n* $\\lambda$ the shrinkage parameter\n* $d$ the number of splits in each tree\n\n\n\n## Tuning parameters for boosting  {.smaller}\n\n\n::: question\nWhat do you think you can use to pick $B$?\n:::\n\n* Unlike **bagging** and **random forest** with **boosting** you can overfit if $B$ is too large\n\n\n* Cross-validation, of course!\n\n\n\n## Tuning parameters for boosting  {.smaller}\n\n* The _shrinkage parameter_ $\\lambda$ controls the rate at which boosting learn\n\n\n* $\\lambda$ is a small, positive number, typically 0.01 or 0.001\n\n\n* It depends on the problem, but typically a very small $\\lambda$ can require a very large $B$ for good performance\n\n\n\n## Tuning parameters for boosting  {.smaller}\n\n* _The number of splits_, $d$, in each tree controls the _complexity_ of the boosted ensemble\n\n\n* Often $d=1$ is a good default\n\n\n* _brace yourself for another tree pun!_\n\n\n* In this case we call the tree a _stump_ meaning it just has a single split\n\n\n* This results in an _additive model_\n\n\n* You can think of $d$ as the _interaction depth_ it controls the interaction order of the boosted model, since $d$ splits can involve at most $d$ variables\n\n\n## Boosted trees in R {.smaller}\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|1|2|3|4|5|7|\"}\nboost_spec <- boost_tree(\n  mode = \"classification\", \n  tree_depth = 1, \n  trees = 1000, \n  learn_rate = 0.001, \n) |>\n  set_engine(\"xgboost\")  \n```\n:::\n\n\n* Set the `mode` as you would with a bagged tree or random forest\n* `tree_depth` here is the depth of each tree, let's set that to 1  \n* `trees` is the number of trees that are fit, this is equivalent to `B`\n* `learn_rate` is $\\lambda$\n\n\n\n## Make a recipe\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec <- recipe(HD ~ Age + Sex + ChestPain + RestBP + Chol + Fbs + \n             RestECG + MaxHR + ExAng + Oldpeak + Slope + Ca + Thal,           \n           data = heart) |>\n  step_dummy(all_nominal_predictors())  \n```\n:::\n\n\n* `xgboost` wants you to have all numeric data, that means we need to make dummy variables\n* because `HD` (the outcome) is also categorical, we can use `all_nominal_predictors` to make sure we don't turn the outcome into dummy variables as well\n\n## Fit the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf <- workflow() |>\n  add_recipe(rec) |>\n  add_model(boost_spec)\nmodel <- fit(wf, data = heart)\n```\n:::\n\n\n\n\n\n\n## <i class=\"fas fa-laptop\"></i> `Boosting`\n\nHow would this code change if I wanted to tune `B` the number of bootstrapped training samples?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboost_spec <- boost_tree( \n  mode = \"classification\", \n  tree_depth = 1, \n  trees = 1000, \n  learn_rate = 0.001, \n) |>\n  set_engine(\"xgboost\") \n```\n:::\n\n\n\n\n## <i class=\"fas fa-laptop\"></i> `Boosting`\n\nFit a **boosted model** to the data from the previous application exercise.\n\n\n## Boosting Vs Bagging\n\n<https://www.youtube.com/watch?v=tjy0yL1rRRU&t=4s>\n\n# Variable Importance\n\n## Variable importance  {.smaller}\n\n* For bagged or random forest _regression trees_, we can record the _total RSS_ that is decreased due to splits of a given predictor $X_i$ averaged over all $B$ trees\n\n\n* A large value would indicate that that variable is _important_\n\n\n\n## Variable importance  {.smaller}\n\n* For bagged or random forest _classification trees_ we can add up the total amount that the Gini Index is decreased by splits of a given predictor, $X_i$, averaged over $B$ trees\n\n\n\n## Variable importance in R  {.smaller}\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|7|\"}\nrf_spec <- rand_forest(\n  mode = \"classification\",\n  mtry = 3\n) |> \n  set_engine(\n    \"ranger\",\n    importance = \"impurity\") \n\nwf <- workflow() |>\n  add_recipe(\n    recipe(HD ~ Age + Sex + ChestPain + RestBP + Chol + Fbs + \n             RestECG + MaxHR + ExAng + Oldpeak + Slope + Ca + Thal,               \n           data = heart)\n  ) |>\n  add_model(rf_spec)\nmodel <- fit(wf, data = heart)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nranger::importance(model$fit$fit$fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       Age        Sex  ChestPain     RestBP       Chol        Fbs    RestECG      MaxHR \n 8.9146013  4.1933971 16.6707756  7.0790178  7.3867806  0.7282573  1.6848589 13.8607409 \n     ExAng    Oldpeak      Slope         Ca       Thal \n 6.8637986 12.6144380  5.7571335 16.5656792 14.7467468 \n```\n\n\n:::\n:::\n\n\n\n\n## Variable importance\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ranger)\nimportance(model$fit$fit$fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       Age        Sex  ChestPain     RestBP       Chol        Fbs    RestECG      MaxHR \n 8.9146013  4.1933971 16.6707756  7.0790178  7.3867806  0.7282573  1.6848589 13.8607409 \n     ExAng    Oldpeak      Slope         Ca       Thal \n 6.8637986 12.6144380  5.7571335 16.5656792 14.7467468 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvar_imp <- ranger::importance(model$fit$fit$fit)\n```\n:::\n\n\n\n\n## Plotting variable importance\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvar_imp_df <- data.frame(\n  variable = names(var_imp),\n  importance = var_imp\n)\n\nvar_imp_df |>\n  ggplot(aes(x = variable, y = importance)) +\n  geom_col()\n```\n\n::: {.cell-output-display}\n![](07-3-boost_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n::: question\nHow could we make this plot better?\n:::\n\n\n\n## Plotting variable importance\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvar_imp_df |>\n  ggplot(aes(x = variable, y = importance)) +\n  geom_col() + \n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](07-3-boost_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n\n::: question\nHow could we make this plot better?\n:::\n\n\n\n\n## Plotting variable importance {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvar_imp_df |>\n  mutate(variable = factor(variable, \n                           levels = variable[order(var_imp_df$importance)])) |>\n  ggplot(aes(x = variable, y = importance)) +\n  geom_col() + \n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](07-3-boost_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n",
    "supporting": [
      "07-3-boost_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}