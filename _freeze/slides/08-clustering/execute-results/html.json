{
  "hash": "45fde4fbe6ea7a3983bc81641b50e934",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 12 - Unsupervised Learning\"\nsubtitle: \"Clustering\"\nformat: \n  revealjs:\n    output-file: \"08-clustering.html\"\n    slide-number: true\n  html:\n    output-file: \"08-clustering_o.html\"\neditor_options: \n  chunk_output_type: console\nlogo: \"img/icon.png\"\n---\n\n\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\ndata(penguins)\nlibrary(tidyclust)\n#install.packages('tidyclust')\n#install.packages('factoextra')\n```\n:::\n\n\n\n# Topics\n\n1. Unsupervised learning\n2. k-means clustering\n3. k-medoids clustering\n4. distance metrics\n5. hierarchical clustering\n\n\n## Unsupervised learning\n\n> Grouping or categorizing observational units (objects) without any pre-assigned labels or scores (no outcome information!)\n\n\n\n## Some examples:\n\n* Latent Dirichlet Allocation:  <a href=\"https://ziqixiong.shinyapps.io/TopicModeling/\" target = \"_blank\">Topic Modeling of TSL Articles</a>\n\n* Network Analysis: <a href = \"https://espresso.economist.com/6412121cbb2dc2cb9e460cfee7046be2\" target = \"_blank\">Political Books</a>\n\n* Network & Clustering: <a href = \"http://varianceexplained.org/r/love-actually-network/\" target = \"_blank\">Characters in 'Love Actually'</a>\n\n\n\n## $k$-means Clustering\n\n$k$-means clustering is an unsupervised partitioning algorithm designed to find a partition of the observations such that the following objective function is minimized (find the smallest within cluster sum of squares):\n\n$$\\text{arg}\\,\\min\\limits_{C_1, \\ldots, C_k} \\Bigg\\{ \\sum_{k=1}^K \\sum_{i \\in C_k} \\sum_{j=1}^p (x_{ij} - \\overline{x}_{kj})^2 \\Bigg\\}$$\n\n\n# Monsters clustering\n\n\n::: {.cell preview='true'}\n::: {.cell-output-display}\n![Artwork by @allison_horst.](../slides/img/kmeans.gif){fig-alt='Monsters as cluster centers moving around throughout the k-means algorithm.'}\n:::\n:::\n\n\n\n\n## A fun applet!! \n\n<https://www.naftaliharris.com/blog/visualizing-k-means-clustering/>\n\n\n\n\n\n## **Algorithm**:   $k$-Means Clustering {.smaller}\n\n\n1. Randomly assign a number, from 1 to $k$, to each of the observations.  These serve as initial cluster assignments for the observations.\n\n. . .\n\n2. Iterate until the cluster assignments stop changing:  \n    (a) For each of the $k$ clusters, compute the cluster centroid. The $k^{th}$ cluster centroid is the vector of the $p$ feature means for the observations in the $k^{th}$ cluster.\n    \n. . .\n\n    (b) Assign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance).\n    \n. . .\n\n3.  Ties?  Do something consistent:  for example, leave in the current cluster.\n\n\n\n\n## Convergence?  Yes!  (local...)\n\n1. If a point is \"closer\" to a different center, moving it will lower the objective function.\n\n2. Averages minimize squared differences, so taking the new average will result in a lower objective function.\n\n3. If a point is equidistant from two clusters, the point won't move.\n\n4. The algorithm must converge in finite number of steps because there are finitely many points.\n\n\n\n## Scaling\n\n\n::: {.cell}\n\n:::\n\n\n\n:::{.panel-tabset}\n\n### raw data\n\n::::{.columns}\n:::{.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnorm_clust %>%\n  kmeans(centers = 2) %>%\n  augment(norm_clust) %>%\n  ggplot() + \n  geom_point(aes(x = x1, \n                 y = x2, \n                 color = .cluster)) +\n  ggtitle(\"k-means (k=2) on raw data\")\n```\n:::\n\n\n:::\n\n:::{.column}\n\n::: {.cell}\n::: {.cell-output-display}\n![](08-clustering_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::::\n\n### scalled data\n\n::::{.columns}\n:::{.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnorm_clust %>%\n  mutate(across(everything(), \n                scale)) %>%\n  kmeans(centers = 2) %>%\n  augment(norm_clust) %>%\n  ggplot() + \n  geom_point(aes(x = x1, \n                 y = x2, \n                 color = .cluster)) +\n  ggtitle(\"k-means (k=2) on raw data\")\n```\n:::\n\n\n:::\n\n:::{.column}\n\n::: {.cell}\n::: {.cell-output-display}\n![](08-clustering_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n:::\n::::\n\n:::\n\n## K-means tidymodels\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins_recipe <- recipe(~.,data = penguins) |>\n  update_role(all_predictors(), new_role = \"predictor\") |>\n  step_select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)|>\n  step_naomit(all_predictors()) |>\n  step_scale(all_predictors()) \n\nkmeans_spec <- k_means(num_clusters =3)\n\n# Create a workflow\nwf <- workflow() |>\n  add_recipe(penguins_recipe) |>\n  add_model(kmeans_spec) #no engine since base R\n\n# Train the model\nkm_fit <- wf |>\n  fit(data = penguins)\n```\n:::\n\n\n## k-means outputs\n\n\n::: {.cell}\n\n```{.r .cell-code}\nextract_cluster_assignment(km_fit) |>\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 1\n  .cluster \n  <fct>    \n1 Cluster_1\n2 Cluster_1\n3 Cluster_1\n4 Cluster_1\n5 Cluster_1\n6 Cluster_1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nextract_centroids(km_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  .cluster  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  <fct>              <dbl>         <dbl>             <dbl>       <dbl>\n1 Cluster_1           7.00          9.17              13.4        4.47\n2 Cluster_2           8.70          9.50              14.0        4.87\n3 Cluster_3           8.70          7.59              15.4        6.33\n```\n\n\n:::\n:::\n\n\n## k-means outputs\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkm_summary<- extract_fit_summary(km_fit)\nkm_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$cluster_names\n[1] Cluster_1 Cluster_2 Cluster_3\nLevels: Cluster_1 Cluster_2 Cluster_3\n\n$centroids\n# A tibble: 3 × 4\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n           <dbl>         <dbl>             <dbl>       <dbl>\n1           7.00          9.17              13.4        4.47\n2           8.70          9.50              14.0        4.87\n3           8.70          7.59              15.4        6.33\n\n$n_members\n[1] 132  87 123\n\n$sse_within_total_total\n[1] 122.1477 112.9852 143.1502\n\n$sse_total\n[1] 1364\n\n$orig_labels\n  [1] 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [43] 2 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 2 1 1 1 2 1 2 1 1 1 2 1 2 1 1 1\n [85] 1 1 1 1 1 1 2 1 1 1 2 1 1 1 2 1 2 1 1 1 1 1 1 1 2 1 2 1 2 1 2 1 1 1 1 1 1 1 2 1 1 1\n[127] 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[169] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[211] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[253] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[295] 1 2 1 2 2 2 2 2 2 2 1 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2\n[337] 2 2 2 2 2 2\n\n$cluster_assignments\n  [1] Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1\n  [9] Cluster_2 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1\n [17] Cluster_2 Cluster_1 Cluster_2 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1\n [25] Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1\n [33] Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1\n [41] Cluster_1 Cluster_1 Cluster_2 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1\n [49] Cluster_2 Cluster_1 Cluster_1 Cluster_1 Cluster_2 Cluster_1 Cluster_1 Cluster_1\n [57] Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_2 Cluster_1 Cluster_1 Cluster_1\n [65] Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_2 Cluster_1 Cluster_1 Cluster_1\n [73] Cluster_2 Cluster_1 Cluster_2 Cluster_1 Cluster_1 Cluster_1 Cluster_2 Cluster_1\n [81] Cluster_2 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1\n [89] Cluster_1 Cluster_1 Cluster_2 Cluster_1 Cluster_1 Cluster_1 Cluster_2 Cluster_1\n [97] Cluster_1 Cluster_1 Cluster_2 Cluster_1 Cluster_2 Cluster_1 Cluster_1 Cluster_1\n[105] Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_2 Cluster_1 Cluster_2 Cluster_1\n[113] Cluster_2 Cluster_1 Cluster_2 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1\n[121] Cluster_1 Cluster_1 Cluster_2 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1\n[129] Cluster_2 Cluster_1 Cluster_2 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1\n[137] Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1\n[145] Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_1 Cluster_2 Cluster_3\n[153] Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3\n[161] Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3\n[169] Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3\n[177] Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3\n[185] Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3\n[193] Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3\n[201] Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3\n[209] Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3\n[217] Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3\n[225] Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3\n[233] Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3\n[241] Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3\n[249] Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3\n[257] Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3\n[265] Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3 Cluster_3\n[273] Cluster_3 Cluster_3 Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2\n[281] Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2\n[289] Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_1 Cluster_2\n[297] Cluster_1 Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2\n[305] Cluster_1 Cluster_2 Cluster_1 Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2\n[313] Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2\n[321] Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2\n[329] Cluster_1 Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2\n[337] Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2 Cluster_2\nLevels: Cluster_1 Cluster_2 Cluster_3\n```\n\n\n:::\n:::\n\n\n## Measuring k-means fit {.smaller}\n\nSum of squared error\n\n- One simple metric is the within cluster sum-of-squared error (WSS), which measures the sum of all distances from observations to their cluster center. \n\n- This is sometimes scaled with the total sum-of-squared error (TSS), the distance from all observations to the global centroid; in particular, the ratio WSS/TSS is often computed. \n\n- Small values of WSS or of the WSS/TSS ratio suggest that the observations within clusters are closer (more similar) to each other than they are to the other clusters.\n\n- The WSS and TSS come “for free” with the model fit summary, or they can be accessed directly from the model fit.\n\n## WSS {.smaller}\nNote we use  the fit object, not summary. \n\n::: {.cell}\n\n```{.r .cell-code}\nkm_fit |> sse_within_total()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric          .estimator .estimate\n  <chr>            <chr>          <dbl>\n1 sse_within_total standard        378.\n```\n\n\n:::\n\n```{.r .cell-code}\nkm_fit |> sse_total()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric   .estimator .estimate\n  <chr>     <chr>          <dbl>\n1 sse_total standard        1364\n```\n\n\n:::\n\n```{.r .cell-code}\nkm_fit |> sse_ratio()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric   .estimator .estimate\n  <chr>     <chr>          <dbl>\n1 sse_ratio standard       0.277\n```\n\n\n:::\n\n```{.r .cell-code}\nkm_fit |> sse_within()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  .cluster    wss n_members\n  <fct>     <dbl>     <int>\n1 Cluster_1  122.       132\n2 Cluster_2  113.        87\n3 Cluster_3  143.       123\n```\n\n\n:::\n:::\n\n\n\n## Evaluating clustering (which $k$?) {.smaller}\n\n- Silhouette Width (use $k$ with smallest silhouette width)\n\n- Elbow plot (use $k$ at elbow on plot of $k$ vs. within cluster sum of squares)\n\n- The silhouette of a single observation is proportional to the average distance from that observation to within-cluster observations minus the average distance to outside-cluster observations; normalized by the greater of these two average.\n\n- A large silhouette (close to 1) suggests that an observation is more similar to those within its cluster than those outside its cluster.\n\n- We can average all silhouettes to get a metric for the full clustering fit. \n\n\n\n\n## Silhouette width {.smaller}\n\nConsider observation $i \\in$ cluster $C_1$.  Let\n\n$$d(i, C_k) = \\mbox{average dissimilarity of } i \\mbox{ to all objects in cluster } C_k$$\n$$a(i) =  \\mbox{average dissimilarity of } i \\mbox{ to all objects in } C_1.$$\n$$b(i) = \\min_{C_k \\ne C_1} d(i,C_k) = \\mbox{distance to the next closest neighbor cluster}$$\n$$s(i) = \\frac{b(i) - a(i)}{\\max \\{ a(i), b(i) \\}}$$\n$$\\mbox{average}_{i \\in C_1} s(i) = \\mbox{average silhouette width for cluster } C_1$$\n\nNote that if $a(i) < b(i)$ then $i$ is well classified with a maximum $s(i) = 1$.   If $a(i) > b(i)$ then $i$ is *not* well classified with a maximum $s(i) = -1$.\n\n## Silhouette width - tidymodels\n\nThe computation of the silhouette depends on the original observation values, a dataset must also be supplied to the function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# only needed because I didn't split and handle NAs before\npenguins_na_drop <- penguins |> drop_na(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) \n\n    \nkm_fit %>%\n  silhouette_avg(penguins_na_drop )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric        .estimator .estimate\n  <chr>          <chr>          <dbl>\n1 silhouette_avg standard       0.447\n```\n\n\n:::\n:::\n\n\n## K Means Clustering, find K\n\n\n::: {.cell}\n\n```{.r .cell-code}\npen_recipe2 <- recipe(~.,data = penguins_na_drop) |>\n  update_role(all_predictors(), new_role = \"predictor\") |>\n  step_select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)|>\n  step_scale(all_predictors()) \n\npen_cv <- vfold_cv(penguins_na_drop,5)\n\nkm_spec2 <- k_means(num_clusters = tune())\n\nclust_num_grid <- grid_regular(num_clusters(),levels = 10)\n\nwf_tune <- workflow()|>\n  add_recipe(pen_recipe2)|>\n  add_model(km_spec2)\n  \nres <- tune_cluster(\n  wf_tune,\n  resamples = pen_cv,\n  grid = clust_num_grid,\n  control = control_grid(save_pred = TRUE, extract = identity),\n  metrics = cluster_metric_set(sse_within_total, sse_total, sse_ratio, silhouette_avg))\n```\n:::\n\n\n## K Means Clustering, find K\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres_metrics <- res %>% collect_metrics()\nres_metrics\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 40 × 7\n   num_clusters .metric          .estimator     mean     n  std_err .config              \n          <int> <chr>            <chr>         <dbl> <int>    <dbl> <chr>                \n 1            1 silhouette_avg   standard    NaN         0 NA       Preprocessor1_Model01\n 2            1 sse_ratio        standard      1         5  0       Preprocessor1_Model01\n 3            1 sse_total        standard   1090.        5  0.980   Preprocessor1_Model01\n 4            1 sse_within_total standard   1090.        5  0.980   Preprocessor1_Model01\n 5            2 silhouette_avg   standard      0.531     5  0.00325 Preprocessor1_Model02\n 6            2 sse_ratio        standard      0.413     5  0.00431 Preprocessor1_Model02\n 7            2 sse_total        standard   1090.        5  0.980   Preprocessor1_Model02\n 8            2 sse_within_total standard    451.        5  4.46    Preprocessor1_Model02\n 9            3 silhouette_avg   standard      0.440     5  0.00431 Preprocessor1_Model03\n10            3 sse_ratio        standard      0.341     5  0.0177  Preprocessor1_Model03\n# ℹ 30 more rows\n```\n\n\n:::\n:::\n\n\n## K Means Clustering, find K\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres_metrics %>%\n  filter(.metric == \"silhouette_avg\") %>%\n  ggplot(aes(x = num_clusters, y = mean)) +\n  geom_point() +\n  geom_line() +\n  theme_minimal() +\n  ylab(\"silhouette_avg\") +\n  xlab(\"Number of clusters\") +\n  scale_x_continuous(breaks = 1:10)\n```\n\n::: {.cell-output-display}\n![](08-clustering_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n\n## Distance metric (mathematically)\n\n1. $d({\\bf x}, {\\bf y}) \\geq 0$\n2. $d({\\bf x}, {\\bf y}) = d({\\bf y}, {\\bf x})$\n3. $d({\\bf x}, {\\bf y}) = 0$ iff ${\\bf x} = {\\bf y}$\n4. $d({\\bf x}, {\\bf y}) \\leq d({\\bf x}, {\\bf z}) + d({\\bf z}, {\\bf y})$  for all other vectors ${\\bf z}$.\n\n\n\n## Distance measures (clustering) {.smaller}\n\n* Euclidean Distance\n\n. . .\n\n$$d_E({\\bf x}, {\\bf y}) = \\sqrt{\\sum_{i=1}^p (x_i - y_i)^2}$$\n\n* Pearson Correlation Distance\n\n. . . \n\n$$d_P({\\bf x}, {\\bf y}) = 1 - r_P ({\\bf x}, {\\bf y})$$\n$$\\mbox{ or } d_P({\\bf x}, {\\bf y}) =  1 - |r_P ({\\bf x}, {\\bf y})|$$\n$$\\mbox{ or } d_P({\\bf x}, {\\bf y}) =  1 - (r_P ({\\bf x}, {\\bf y}))^2$$\n\n\n## Correlation distance isn't a distance metric! {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx1 <- c(1,2,3)\nx2 <- c(1, 4, 10)\nx3 <- c(9, 2, 2)\n\n# d(1,2)\n1 - cor(x1, x2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01801949\n```\n\n\n:::\n\n```{.r .cell-code}\n# d(1,3)\n1 - cor(x1, x3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.866025\n```\n\n\n:::\n\n```{.r .cell-code}\n# d(2,3)\n1 - cor(x2, x3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.755929\n```\n\n\n:::\n\n```{.r .cell-code}\n# d(1,3) > d(1,2) + d(2,3)\n1 - cor(x1, x2) + 1 - cor(x2, x3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.773948\n```\n\n\n:::\n:::\n\n\n\n## Correlation distance isn't a distance metric!\n\nUsing absolute distance doesn't fix things.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# d(1,2)\n1 - abs(cor(x1, x2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01801949\n```\n\n\n:::\n\n```{.r .cell-code}\n# d(1,3)\n1 - abs(cor(x1, x3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1339746\n```\n\n\n:::\n\n```{.r .cell-code}\n# d(2,3)\n1 - abs(cor(x2, x3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2440711\n```\n\n\n:::\n\n```{.r .cell-code}\n# d(2,3) > d(1,2) + d(1,3)\n1 - abs(cor(x1, x2)) + 1 - abs(cor(x1, x3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1519941\n```\n\n\n:::\n:::\n\n\n\n## Cosine Distance  (for clustering) {.smaller}\n\n$$d_C({\\bf x}, {\\bf y}) =  \\frac{{\\bf x} \\cdot {\\bf y}}{|| {\\bf x} ||  ||{\\bf y}||}$$\n$$= \\frac{\\sum_{i=1}^p x_i y_i}{\\sqrt{\\sum_{i=1}^p x_i^2 \\sum_{i=1}^p y_i^2}}$$\n$$= 1 - r_P ({\\bf x}, {\\bf y})  \\ \\ \\ \\ \\mbox{if } \\overline{\\bf x} = \\overline{\\bf y} = 0$$\n* Hamming Distance\n\n\\begin{align}\nd_H({\\bf x}, {\\bf y}) = \\sum_{i=1}^p I(x_i \\ne y_i)\n\\end{align}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![The Hamming distance across the two DNA strands is 7.](../slides/img/hamdistGCTA.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\n\n## `dist` function in R\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![The function `dist` in `R` calculates the distances given above.](../slides/img/distR.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n## String distances {.smaller}\n\n<https://www.kdnuggets.com/2019/01/comparison-text-distance-metrics.html>\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Comparison of string distance metrics from https://www.kdnuggets.com/2019/01/comparison-text-distance-metrics.html.](../slides/img/text-distance-infographics.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n## k-Medoids\n\nShortcomings of k-means\n\n* Center is calculated as average (establishes Euclidean distance)\n\n* Because center changes, distances must be re-calculated\n\n* Really, only Euclidean distance makes sense\n\n\n## Partitioning Around Medoids (PAM)\n\nFind the observations (data values!) $m_k$ that solve:\n\n$$\\text{arg}\\,\\min\\limits_{C_1, \\ldots, C_k} \\Bigg\\{ \\sum_{k=1}^K \\sum_{i \\in C_k}d(x_i, m_k) \\Bigg\\}$$\n\n- Not well implemented in R\n\n## Hierarchical Clustering\n\n> is a set of nested clusters that are organized as a tree.  Note that objects that belong to a child cluster also belong to the parent cluster.\n\nStat Quest: <https://www.youtube.com/watch?v=7xHsRkOdVwo>\n\n\n## **Algorithm**:  Agglomerative Hierarchical Clustering Algorithm {.smaller}\n\n1. Begin with $n$ observations and a measure (such as Euclidean distance) of all the ${n \\choose 2} = n(n-1)/2$ pairwise dissimilarities. Treat each observation as its own cluster.\n\n. . .\n\n2. For $i = n, n - 1, \\ldots , 2$:  \n   a. Examine all pairwise inter-cluster dissimilarities among the $i$ clusters and identify the pair of clusters that are least dissimilar (that is, most similar). Fuse these two clusters. The dissimilarity between these two clusters indicates the height in the dendrogram at which the fusion should be placed. \n   \n. . .\n\n   b. Compute the new pairwise inter-cluster dissimilarities among the $i - 1$ remaining clusters.\n\n\n\n\n## Definitions {.smaller}\n\n**Agglomerative** methods start with each object (e.g., gene, penguin, etc.) in its own group.  Groups are merged until all objects are together in one group.\n\n**Divisive** methods start with all objects in one group and break up the groups sequentially until all objects are individuals.\n\n**Single Linkage** algorithm defines the distance between groups as that of the closest pair of individuals.\n\n**Complete Linkage** algorithm defines the distance between groups as that of the farthest pair of individuals.\n\n**Average Linkage** algorithm defines the distance between groups as the average of the distances between all pairs of individuals across the groups.\n\n\n\n## Hierarchical clustering in R - tidymodels {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define a hierarchical clustering model\nhc_spec <- hier_clust(linkage_method = \"complete\",\n                      num_clusters = 3)\n\n# Create a workflow\nworkflow <- workflow() |>\n  add_recipe(pen_recipe2) |>\n  add_model(hc_spec)\n\n# fit\nhc_fit <- workflow |>\n  fit(data = penguins_na_drop)\n```\n:::\n\n\n## Output {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhc_summary <- hc_fit %>% extract_fit_summary()\nhc_summary %>% str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 7\n $ cluster_names         : Factor w/ 3 levels \"Cluster_1\",\"Cluster_2\",..: 1 2 3\n $ centroids             : tibble [3 × 4] (S3: tbl_df/tbl/data.frame)\n  ..$ bill_length_mm   : num [1:3] 7.2 8.7 9.15\n  ..$ bill_depth_mm    : num [1:3] 9.27 7.59 9.4\n  ..$ flipper_length_mm: num [1:3] 13.5 15.4 14\n  ..$ body_mass_g      : num [1:3] 4.6 6.33 4.71\n $ n_members             : int [1:3] 165 123 54\n $ sse_within_total_total: num [1:3] 169.5 120.9 49.9\n $ sse_total             : num 656\n $ orig_labels           : NULL\n $ cluster_assignments   : Factor w/ 3 levels \"Cluster_1\",\"Cluster_2\",..: 1 1 1 1 1 1 1 1 1 1 ...\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# This depends on our linkage chosen\nhc_preds <- hc_fit %>% predict(penguins_na_drop)\nhc_preds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 342 × 1\n   .pred_cluster\n   <fct>        \n 1 Cluster_1    \n 2 Cluster_1    \n 3 Cluster_1    \n 4 Cluster_1    \n 5 Cluster_1    \n 6 Cluster_1    \n 7 Cluster_1    \n 8 Cluster_1    \n 9 Cluster_3    \n10 Cluster_1    \n# ℹ 332 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhc_fit  |> extract_cluster_assignment(num_clusters = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 342 × 1\n   .cluster \n   <fct>    \n 1 Cluster_1\n 2 Cluster_1\n 3 Cluster_1\n 4 Cluster_1\n 5 Cluster_1\n 6 Cluster_1\n 7 Cluster_1\n 8 Cluster_1\n 9 Cluster_1\n10 Cluster_1\n# ℹ 332 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nhc_preds <- hc_fit |> augment(penguins_na_drop)\nhead(hc_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 9\n  .pred_cluster species island  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  <fct>         <fct>   <fct>            <dbl>         <dbl>             <int>       <int>\n1 Cluster_1     Adelie  Torger…           39.1          18.7               181        3750\n2 Cluster_1     Adelie  Torger…           39.5          17.4               186        3800\n3 Cluster_1     Adelie  Torger…           40.3          18                 195        3250\n4 Cluster_1     Adelie  Torger…           36.7          19.3               193        3450\n5 Cluster_1     Adelie  Torger…           39.3          20.6               190        3650\n6 Cluster_1     Adelie  Torger…           38.9          17.8               181        3625\n# ℹ 2 more variables: sex <fct>, year <int>\n```\n\n\n:::\n:::\n\n\n\n## Graph {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(factoextra)\n\nhc_fit  %>%\n  extract_fit_engine() %>%\n  fviz_dend(main = \"complete\", k = 3)\n```\n\n::: {.cell-output-display}\n![](08-clustering_files/figure-revealjs/unnamed-chunk-26-1.png){width=960}\n:::\n:::\n\n\n\n## <i class=\"fas fa-laptop\"></i> `Clustering`\n\n- Perform Kmeans and Hierarchical Clustering on the `iris` dataset (use `data(\"iris\")`)",
    "supporting": [
      "08-clustering_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}