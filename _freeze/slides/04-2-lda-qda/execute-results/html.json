{
  "hash": "eb390d52b4b899f1dccedcb4d5488d75",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 4 Part 2\"\nsubtitle: \"Multinomial Logisitc, LDA, QDA\"\nformat: \n  revealjs:\n    slide-number: true\n    chalkboard: true\n---\n\n\n## Recap {.small}\n\n* We had a _logistic regression_ refresher\n\n- Now...\n\n  *   What if our response has more than two levels? \n  *   What if logistic regression is a poor fit? \n\n## Setup \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR)\nlibrary(Stat2Data)\n#install.packages(\"discrim\")\n```\n:::\n\n\n\n## Multinomial Logistic\n\n- So far we have discussed logistic regression with two classes. \n\n- It is easily generalized to more than two classes. \n\n\n## Confounding {.small}\n\nRecall our defaults data with variable `default`, `student`, and `balance`\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-2-lda-qda_files/figure-revealjs/plot3-1.png){width=960}\n:::\n:::\n\n\n:::question\nWhat is going on here?\n:::\n\n\n## Confounding {.small}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-2-lda-qda_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n* Students tend to have higher balances than non-students\n* Their **marginal** default rate is higher\n* For each level of balance, students default less \n* Their **conditional** default rate is lower\n\n\n## Multiple logistic regression {.small}\n\n$$\\log\\left(\\frac{p(X)}{1-p(X)}\\right)=\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p$$\n$$p(X) = \\frac{e^{\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p}}{1+e^{\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p}}$$\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> -10.8690452 </td>\n   <td style=\"text-align:right;\"> 0.4922555 </td>\n   <td style=\"text-align:right;\"> -22.080088 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> balance </td>\n   <td style=\"text-align:right;\"> 0.0057365 </td>\n   <td style=\"text-align:right;\"> 0.0002319 </td>\n   <td style=\"text-align:right;\"> 24.737563 </td>\n   <td style=\"text-align:right;\"> 0.0000000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> income </td>\n   <td style=\"text-align:right;\"> 0.0000030 </td>\n   <td style=\"text-align:right;\"> 0.0000082 </td>\n   <td style=\"text-align:right;\"> 0.369815 </td>\n   <td style=\"text-align:right;\"> 0.7115203 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> studentYes </td>\n   <td style=\"text-align:right;\"> -0.6467758 </td>\n   <td style=\"text-align:right;\"> 0.2362525 </td>\n   <td style=\"text-align:right;\"> -2.737646 </td>\n   <td style=\"text-align:right;\"> 0.0061881 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n* Why is the coefficient for `student` negative now when it was positive before?\n\n## Logistic regression for more than two classes {.small}\n\n$$P(Y=k|X) = \\frac{e ^{\\beta_{0k}+\\beta_{1k}X_1+\\dots+\\beta_{pk}X_p}}{\\sum_{l=1}^Ke^{\\beta_{0l}+\\beta_{1l}X_1+\\dots+\\beta_{pl}X_p}}$$\n\n* We generalize this to situations with **multiple** classes\n* Here we have a linear function for **each** of the $K$ classes\n* This is known as **multinomial logistic regression**\n\n\n# Linear Discriminant Analysis (LDA)\n\n## LDA Warmup\n\nTo give us a general overview, we are going to watch the StatQuest video on the topic: <https://www.youtube.com/watch?v=azXCzI57Yfc>\n\n## Discriminant Analysis\n\n- Here the approach is to model the distribution of X in each of the classes separately, and then use Bayes theorem to flip things around and obtain $P(Y|X)$. \n\n- When we use normal (Gaussian) distributions for each class, this leads to linear or quadratic discriminant analysis.\n\n- However, this approach is quite general, and other distributions can be used as well. We will focus on normal distributions.\n\n## Why Another Approach? {.smaller}\n\n- When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.\n\n- If n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\n\n- Linear discriminant analysis is popular when we have more than two response classes, because it also provides low-dimensional views of the data.\n\n## Bayes Theorem (classification)\n\nThomas Bayes was a famous mathematician whose name represents a big subfield of statistical and probabilistic modeling. Here we focus on a simple result, known as Bayes theorem:\n\n$$P(Y=k|X=x) = \\frac{P(X=x|Y=k)\\cdot P(Y=k)}{P(X=x)}$$\n\n## Bayes for Discriminant Analysis\n\n\n$$P(Y=k|X=x) = \\frac{\\pi_kf_k(x)}{\\sum_{l=1}^K\\pi_lf_l(x)} \\text{, where}$$\n\n- $f_k(x)=P(X=x|Y=k)$ is the *density* for $X$ in class $k$. Here we use normal's but they could be other distributions (such as $\\chi^2$)\n\n- $\\pi_k = P(Y=k)$ is the marginal or prior probability for class $k$. \n\n## Classify to the highest density {.smaller}\n\n\n$$\\pi_1=.5, \\pi_2=.5$$\n\n:::: {.columns}\n\n::: {.column}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-2-lda-qda_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n\n:::\n\n::: {.column}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-2-lda-qda_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::::\n\n- We classify a new point according to which density is highest.\n\n- When the priors are different, we take them into account as well, and compare $\\pi_kf_k(x)$. \n\n- On the right, we favor the pink class - the decision boundary has shifted to the left.\n\n## LDA (when $p=1$)\n\nThe Gaussian (normal) density has the form\n\n$$f_k(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k}e^{-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma_k})^2}$$\n\n- $\\mu_k$ is the mean, $\\sigma_k^2$ the variance (in class $k$)\n\n- For now, we assume $\\sigma_k=\\sigma$ for all groups (we will need to check this with real data)\n\n## LDA (when $p=1$)\n\nWe plug this $f_k(x)$ into Bayes formula and after some simplifying we get:\n\n$$p_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma_k})^2}}{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma_k})^2}}$$\n\n## Discriminant Function {.smaller}\n\nTo classify at the value X = x, we need to see which of the $p_k(x)$ is largest. Taking logs, and discarding terms that do not depend on $k$, we see that this is equivalent to assigning x to the class with the largest *discriminant* score:\n\n$$\\delta_k(x) = x\\cdot \\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+log(\\pi_k)$$\n\n- Importantly, $\\delta_k(x)$ is a *linear* function of $x$.\n\n- If there are $K=2$ classes and $\\pi_1=\\pi_2=.5$, then the *decision boundry* is at \n\n$$x=\\frac{\\mu_1+\\mu_2}{2}$$\n\n## Maximizing $\\delta_k(x)$ {.smaller}\n\n- In order to maximize this, we need estimates for all the parameters\n\n::: question\nWhat should we estimate $\\hat{\\pi_k}$, $\\mu_k$, and $\\sigma^2$ with? \n:::\n\n\n## Maximizing $\\delta_k(x)$ {.smaller}\n\n$$\\hat{\\pi}_k = \\frac{n_k}{n}$$\n\n$$\\hat{\\mu}_k = \\frac{1}{n_k}\\sum_{i:y_k=k}x_i$$\n\n$$\\hat{\\sigma}^2 = \\frac{1}{n-K}\\sum_{k=1}^K\\sum_{i:y_i=k}(x_i-\\hat{\\mu}_k)^2 = \\sum_{k=1}^K\\frac{n_k-1}{n-K}\\cdot \\hat{\\sigma}_k^2$$\n\nWhere $$\\hat{\\sigma}_k^2 = \\frac{1}{n_k-1}\\sum_{i:y_i=k}(x_i-\\hat{\\mu}_k)^2$$\n\n## LDA In R Example ($p=1$)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"BlueJays\") # Bring data into environment\nlibrary(Stat2Data)\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(discrim)\n```\n:::\n\n\n::: question\nCan we determine the sex of a blue jay by measuring the distance from the tip of the bill to the back of the head (Head)? \n:::\n\n## LDA Bluejays EDA\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntt_split <- initial_split(BlueJays,prop=.7)\n\nBJ_train <- training(tt_split)\n\nBJ_train |> ggplot(aes(x = Head,y=KnownSex)) +\n  geom_boxplot() +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](04-2-lda-qda_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n## LDA Bluejays EDA\n\n::: question\nWhat assumptions should we check? \n:::\n\n- Normality of our predictor\n\n- Constant variance between groups. \n\n## LDA Bluejays EDA - Normality\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(patchwork)\n\n(BJ_train |> ggplot(aes(sample = Head)) + geom_qq())+\n  (BJ_train |> ggplot(aes(x = Head)) + geom_histogram())\n```\n\n::: {.cell-output-display}\n![](04-2-lda-qda_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n## LDA Bluejays EDA - Variance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBJ_train |> group_by(KnownSex)|>\n  summarize(Head_sd = sd(Head))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 2\n  KnownSex Head_sd\n  <fct>      <dbl>\n1 F           1.18\n2 M           1.25\n```\n\n\n:::\n:::\n\n\n- Very similar standard deviations (and thus variances)\n\n## Fit LDA\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlda_spec <- discrim_linear() |>\n  set_mode(\"classification\")|>\n  set_engine(\"MASS\")\n\nlda_fit<-  lda_spec |> \n  fit(KnownSex ~ Head,\n      data = BJ_train)\n```\n:::\n\n\n## Check The Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBJTest <- testing(tt_split)\n\nlda_fit |> \n  augment(new_data = BJTest) %>%\n  conf_mat(truth = KnownSex, estimate = .pred_class) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction  F  M\n         F 10  0\n         M 12 15\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlda_fit|> \n  augment(new_data = BJTest) |> \n  accuracy(truth = KnownSex,estimate=.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.676\n```\n\n\n:::\n:::\n\n## Compare to Logistic\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(KnownSex ~ Head,\n      data = BJ_train) |>\n  augment(new_data = BJTest)|>\n    accuracy(truth = KnownSex,estimate=.pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.676\n```\n\n\n:::\n:::\n\n\n\n## `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 640 512\" style=\"height:1em;width:1.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M128 32C92.7 32 64 60.7 64 96V352h64V96H512V352h64V96c0-35.3-28.7-64-64-64H128zM19.2 384C8.6 384 0 392.6 0 403.2C0 445.6 34.4 480 76.8 480H563.2c42.4 0 76.8-34.4 76.8-76.8c0-10.6-8.6-19.2-19.2-19.2H19.2z\"/></svg>`{=html} `Application Exercise`\n\nWe are going to use the `penguins` data from the `palmerpeguins` package. \n\n- Conduct basic EDA to see if penguin bill length is different by sex and if LDA is an appropriate model choice. \n\n- Use LDA to predict penguin sex based on their bill length using training and testing data. Get the accuracy on the testing set. \n\n- Fit a logistic regression model and get it's accuracy to see which did better. \n\n## LDA with $p>1$\n\n- When we have 2 or more predictors, the distribution becomes multivariate.\n\n- If the covariance between predictors is 0 within each class of the response, LDA is still appropriate. \n\n\n## LDA with $p>1$ {.smaller}\n\n- For example, with 2 normal predictors, their distributions in 3d would like like this:\n\n\n![](img/ldap2.png)\n\n- Luckily, the discriminate function remains `linear`\n\n$$\\delta_k(x) = c_{k0} + c_{k1}x_1+...c_{kp}x_p$$\n\n## Example: $p=2,K=3$ {.smaller}\n\n- There is no limit on the number of levels of the categorical response for LDA\n\n- Suppose $\\pi_1=\\pi_2\\pi_3=1/3$\n\n![](img/bayesdecboundry.png)\n- The dashed lines are known as the `Bayes decision boundaries`\n\n## Probabilities {.smaller}\n\n- Once the estimates for the $\\hat{\\delta}_k(x)$ have been found, we can plug them in and get:\n\n- $$\\hat{P}(Y=k|X=x)=\\frac{e^{\\hat{\\delta}_k(x)}}{\\sum_{l=1}^Ke^{\\hat{\\delta}^l(x)}}$$\n\n- Classifying to the largest $\\hat{\\delta}_k(x)$ amounts to classifying to the class for which $\\hat{P}(Y = k|X = x)$ is largest.\n\n- When $K = 2$, we classify to class 2 if $\\hat{P}(Y = 2|X = x)\\geq 0.5$, else to class 1.\n\n\n## Example - Credit Card Fraud\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction   No  Yes\n       No  2889   83\n       Yes    2   26\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.972\n```\n\n\n:::\n:::\n\n- Accuracy of 97.7% on a testing set!\n\n- What about the different types of errors? \n\n## Errors\n\n- Of the true `yes`, we made errors at the rate of 83/(83+26), 76%\n\n- Of the true `no`, we made errors at the rate of 2/(2889+2), .069%\n\n\n## Types of Errors {.smaller}\n- Recall:\n  -   False positive rate: The fraction of negative examples that are classified as positive.\n\n  -   False negative rate: The fraction of positive examples that are classified as negative.\n\n- Remember the model gave probabilities, the final `yes` or no is decided by\n\n- $$\\hat{P}(Default=Yes|Balance,Student) \\geq .5$$\n  -   We can adjust our error rates by changing that *threshold*\n\n\n## Sensitivity and Specificity {.smaller}\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction   No  Yes\n       No  2889   83\n       Yes    2   26\n```\n\n\n:::\n:::\n\n\n- The *sensitivity* is the true positive rate. \n  -   The rate at which we correctly predict a person will default. \n  -   26/(83+26) = .24 (24%)\n  \n- The *specificity* is the true negative rate. \n  -   The rate at which we correctly predict a person will not default. \n  -   2889/(2889+2) =.999 (99.9%)\n  \n\n## Varying the *threshold*\n\n- In order to determine the best threshold, we want to maximize the *specificty* and *sensitivity*. \n\n- OR - maximize the *sensitivty* and minimize *1-specificity*.\n\n- In order to to do this we use an $ROC$ curve which stands for *receiver operating characteristic curve*.\n\n## ROC Curve\n\nWe want to maximize the area under this curve, called *ROC AUC*\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"3\"}\nlda_roc<- lda_fit_2 |>\n  augment(new_data = testing(def_splits)) |>\n  roc_curve(truth = default,.pred_No) \n\nhead(lda_roc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 Ã— 3\n  .threshold specificity sensitivity\n       <dbl>       <dbl>       <dbl>\n1  -Inf          0                 1\n2     0.0819     0                 1\n3     0.129      0.00917           1\n4     0.151      0.0183            1\n5     0.154      0.0275            1\n6     0.201      0.0367            1\n```\n\n\n:::\n:::\n\n\n## ROC Curve\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(lda_roc)\n```\n\n::: {.cell-output-display}\n![](04-2-lda-qda_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n## ROC AUC\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlda_fit_2 |>\n  augment(new_data = testing(def_splits)) |>\n  roc_auc(truth = default,.pred_No) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.957\n```\n\n\n:::\n:::\n\nIf we want to tune our model better, we can optimize the ROC AUC by changing the threshold. \n\n:::question\nI did a train/test approach here. What should I do different if I want to tune for threshold?\n:::\n\n# Quadratic Discriminant Analysis (QDA)\n\n## QDA\n\n- QDA arises when $p>1$ and the is a covariance structure between the predictors within the same level of the response. \n\n- This introduces a squared term into the maximization problem of the $\\delta_k(x)$, thus the name. \n\n## QDA In R\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"1\"}\nqda_fit<-discrim_quad() |>\n  set_mode(\"classification\")|>\n  set_engine(\"MASS\")|> \n  fit(default ~ balance + student,\n      data = training(def_splits))\n\nqda_fit |>\n  augment(new_data = testing(def_splits)) %>%\n  conf_mat(truth = default, estimate = .pred_class) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction   No  Yes\n       No  2888   80\n       Yes    3   29\n```\n\n\n:::\n\n```{.r .cell-code  code-line-numbers=\"1\"}\nqda_fit |>\n  augment(new_data = testing(def_splits)) %>%\n  roc_auc(truth = default,.pred_No) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.957\n```\n\n\n:::\n:::\n\n\n## Tuning By Threhold\n\n<https://www.tidymodels.org/start/case-study/>\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}