{
  "hash": "8d5f495d7a9e78db1215930822e02d25",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 6 Part 4\"\nsubtitle: \"Dimension Reduction\"\nformat: \n  revealjs:\n    output-file: \"06-4-dim-red.html\"\n    slide-number: true\n  html:\n    output-file: \"06-4-dim-red_o.html\"\neditor_options: \n  chunk_output_type: console\nlogo: \"img/icon.png\"\n---\n\n\n\n\n## Setup\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ISLR2)\n#install.packages('pak')\n#pak::pak(\"mixOmics\") #Very large!\n```\n:::\n\n\n\n## Dimension Reduction\n\n- Lasso does dimension reduction when fitting\n\n- The methods that we have discussed so far in this chapter\nhave involved fitting linear regression models, via least\nsquares or a shrunken approach, using the original\npredictors, $X_1$,$X_2$,...,$X_p$.\n\n- Next we cover a few approaches that transform the\npredictors and then fit a least squares model using the\ntransformed variables. We will refer to these techniques as\ndimension reduction methods.\n\n## The Transformations {.smaller}\n\n- Let $Z_1,...,Z_m$ represent $M<p$ _linear combinations_ of our original $p$ predictors. Or, $$Z_m = \\sum_{j=1}^p\\phi_{mj}X_j$$ for some constants $\\phi_{m1},...,\\phi_{mp}$. \n\n- We can then fit linear regression model, $$y_i = \\theta_0 + \\sum{m=1}^M\\theta_mz_{im}+\\epsilon_i\\text{, }i=1,...,n,$$ using ordinary least squares. \n\n\n## The Coefficients {.smaller}\n\n- Note that in our transformed model, the regression coefficients are given by $\\theta_0...\\theta_M$. \n\n- If the constants $\\phi_{m1},...,\\phi_{mp}$ are chosen well, then this dimension reduction can often outperform OLS regression. \n\n- Also note, that $\\beta_j = \\sum_{m=1}^M\\theta_m\\phi_{mf}$, and thus this new transformed model is a special case of OLS\n\n- This effectively constrains the $\\beta_j$'s. \n\n## Principal Components Regression {.smaller}\n\n- Here we apply principal components analysis (PCA)\n(discussed in Chapter 10 of the text) to define the linear combinations of the predictors, for use in our regression.\n\n- The first principal component is that (normalized) linear combination of the variables with the largest variance.\n\n- The second principal component has largest variance,\nsubject to being uncorrelated with the first. And so on.\n\n- Hence with many correlated original variables, we replace them with a small set of principal components that capture their joint variation.\n\n## PCR - How does it work?\n\nVideo\n\n<https://www.youtube.com/watch?v=SWfucxnOF8c>\n\n## Partial Least Squares {.smaller}\n\n- PCR identifies linear combinations, or directions, that best\nrepresent the predictors $X_1,...X_p$\n\n- These directions are identified in an unsupervised way, since\nthe response Y is not used to help determine the principal\ncomponent directions.\n\n- That is, the response does not supervise the identification\nof the principal components.\n\n- Consequently, PCR suffers from a potentially serious\ndrawback: there is no guarantee that the directions that\nbest explain the predictors will also be the best directions\nto use for predicting the response.\n\n## More PLS {.smaller}\n\n- Like PCR, PLS is a dimension reduction method, which\nfirst identifies a new set of features $Z_1,...,Z_m$ that are\nlinear combinations of the original features, and then its a\nlinear model via OLS using these $M$ new features.\n\n- But unlike PCR, PLS identifies these new features in a\nsupervised way, that is, it makes use of the response Y in\norder to identify new features that not only approximate\nthe old features well, but also that are related to the\nresponse.\n\n- Roughly speaking, the PLS approach attempts to find\ndirections that help explain both the response and the\npredictors.\n\n## Even More PLS {.smaller}\n\nVideo\n\n<https://www.youtube.com/watch?v=Vf7doatc2rA>\n\n## PCR in tidymodels {.smaller}\n\n- Use _step_normalize_ and then _step_pca_\n- Fit a linear regression model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(iris)\n\nlm_spec <- linear_reg() |>\n  set_engine(\"lm\")\n\niris_rec_pcr <- recipe(Sepal.Width ~ .,data = iris)|>\n    step_dummy(all_nominal_predictors())|>\n    step_normalize(all_predictors()) |>\n    step_pca(all_numeric_predictors())          #New\n\npcr_wf <- workflow() |>\n  add_model(lm_spec)|>\n  add_recipe(iris_rec_pcr)\n```\n:::\n\n\n\n\n\n## PCR in tidymodels {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_pcr_fit <- pcr_wf |> \n  fit(data = iris)\n\ntidy_pcr_fit<- iris_pcr_fit |> tidy()\n\ntidy_pcr_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   3.06      0.0219    140.   1.08e-155\n2 PC1          -0.0682    0.0119     -5.73 5.61e-  8\n3 PC2          -0.157     0.0191     -8.23 1.01e- 13\n4 PC3           0.436     0.0456      9.56 4.58e- 17\n5 PC4          -0.910     0.122      -7.46 7.40e- 12\n6 PC5           0.356     0.204       1.75 8.29e-  2\n```\n\n\n:::\n:::\n\n\n\n\n## PCR in tidymodels {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(learntidymodels)\n\nplot_top_loadings(iris_pcr_fit)\n```\n\n::: {.cell-output-display}\n![](06-4-dim-red_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n## PCR \n\n- For now we are going to let it choose our number of components\n\n- Once we cover PCA formally, we can do more.\n\n## PLS in tidymodels {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_rec_pls <- recipe(Sepal.Width ~ .,data = iris)|>\n    step_dummy(all_nominal_predictors())|>\n    step_normalize(all_predictors()) |>\n    step_pls(all_numeric_predictors(),outcome = \"Sepal.Width\")          #New\n\npls_wf <- workflow() |>\n  add_model(lm_spec)|>\n  add_recipe(iris_rec_pls)\n\niris_pls_fit <- pls_wf |> \n  fit(data = iris)\n\ntidy_pls_fit<- iris_pls_fit |> tidy()\n\ntidy_pls_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    3.06     0.0283    108.   7.30e-142\n2 PLS1           0.146    0.0190      7.72 1.68e- 12\n3 PLS2           0.132    0.0247      5.34 3.49e-  7\n```\n\n\n:::\n:::\n\n\n\n## PLS in tidymodels {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_top_loadings(iris_pls_fit,type = 'pls')\n```\n\n::: {.cell-output-display}\n![](06-4-dim-red_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::",
    "supporting": [
      "06-4-dim-red_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}