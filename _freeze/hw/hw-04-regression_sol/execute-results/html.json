{
  "hash": "bf405d058cf9de6aacc96248b4bf2067",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Homework 4 Solutions\"\neditor: \n  markdown: \n    wrap: 72\neditor_options: \n  chunk_output_type: console\n---\n\n\n### 1.\n\n**We perform best subset, forward stepwise, and backward stepwise\nselection on a single data set. For each approach, we obtain p + 1\nmodels, containing** $0, 1, 2, . . . ,p$ **predictors.**\n\n**Explain your answers:**\n\n**(a) Which of the three models with k predictors has the smallest\ntraining RSS?**\n\nBest subsets since it considers every possible combination. It is\npossible both forward and backward stepwise pick the same k predictor\nmodel and have the same training RSS.\n\n(b) **Which of the three models with k predictors has the smallest test\n    RSS?**\n    \nYou cannot determine this to be sure. It depends if the best subset choice on the training set overfit. If it did, one of the other approach models could perform better. \n\n\n(c) **True or False:**\n\ni.  **The predictors in the k-variable model identified by forward\n    stepwise are a subset of the predictors in the (k+1)-variable model\n    identified by forward stepwise selection.**\n\nTrue, FSR keeps all k predictors from the previous step and adds an additional variable. \n\nii. **The predictors in the k-variable model identified by backward\n    stepwise are a subset of the predictors in the (k + 1)- variable\n    model identified by backward stepwise selection.**\n    \nTrue, BSR drops 1 variable form a k+1 predictor model and goes down to a k variable model. The k variables must be in the k+1 variable model. \n\n\niii. **The predictors in the k-variable model identified by backward\n     stepwise are a subset of the predictors in the (k + 1)- variable\n     model identified by forward stepwise selection.**\n\nFalse, not always true. \n\niv. **The predictors in the k-variable model identified by forward\n    stepwise are a subset of the predictors in the (k+1)-variable model\n    identified by backward stepwise selection.**\n    \nFalse, not always true. \n\nv.  **The predictors in the k-variable model identified by best subset\n    are a subset of the predictors in the (k + 1)-variable model\n    identified by best subset selection.**\n    \nFalse, not always true. \n\n### **8.**\n\n**In this exercise, we will generate simulated data, and will then use\nthis data to perform best subset selection.**\n\n(a) **Use the rnorm() function to generate a predictor X of length n =\n    100, as well as a noise vector $\\epsilon$ of length n = 100.**\n    \n    \n\n::: {.cell}\n\n```{.r .cell-code}\nX <- rnorm(100, mean = 10, sd = 10) \nep <- rnorm(100,sd = 30)\n```\n:::\n\n\n\n(b) **Generate a response vector** $Y$ **of length** $n=100$ **according\n    to the model**\n\n$$Y=\\beta_0 + \\beta_1X+\\beta_2X^2+\\beta_3X^3+\\epsilon,$$\n\n**where** $\\beta_0,\\beta_1,$ **and** $\\beta_3$ **are constants of your\nchoice.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nY = 5 + 3*X + 10*X^2+ 2*X^3 + ep\n```\n:::\n\n\n\n\n(c) **Use the regsubsets() function to perform best subset selection in\n    order to choose the best model containing the predictors $X$,$X^2$, . . .\n    ,$X^{10}$. What is the best model obtained according to $C_p$, BIC, and\n    adjusted $R^2$? Show some plots to provide evidence for your answer,\n    and report the coefficients of the best model obtained. Note you\n    will need to use the data.frame() function to create a single data\n    set containing both X and Y .**\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nsimdata <- data.frame(ysim = Y,xsim = X)\n\nhead(simdata)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        ysim      xsim\n1 1845.03133  8.368388\n2   34.56759 -3.272097\n3   14.39104  1.845040\n4 5054.90702 12.113334\n5  457.36512  4.704802\n6   42.17828  1.571858\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncreate_metrics_table <- function(X){\n  K <- length(X$rsq)\n  metrics_df <- data.frame(num_pred= 1:K, # K different models\n                           Rsq = X$rsq,\n                           rss = X$rss,\n                           adjr2 = X$adjr2,\n                           cp = X$cp,\n                           bic = X$bic) |>\n    tidyr::pivot_longer(cols=Rsq:bic,\n                 names_to = \"metric\",values_to = \"metric_val\")\n  # This pivot puts the metric values in 1 column \n  # and creates another column for the name of \n  # the metric\n  return(metrics_df)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(leaps)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'leaps' was built under R version 4.3.2\n```\n\n\n:::\n\n```{.r .cell-code}\nbss_fit <- regsubsets(ysim~poly(xsim,10),data = simdata)\nbss_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSubset selection object\nCall: regsubsets.formula(ysim ~ poly(xsim, 10), data = simdata)\n10 Variables  (and intercept)\n                 Forced in Forced out\npoly(xsim, 10)1      FALSE      FALSE\npoly(xsim, 10)2      FALSE      FALSE\npoly(xsim, 10)3      FALSE      FALSE\npoly(xsim, 10)4      FALSE      FALSE\npoly(xsim, 10)5      FALSE      FALSE\npoly(xsim, 10)6      FALSE      FALSE\npoly(xsim, 10)7      FALSE      FALSE\npoly(xsim, 10)8      FALSE      FALSE\npoly(xsim, 10)9      FALSE      FALSE\npoly(xsim, 10)10     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: exhaustive\n```\n\n\n:::\n\n```{.r .cell-code}\nbss_summary <- summary(bss_fit)\n\nbss_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSubset selection object\nCall: regsubsets.formula(ysim ~ poly(xsim, 10), data = simdata)\n10 Variables  (and intercept)\n                 Forced in Forced out\npoly(xsim, 10)1      FALSE      FALSE\npoly(xsim, 10)2      FALSE      FALSE\npoly(xsim, 10)3      FALSE      FALSE\npoly(xsim, 10)4      FALSE      FALSE\npoly(xsim, 10)5      FALSE      FALSE\npoly(xsim, 10)6      FALSE      FALSE\npoly(xsim, 10)7      FALSE      FALSE\npoly(xsim, 10)8      FALSE      FALSE\npoly(xsim, 10)9      FALSE      FALSE\npoly(xsim, 10)10     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: exhaustive\n         poly(xsim, 10)1 poly(xsim, 10)2 poly(xsim, 10)3 poly(xsim, 10)4\n1  ( 1 ) \"*\"             \" \"             \" \"             \" \"            \n2  ( 1 ) \"*\"             \"*\"             \" \"             \" \"            \n3  ( 1 ) \"*\"             \"*\"             \"*\"             \" \"            \n4  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n5  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n6  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n7  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n8  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n         poly(xsim, 10)5 poly(xsim, 10)6 poly(xsim, 10)7 poly(xsim, 10)8\n1  ( 1 ) \" \"             \" \"             \" \"             \" \"            \n2  ( 1 ) \" \"             \" \"             \" \"             \" \"            \n3  ( 1 ) \" \"             \" \"             \" \"             \" \"            \n4  ( 1 ) \" \"             \" \"             \" \"             \" \"            \n5  ( 1 ) \"*\"             \" \"             \" \"             \" \"            \n6  ( 1 ) \"*\"             \" \"             \"*\"             \" \"            \n7  ( 1 ) \"*\"             \"*\"             \"*\"             \" \"            \n8  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n         poly(xsim, 10)9 poly(xsim, 10)10\n1  ( 1 ) \" \"             \" \"             \n2  ( 1 ) \" \"             \" \"             \n3  ( 1 ) \" \"             \" \"             \n4  ( 1 ) \" \"             \" \"             \n5  ( 1 ) \" \"             \" \"             \n6  ( 1 ) \" \"             \" \"             \n7  ( 1 ) \" \"             \" \"             \n8  ( 1 ) \" \"             \" \"             \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ggplot2' was built under R version 4.3.3\n```\n\n\n:::\n\n```{.r .cell-code}\nbss_full_data_metrics <- create_metrics_table(bss_summary)\nhead(bss_full_data_metrics)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  num_pred metric metric_val\n     <int> <chr>       <dbl>\n1        1 Rsq       6.57e-1\n2        1 rss       5.51e+9\n3        1 adjr2     6.53e-1\n4        1 cp        4.95e+6\n5        1 bic      -9.77e+1\n6        2 Rsq       9.45e-1\n```\n\n\n:::\n\n```{.r .cell-code}\nbss_full_data_metrics |> \n  ggplot(aes(y=metric_val,x=num_pred))+\n    geom_line() + geom_point()+\n    facet_wrap(~metric,scales = \"free_y\")+\n    labs(title=\"Best Subset Regression\",\n         subtitle = \"On Full Data\")\n```\n\n::: {.cell-output-display}\n![](hw-04-regression_sol_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nAccording to $C_p$, BIC, and adjusted $R^2$ the best model is the three variable model. On the adjusted $R^$ graph you can see substantial improvements in the value between the 1 and 2 variable and the 2 and 3 variable model, and then little to no improvements beyond 3 variables. With $C_p$ and BIC we see the same behavior with with substantial decreases in the values between 1 and 2 variable models and then again between 2 and 3 variable models. All 3 metrics agree that the best model is a model with 3 variables. \n\n\n(d) **Repeat (c), using forward stepwise selection and also using\n    backwards stepwise selection. How does your answer compare to the\n    results in (c)?**\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nfsw_fit <- regsubsets(ysim~poly(xsim,10),data = simdata,method = \"forward\")\nfsw_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSubset selection object\nCall: regsubsets.formula(ysim ~ poly(xsim, 10), data = simdata, method = \"forward\")\n10 Variables  (and intercept)\n                 Forced in Forced out\npoly(xsim, 10)1      FALSE      FALSE\npoly(xsim, 10)2      FALSE      FALSE\npoly(xsim, 10)3      FALSE      FALSE\npoly(xsim, 10)4      FALSE      FALSE\npoly(xsim, 10)5      FALSE      FALSE\npoly(xsim, 10)6      FALSE      FALSE\npoly(xsim, 10)7      FALSE      FALSE\npoly(xsim, 10)8      FALSE      FALSE\npoly(xsim, 10)9      FALSE      FALSE\npoly(xsim, 10)10     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: forward\n```\n\n\n:::\n\n```{.r .cell-code}\nfsw_summary <- summary(fsw_fit)\n\nfsw_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSubset selection object\nCall: regsubsets.formula(ysim ~ poly(xsim, 10), data = simdata, method = \"forward\")\n10 Variables  (and intercept)\n                 Forced in Forced out\npoly(xsim, 10)1      FALSE      FALSE\npoly(xsim, 10)2      FALSE      FALSE\npoly(xsim, 10)3      FALSE      FALSE\npoly(xsim, 10)4      FALSE      FALSE\npoly(xsim, 10)5      FALSE      FALSE\npoly(xsim, 10)6      FALSE      FALSE\npoly(xsim, 10)7      FALSE      FALSE\npoly(xsim, 10)8      FALSE      FALSE\npoly(xsim, 10)9      FALSE      FALSE\npoly(xsim, 10)10     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: forward\n         poly(xsim, 10)1 poly(xsim, 10)2 poly(xsim, 10)3 poly(xsim, 10)4\n1  ( 1 ) \"*\"             \" \"             \" \"             \" \"            \n2  ( 1 ) \"*\"             \"*\"             \" \"             \" \"            \n3  ( 1 ) \"*\"             \"*\"             \"*\"             \" \"            \n4  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n5  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n6  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n7  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n8  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n         poly(xsim, 10)5 poly(xsim, 10)6 poly(xsim, 10)7 poly(xsim, 10)8\n1  ( 1 ) \" \"             \" \"             \" \"             \" \"            \n2  ( 1 ) \" \"             \" \"             \" \"             \" \"            \n3  ( 1 ) \" \"             \" \"             \" \"             \" \"            \n4  ( 1 ) \" \"             \" \"             \" \"             \" \"            \n5  ( 1 ) \"*\"             \" \"             \" \"             \" \"            \n6  ( 1 ) \"*\"             \" \"             \"*\"             \" \"            \n7  ( 1 ) \"*\"             \"*\"             \"*\"             \" \"            \n8  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n         poly(xsim, 10)9 poly(xsim, 10)10\n1  ( 1 ) \" \"             \" \"             \n2  ( 1 ) \" \"             \" \"             \n3  ( 1 ) \" \"             \" \"             \n4  ( 1 ) \" \"             \" \"             \n5  ( 1 ) \" \"             \" \"             \n6  ( 1 ) \" \"             \" \"             \n7  ( 1 ) \" \"             \" \"             \n8  ( 1 ) \" \"             \" \"             \n```\n\n\n:::\n\n```{.r .cell-code}\nfsw_full_data_metrics <- create_metrics_table(fsw_summary)\nhead(fsw_full_data_metrics)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  num_pred metric metric_val\n     <int> <chr>       <dbl>\n1        1 Rsq       6.57e-1\n2        1 rss       5.51e+9\n3        1 adjr2     6.53e-1\n4        1 cp        4.95e+6\n5        1 bic      -9.77e+1\n6        2 Rsq       9.45e-1\n```\n\n\n:::\n\n```{.r .cell-code}\nfsw_full_data_metrics |> \n  ggplot(aes(y=metric_val,x=num_pred))+\n    geom_line() + geom_point()+\n    facet_wrap(~metric,scales = \"free_y\")+\n    labs(title=\"Forward Stepwise Regression\",\n         subtitle = \"On Full Data\")\n```\n\n::: {.cell-output-display}\n![](hw-04-regression_sol_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbsw_fit <- regsubsets(ysim~poly(xsim,10),data = simdata,method = \"backward\")\nfsw_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSubset selection object\nCall: regsubsets.formula(ysim ~ poly(xsim, 10), data = simdata, method = \"forward\")\n10 Variables  (and intercept)\n                 Forced in Forced out\npoly(xsim, 10)1      FALSE      FALSE\npoly(xsim, 10)2      FALSE      FALSE\npoly(xsim, 10)3      FALSE      FALSE\npoly(xsim, 10)4      FALSE      FALSE\npoly(xsim, 10)5      FALSE      FALSE\npoly(xsim, 10)6      FALSE      FALSE\npoly(xsim, 10)7      FALSE      FALSE\npoly(xsim, 10)8      FALSE      FALSE\npoly(xsim, 10)9      FALSE      FALSE\npoly(xsim, 10)10     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: forward\n```\n\n\n:::\n\n```{.r .cell-code}\nbsw_summary <- summary(bsw_fit)\n\nbsw_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSubset selection object\nCall: regsubsets.formula(ysim ~ poly(xsim, 10), data = simdata, method = \"backward\")\n10 Variables  (and intercept)\n                 Forced in Forced out\npoly(xsim, 10)1      FALSE      FALSE\npoly(xsim, 10)2      FALSE      FALSE\npoly(xsim, 10)3      FALSE      FALSE\npoly(xsim, 10)4      FALSE      FALSE\npoly(xsim, 10)5      FALSE      FALSE\npoly(xsim, 10)6      FALSE      FALSE\npoly(xsim, 10)7      FALSE      FALSE\npoly(xsim, 10)8      FALSE      FALSE\npoly(xsim, 10)9      FALSE      FALSE\npoly(xsim, 10)10     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: backward\n         poly(xsim, 10)1 poly(xsim, 10)2 poly(xsim, 10)3 poly(xsim, 10)4\n1  ( 1 ) \"*\"             \" \"             \" \"             \" \"            \n2  ( 1 ) \"*\"             \"*\"             \" \"             \" \"            \n3  ( 1 ) \"*\"             \"*\"             \"*\"             \" \"            \n4  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n5  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n6  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n7  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n8  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n         poly(xsim, 10)5 poly(xsim, 10)6 poly(xsim, 10)7 poly(xsim, 10)8\n1  ( 1 ) \" \"             \" \"             \" \"             \" \"            \n2  ( 1 ) \" \"             \" \"             \" \"             \" \"            \n3  ( 1 ) \" \"             \" \"             \" \"             \" \"            \n4  ( 1 ) \" \"             \" \"             \" \"             \" \"            \n5  ( 1 ) \"*\"             \" \"             \" \"             \" \"            \n6  ( 1 ) \"*\"             \" \"             \"*\"             \" \"            \n7  ( 1 ) \"*\"             \"*\"             \"*\"             \" \"            \n8  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n         poly(xsim, 10)9 poly(xsim, 10)10\n1  ( 1 ) \" \"             \" \"             \n2  ( 1 ) \" \"             \" \"             \n3  ( 1 ) \" \"             \" \"             \n4  ( 1 ) \" \"             \" \"             \n5  ( 1 ) \" \"             \" \"             \n6  ( 1 ) \" \"             \" \"             \n7  ( 1 ) \" \"             \" \"             \n8  ( 1 ) \" \"             \" \"             \n```\n\n\n:::\n\n```{.r .cell-code}\nbsw_full_data_metrics <- create_metrics_table(bsw_summary)\nhead(bsw_full_data_metrics)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  num_pred metric metric_val\n     <int> <chr>       <dbl>\n1        1 Rsq       6.57e-1\n2        1 rss       5.51e+9\n3        1 adjr2     6.53e-1\n4        1 cp        4.95e+6\n5        1 bic      -9.77e+1\n6        2 Rsq       9.45e-1\n```\n\n\n:::\n\n```{.r .cell-code}\nbsw_full_data_metrics |> \n  ggplot(aes(y=metric_val,x=num_pred))+\n    geom_line() + geom_point()+\n    facet_wrap(~metric,scales = \"free_y\")+\n    labs(title=\"Backward Stepwise Regression\",\n         subtitle = \"On Full Data\")\n```\n\n::: {.cell-output-display}\n![](hw-04-regression_sol_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nWith both forward and backwards stepwise regression we see identical behavior of BIC, $C_p$ and adjusted $R^2$ as with the best subset regression, the best model is with 3 predictors. \n\n(e) **Now fit a lasso model to the simulated data, again using $X$,$X^2$, . .\n    . ,$X^{10}$ as predictors. Use cross-validation to select the optimal\n    value of $\\lambda$. Create plots of the cross-validation error as a function\n    of $\\lambda$. Report the resulting coefficient estimates, and discuss the\n    results obtained.**\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n✔ broom        1.0.5          ✔ rsample      1.2.1     \n✔ dials        1.2.1          ✔ tibble       3.2.1     \n✔ dplyr        1.1.4          ✔ tidyr        1.3.1     \n✔ infer        1.0.6          ✔ tune         1.2.1.9000\n✔ modeldata    1.3.0          ✔ workflows    1.1.4     \n✔ parsnip      1.2.1          ✔ workflowsets 1.0.1     \n✔ purrr        1.0.2          ✔ yardstick    1.3.0     \n✔ recipes      1.0.10         \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'broom' was built under R version 4.3.1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'dials' was built under R version 4.3.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'scales' was built under R version 4.3.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'dplyr' was built under R version 4.3.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'infer' was built under R version 4.3.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'modeldata' was built under R version 4.3.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'parsnip' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'purrr' was built under R version 4.3.1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'recipes' was built under R version 4.3.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'rsample' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'tidyr' was built under R version 4.3.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'workflows' was built under R version 4.3.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'workflowsets' was built under R version 4.3.1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'yardstick' was built under R version 4.3.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(434)\n\nsimdat_alt <- simdata|>\n  bind_cols(poly(simdata$xsim,degree=10,simple = T,raw = T))|>\n  select(-xsim)\n\nsim_cv <- vfold_cv(simdat_alt, v = 5)\n\nlasso_spec <- \n  linear_reg(penalty = tune(), mixture = 1) |> \n  set_engine(\"glmnet\") \n\nlam_grid <- expand_grid(penalty = seq(0, 325, by = 10))\n\n\nrec <- recipe(ysim ~ ., data = simdat_alt) |>\n  step_normalize(all_predictors())\n\nresults <- tune_grid(lasso_spec,\n                     preprocessor = rec,\n                     grid = lam_grid, \n                     resamples = sim_cv)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'glmnet' was built under R version 4.3.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'Matrix' was built under R version 4.3.2\n```\n\n\n:::\n\n```{.r .cell-code}\nmetrics<- results |>\n            collect_metrics()\n\nmetrics\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 66 × 7\n   penalty .metric .estimator   mean     n     std_err .config              \n     <dbl> <chr>   <chr>       <dbl> <int>       <dbl> <chr>                \n 1       0 rmse    standard   355.       5 49.1        Preprocessor1_Model01\n 2       0 rsq     standard     1.00     5  0.00000335 Preprocessor1_Model01\n 3      10 rmse    standard   355.       5 49.1        Preprocessor1_Model02\n 4      10 rsq     standard     1.00     5  0.00000335 Preprocessor1_Model02\n 5      20 rmse    standard   355.       5 49.1        Preprocessor1_Model03\n 6      20 rsq     standard     1.00     5  0.00000335 Preprocessor1_Model03\n 7      30 rmse    standard   355.       5 49.1        Preprocessor1_Model04\n 8      30 rsq     standard     1.00     5  0.00000335 Preprocessor1_Model04\n 9      40 rmse    standard   355.       5 49.1        Preprocessor1_Model05\n10      40 rsq     standard     1.00     5  0.00000335 Preprocessor1_Model05\n# ℹ 56 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nmetrics |> \n  filter(.metric ==\"rmse\") |> \n  group_by(penalty)|>\n  summarise(penalty = min(penalty),\n            mean = mean(mean))|>\n  ggplot(aes(x=penalty,y=mean)) +\n    geom_point()+\n    labs(title = \"Lasso Penalty vs RMSE\")\n```\n\n::: {.cell-output-display}\n![](hw-04-regression_sol_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\nfinal_spec <- linear_reg(penalty = 0, mixture = 1) |>\n  set_engine(\"glmnet\")\n\nfinal_lasso_workflow <- workflow()|>\n  add_model(final_spec) |> \n  add_recipe(rec)\n\nfinal_lasso_fit <- final_lasso_workflow|>\n  fit(data =simdat_alt)\n\nfinal_lasso_fit |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 11 × 3\n   term        estimate penalty\n   <chr>          <dbl>   <dbl>\n 1 (Intercept)    8587.       0\n 2 1                 0        0\n 3 2              1848.       0\n 4 3             10578.       0\n 5 4                 0        0\n 6 5                 0        0\n 7 6                 0        0\n 8 7                 0        0\n 9 8                 0        0\n10 9                 0        0\n11 10                0        0\n```\n\n\n:::\n:::\n\n\nLasso regression keeps only variables $X^2$ and $X^3$ which misses X. \n\n\n(f) **Now generate a response vector Y according to the model**\n\n$$Y=\\beta_0 + \\beta_7X^7 + \\epsilon$$\n\n**and perform best subset selection and the lasso. Discuss the results\nobtained.**\n\nNew Sim Data\n\n::: {.cell}\n\n```{.r .cell-code}\nY2 = 5 + 3*X + 5*X^7 + ep\n\nsimdata_2 <- data.frame(ysim = Y2,xsim = X)\n\n\n## This is to fit the model with the polynomials\nsimdat_alt_2 <- simdata_2|>\n  bind_cols(poly(simdata$xsim,degree=10,simple = T,raw = T))|>\n  select(-xsim)\n```\n:::\n\n\n\n### Best Subsets\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbss_fit_2 <- regsubsets(ysim~.,data = simdat_alt_2)\nbss_fit_2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSubset selection object\nCall: regsubsets.formula(ysim ~ ., data = simdat_alt_2)\n10 Variables  (and intercept)\n     Forced in Forced out\n`1`      FALSE      FALSE\n`2`      FALSE      FALSE\n`3`      FALSE      FALSE\n`4`      FALSE      FALSE\n`5`      FALSE      FALSE\n`6`      FALSE      FALSE\n`7`      FALSE      FALSE\n`8`      FALSE      FALSE\n`9`      FALSE      FALSE\n`10`     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: exhaustive\n```\n\n\n:::\n\n```{.r .cell-code}\nbss_summary_2 <- summary(bss_fit_2)\n\nbss_summary_2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSubset selection object\nCall: regsubsets.formula(ysim ~ ., data = simdat_alt_2)\n10 Variables  (and intercept)\n     Forced in Forced out\n`1`      FALSE      FALSE\n`2`      FALSE      FALSE\n`3`      FALSE      FALSE\n`4`      FALSE      FALSE\n`5`      FALSE      FALSE\n`6`      FALSE      FALSE\n`7`      FALSE      FALSE\n`8`      FALSE      FALSE\n`9`      FALSE      FALSE\n`10`     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: exhaustive\n         `1` `2` `3` `4` `5` `6` `7` `8` `9` `10`\n1  ( 1 ) \" \" \" \" \" \" \" \" \" \" \" \" \"*\" \" \" \" \" \" \" \n2  ( 1 ) \"*\" \" \" \" \" \" \" \" \" \" \" \"*\" \" \" \" \" \" \" \n3  ( 1 ) \" \" \" \" \"*\" \"*\" \" \" \" \" \"*\" \" \" \" \" \" \" \n4  ( 1 ) \"*\" \" \" \" \" \" \" \"*\" \"*\" \"*\" \" \" \" \" \" \" \n5  ( 1 ) \"*\" \"*\" \" \" \" \" \"*\" \"*\" \"*\" \" \" \" \" \" \" \n6  ( 1 ) \"*\" \" \" \" \" \"*\" \"*\" \" \" \"*\" \" \" \"*\" \"*\" \n7  ( 1 ) \"*\" \"*\" \" \" \" \" \" \" \"*\" \"*\" \"*\" \"*\" \"*\" \n8  ( 1 ) \"*\" \"*\" \" \" \"*\" \" \" \"*\" \"*\" \"*\" \"*\" \"*\" \n```\n\n\n:::\n\n```{.r .cell-code}\nbss_full_data_metrics_2 <- create_metrics_table(bss_summary_2)\n\nhead(bss_full_data_metrics_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  num_pred metric metric_val\n     <int> <chr>       <dbl>\n1        1 Rsq           1  \n2        1 rss      158450. \n3        1 adjr2         1  \n4        1 cp           46.2\n5        1 bic       -4050. \n6        2 Rsq           1  \n```\n\n\n:::\n\n```{.r .cell-code}\nbss_full_data_metrics_2 |> \n  ggplot(aes(y=metric_val,x=num_pred))+\n    geom_line() + geom_point()+\n    facet_wrap(~metric,scales = \"free_y\")+\n    labs(title=\"Best Subset Regression\",\n         subtitle = \"On Full Data\")\n```\n\n::: {.cell-output-display}\n![](hw-04-regression_sol_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\nBest subsets indicates that either two or eight coefficients should be kept. If we choose 2, the  model would be X and $X^7$, the correct model. If we choose predictors then we would keep all but $X^4$, $X^5$ and $X^6$. \n\n\n### Lasso\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nset.seed(434)\n\nsim_cv <- vfold_cv(simdat_alt_2, v = 5)\n\nlasso_spec <- \n  linear_reg(penalty = tune(), mixture = 1) |> \n  set_engine(\"glmnet\") \n\nlam_grid <- expand_grid(penalty = seq(0, 100, by = 10))\n\n\nrec <- recipe(ysim ~ ., data = simdat_alt_2) |>\n  step_scale(all_predictors())\n\nresults <- tune_grid(lasso_spec,\n                     preprocessor = rec,\n                     grid = lam_grid, \n                     resamples = sim_cv)\n\nmetrics<- results |>\n            collect_metrics()\n\nmetrics\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 22 × 7\n   penalty .metric .estimator         mean     n std_err .config              \n     <dbl> <chr>   <chr>             <dbl> <int>   <dbl> <chr>                \n 1       0 rmse    standard   510011538.       5 3.43e+8 Preprocessor1_Model01\n 2       0 rsq     standard           1.00     5 1.22e-6 Preprocessor1_Model01\n 3      10 rmse    standard   510011538.       5 3.43e+8 Preprocessor1_Model02\n 4      10 rsq     standard           1.00     5 1.22e-6 Preprocessor1_Model02\n 5      20 rmse    standard   510011538.       5 3.43e+8 Preprocessor1_Model03\n 6      20 rsq     standard           1.00     5 1.22e-6 Preprocessor1_Model03\n 7      30 rmse    standard   510011538.       5 3.43e+8 Preprocessor1_Model04\n 8      30 rsq     standard           1.00     5 1.22e-6 Preprocessor1_Model04\n 9      40 rmse    standard   510011538.       5 3.43e+8 Preprocessor1_Model05\n10      40 rsq     standard           1.00     5 1.22e-6 Preprocessor1_Model05\n# ℹ 12 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nmetrics |> \n  filter(.metric ==\"rmse\") |> \n  group_by(penalty)|>\n  summarise(penalty = min(penalty),\n            mean = mean(mean))|>\n  ggplot(aes(x=penalty,y=mean)) +\n    geom_line()+\n    labs(title = \"Lasso Penalty vs RMSE\")\n```\n\n::: {.cell-output-display}\n![](hw-04-regression_sol_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmetrics |> filter(.metric==\"rmse\") |>\n  arrange(mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 11 × 7\n   penalty .metric .estimator       mean     n    std_err .config              \n     <dbl> <chr>   <chr>           <dbl> <int>      <dbl> <chr>                \n 1       0 rmse    standard   510011538.     5 342846372. Preprocessor1_Model01\n 2      10 rmse    standard   510011538.     5 342846372. Preprocessor1_Model02\n 3      20 rmse    standard   510011538.     5 342846372. Preprocessor1_Model03\n 4      30 rmse    standard   510011538.     5 342846372. Preprocessor1_Model04\n 5      40 rmse    standard   510011538.     5 342846372. Preprocessor1_Model05\n 6      50 rmse    standard   510011538.     5 342846372. Preprocessor1_Model06\n 7      60 rmse    standard   510011538.     5 342846372. Preprocessor1_Model07\n 8      70 rmse    standard   510011538.     5 342846372. Preprocessor1_Model08\n 9      80 rmse    standard   510011538.     5 342846372. Preprocessor1_Model09\n10      90 rmse    standard   510011538.     5 342846372. Preprocessor1_Model10\n11     100 rmse    standard   510011538.     5 342846372. Preprocessor1_Model11\n```\n\n\n:::\n\n```{.r .cell-code}\n# Choose lambda = 81\n\nfinal_spec <- linear_reg(penalty = 0, mixture = 1) |>\n  set_engine(\"glmnet\")\n\nfinal_lasso_workflow <- workflow()|>\n  add_model(final_spec) |> \n  add_recipe(rec)\n\nfinal_lasso_fit <- final_lasso_workflow|>\n  fit(data =simdat_alt_2)\n\nfinal_lasso_fit |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 11 × 3\n   term            estimate penalty\n   <chr>              <dbl>   <dbl>\n 1 (Intercept)   130518046.       0\n 2 1                     0        0\n 3 2                     0        0\n 4 3                     0        0\n 5 4                     0        0\n 6 5                     0        0\n 7 6             415202305.       0\n 8 7           24905659724.       0\n 9 8              20982661.       0\n10 9                     0        0\n11 10                    0        0\n```\n\n\n:::\n:::\n\n\nIn this case, lasso keeps the intercept and coefficients for $X^6$, $X^7$ and $X^8$. Best subsets did a better job. \n\n",
    "supporting": [
      "hw-04-regression_sol_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}