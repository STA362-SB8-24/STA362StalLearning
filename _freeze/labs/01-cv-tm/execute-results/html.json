{
  "hash": "e82ecbd5c39c26da62bcf7409e8aebe7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab 01\"\nsubtitle: \"Cross Validation and tidymodels\"\nauthor: \"{{< var citation.websource >}} modfied by {{< var instructor.name >}}\"\nformat: html\n---\n\n\n\n\n# Getting started\n\nGo to our RStudio and create a new R project inside your class folder.\n\n# Packages\n\nIn this lab we will work with three packages: `ISLR` which is a package that accompanies your textbook, `tidyverse` which is a collection of packages for doing data analysis in a \"tidy\" way, and `tidymodels` a collection of packages for statistical modeling.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) \nlibrary(tidymodels)\nlibrary(ISLR)\n```\n:::\n\n\n\n## YAML: \n\nOpen the `.qmd` file in your project, make sure the author is your name, and Render the document.\n\n# Data\n\nFor this lab, we are using the `Auto` data from the `ISLR` package.\n\n# Exercises\n\n## Conceptual questions\n\n1. Explain how k-fold Cross Validation is implemented.\n\n2. What are the advantages / disadvantages of k-fold Cross Validation compared to the Validation Set approach? What are the advantages / disadvantages of k-fold Cross Validation compared to Leave-one-out Cross Validation?\n\n## Data exploration\n\n3. For this analysis, we are using the `Auto` dataset from the `ISLR` package. How many rows are in this dataset? How many columns? Is there any missing data?\n\n4. Our outcome of interest is miles per gallon: `mpg`. Create a publication-ready figure examining the distribution of this variable.\n\n5. Our main predictor of interest is `horsepower`. Create a publication-ready figure looking at the relationship between miles per gallon and horsepower.\n\n\n## K-fold cross validation\n\nWe are trying to decide between three models of varying flexibility:\n\n* **Model 1:** $\\texttt{mpg} = \\beta_0 + \\beta_1 \\texttt{horsepower} + \\epsilon$\n* **Model 2:** $\\texttt{mpg} = \\beta_0 + \\beta_1 \\texttt{horsepower} + \\beta_2 \\texttt{horsepower}^2 + \\epsilon$\n* **Model 3:** $\\texttt{mpg} = \\beta_0 + \\beta_1 \\texttt{horsepower} + \\beta_2 \\texttt{horsepower}^2 + \\beta_3 \\texttt{horsepower}^3 + \\epsilon$\n\n6. Using the `Auto` data, split the data into **two groups** a training data set, saved as `Auto_train` and a testing data set, saved as `Auto_test`. _Be sure to set a seed to ensure that you get the same result each time you Render your document_.\n\n\n\n::: column-margin\nYou can use the `poly()` function to fit a model with a polynomial term. For example, to fit the model $y = \\beta_0 + \\beta_1 \\texttt{x} + \\beta_2 \\texttt{x}^2 + \\beta_3 \\texttt{x}^3 + \\epsilon$, you would run\n`fit(lm_spec, y ~ poly(x, 3), data = data)`\n:::\n\n7. Fit the three models outlined above on the **training** data. Using the model created on the training data, predict `mpg` in the test data set for each model. What is the test RMSE for the three models? Which model would you choose?\n\n8. Fit the same three models, but instead of the validation set approach, perform 5-fold cross validation. Make sure to set a seed so you get the same answer each time you run the analysis. Calculate the overall 5-fold cross validation error for each of the three models. Which model would you chose?\n\n9. The tidymodels package allows you to do this faster! Instead of having a fit 3 (or more!) different models to determine the best flexibility, you can (1) create a **recipe** to specify how you would like to fit a model and then (2) **tune** this model to determine the best output. Copy the code below. What do you think the line `step_poly(horsepower, degree = tune())` does? Hint: you can run `?step_poly` in the Console to learn more about this function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nauto_prep <- Auto |>\n  recipe(mpg ~ horsepower) |>\n  step_poly(horsepower, degree = tune())\n```\n:::\n\n\n\n10. To **tune** this model, you will replace `fit_resamples` with `tune_grid`. The pseudo code to do this is below - you may need to update some names to match what you have named objects in your document. Add the code to **tune** your model based on the code below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nauto_tune <- tune_grid(lm_spec,\n          auto_prep,\n          resamples = auto_cv)\n```\n:::\n\n\n11. Using the `collect_metrics` function, look at the RMSE for `auto_tune`. Which `degree` is preferable?\n\n\n12. You can plot the output from Exercise 11 to make it a bit easier to determine. First, save your output from Exercise 11 as `auto_metrics`. Then _filter_ this data frame to only include rows where `.metric == \"rmse\"`. Save this filtered data frame as `auto_rmse`. Edit the code below to plot the `degree` on the x-axis and `mean` on the y-axis. Describe what this plot shows.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(auto_rmse, aes(x = ----, y = ----)) + \n  geom_line() +\n  geom_pointrange(aes(ymin = mean - std_err, ymax = mean + std_err)) + \n  labs(x = \"Degree\",\n       y = \"Cross validation error\",\n       title = ---)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}