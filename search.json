[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Introduction to Statistical Learning",
    "section": "",
    "text": "Instructor\nInstructor Dr. Tyler George\n   Cornell College, West 311\n    tgeorge@cornellcollege.edu \n\n\n\nClass Meetings\nApril 15th - May 8th\n   9am-11am & 1pm-2pm\n   West 201\n   Course Calendar\n\n\n\nOffice Hours\n   MWTh 3:05pm-4:05pm and by appt.\n   West 311\n   Optional Appointment\n\n\n\nI am available far beyond these times listed. Please email me and we can set up a time to chat about class material or whatever you prefer! I will generally announce changes to office hours in class but I still suggest checking the Course Calendar to verify availability.\n\nYou Are A Priority\nMy goal this block is to help you learn the material. I want to first and foremost recognize that you are an individual and thus are unique and may learn uniquely. Additionally, your health and wellbeing are priority one. Learning cannot happen effectively if you don’t meet your other personal needs. That all being said, I have structured the class in a way that I, from experience teaching and learning myself, think will be most beneficial for the majority of students. I promise you that I will do my best to create an inclusive and engaging learning environment. I ask that you keep an open line of communication between us for when you may need help and/or flexibility. You and your learning are why I am here.\n\n\nCourse Description\nThis course will introduce students to relatively new and powerful statistical techniques used to analyze data. The course will begin with a review of linear regression and an introduction to computer-based variable and model selection methods. Other topics will include classification methods, resampling methods for model-building, non-linear models, and tree-based methods. The computer software program R will be used throughout.\n\n\nLearning Objectives\nAt the end of this course I would like you conceptually understand, be able to use apply, and interpret the results of\n\nThe variance/bias trade-off\nMeasuring quality of fits\nLinear model selection and regularization\nClassification including K nearest neighbors\nCross validation\nDimension reduction\nTree based methods\nUnsupervised learning\n\n\n\nPrerequisite\nTo be successful in this class, you should have completed STA 201, STA 202, and DSC 223.\n\n\nOpen Access Books – Free!\nAll of materials for this class are free.\nThe main text book is: An Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani – it is freely available online.\nThe secondary text is R resource: Tidy Modeling with R by Max Kuhn and Julia Silge – it is freely available online.\n\n\nCourse Site and Moodle\nOur course will run from a combination of Moodle and the course website at https://sta362-sb8-24.github.io/STA362StatLearning/.\n\n\nSoftware – No need to install\nWe will use a combination of technologies in this course including R, and RStudio (server). Luckily for you I have put lots of effort into setting all of this on a machine we have on campus that we will all access with a web browser! You don’t need to install any – in fact for a while I prefer you don’t. More on this in class. If you are an off campus student, please let me know right away, as you may need to checkout a laptop (free) from IT to work on homework from home.\nIf you have any technical problems you should contact IT as soon as possible. Submit a Work Order!\n\n\nGroup Work\nIn this class, I would like you to work in groups for a variety of reasons. A large part of this class is communicating analysis – not just completing analysis. At the beginning of the block, groups will be formed. You should expect to work with this group every day. When we work in groups in class we will decide on roles, specifically who is controlling the one screen will rotate). Group members will rotate roles between tasks to help make sure everybody is sharing work. You won’t be working in a group for everything; any quizzes, and exams may be individual.\n\n\nEvaluations and grades\n\nGrade Category Descriptions\n\nHomework:\nHomeworks will be graded for correctness. The goal is the practice the application of the method and then be able to interpret the result.\n\n\nParticipation\nThis will be measured by attending class and working on the work given including labs and class examples.\n\n\nProject\nThis will entail multiple stages of submission with details accessed through “Project” on the left side of the course website (once available). Some class time will be given for discussing projects with me but not enough to complete the project during class times. I do not anticipate we will start these until week 3.\n\n\nExams\nThere will be a Midterm exam (4/26) and a final exam (morning of 5/8).\n\n\n\n\n\n\n\n\nAssignment\nPoints\n\n\n\n\nHomework\n200\n\n\nParticipation\n100\n\n\nProject\n300\n\n\nExams, two 200pts exams\n400\n\n\nTotal\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\nA\n93-100%\nC\n73-76%\n\n\nA-\n90–92%\nC-\n70-72%\n\n\nB+\n87–89%\nD+\n67-69%\n\n\nB\n83-86%\nD\n63-66%\n\n\nB-\n80-82%\nD-\n60-62%\n\n\nC+\n77-79%\nF\n&lt;60%\n\n\n\n\n\n\n\n\n\nUse of AI\nI expect you to generate your own work in this class. When you submit any kind of work (including projects, exams, homeworks), you are asserting that you have generated and written the text, and code, unless you indicate otherwise by the use of quotation marks and proper attribution for the source. Submitting content as your own that has been generated by someone other than you, or was created or assisted by a computer application or tool, including artificial intelligence (AI) tools such as ChatGPT is cheating and constitutes a violation of our Academic Honesty policy. You may use simple word processing tools to update spelling and grammar in your assignments, but unless given permission otherwise, you may not use AI tools to draft your work, even if you edit, revise, or paraphrase it. There may be opportunities for you to use AI tools in this class. Where they exist, I will clearly specify when and in what capacity it is permissible for you to use these tools.\n\n\nDISABILITIES AND ACCOMODATIONS POLICY\nCornell College makes reasonable accommodations for persons with disabilities. Students should notify the Office of Academic Support and Advising and their course instructor of any disability related accommodations within the first three days of the term for which the accommodations are required, due to the fast pace of the block format. For more information on the documentation required to establish the need for accommodations and the process of requesting the accommodations.\n\n\nACADEMIC HONESTY POLICY\nCornell College expects all members of the Cornell community to act with academic integrity. An important aspect of academic integrity is respecting the work of others. A student is expected to explicitly acknowledge ideas, claims, observations, or data of others, unless generally known. When a piece of work is submitted for credit, a student is asserting that the submission is her or his work unless there is a citation of a specific source. If there is no appropriate acknowledgment of sources, whether intended or not, this may constitute a violation of the College’s requirement for honesty in academic work and may be treated as a case of academic dishonesty. The procedures regarding how the College deals with cases of academic dishonesty appear in The Catalog, under the heading “Academic Honesty.”\n\n\nIllness Policy\nIf you are experiencing COVID-19 symptoms, do not attend class. Perform a home test or contact Director of Student Health Services Lynn O’Brien at student_health@cornellcollege.edu immediately to arrange a COVID-19 test at the Health Center. If you need to isolate due to COVID-19, or if you become unable to attend class for any other health reason, contact me as soon as possible to determine if you are able to continue in the class. A Withdrawal for Health Reasons may be required.\n\n\nMandatory Reporter Reminder\nIt is my goal that you feel supported and able to share information related to your life experiences during classroom discussions, in your written work, and in any one-on-one meetings with me. You should also know that all Cornell College faculty and staff are mandatory reporters. This means that I will keep information you share with me private to the greatest extent possible. However, I am required to share information regarding sexual assault, abuse, criminal behavior, or about a student who may be a danger to themselves or to others. If you wish to speak to someone confidentially who is not a mandatory reporter, you can schedule an appointment with one of the counselors in the Ebersole Health and Wellbeing Center or contact the College Chaplain, Rev. Melea White, at mwhite@cornelllcollege.edu.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Statistical Learning Schedule",
    "section": "",
    "text": "Note: The timeline of topics and assignments might be updated throughout the semester.\n\n\n\n\n\n\n\n\n\nDay\nDate\nTopic\nNotes\nExercises\nLab\nHomework\n\n\n\n\n1\n15 April\nCourse Intro &lt;br&gt; Ch 1 - Stat Learning Examples br&gt; Ch 2 - SL, Bias-Variance Tradeoff\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\n16 April\nCh 5 - Cross Validation &lt;br&gt; Tidy Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\n17 April\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4\n18 April\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5\n19 April\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6\n22 April\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7\n23 April\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8\n24 April\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9\n25 April\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10\n26 April\nExam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11\n29 April\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12\n30 April\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13\n1 May\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14\n2 May\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15\n3 May\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16\n6 May\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n17\n7 May\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n18\n8 May\nExam",
    "crumbs": [
      "Course Contents",
      "Schedule & Assignments"
    ]
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you can identify when that is without getting too frustrated and feel comfortable seeking help.\nYou can ask anonymous course questions by adding them at https://docs.google.com/spreadsheets/d/1R33yYyRdFoRMDtSnD-CzLOqIEwYNrC3kepMm4ZN_PWM/edit?usp=sharing.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#lectures-and-labs",
    "href": "course-support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#office-hours",
    "href": "course-support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nYou are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage each and every one of you to take advantage of this resource! Make a pledge to stop by office hours at least once during the first few days of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of your professors office hours here.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#academic-support",
    "href": "course-support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\n\nQuantitative Reasoning Studio (QRS)\nThere are times you may need help outside of class or office hours. Or, maybe you need something explained in a different way. In those instances, I encourage you to visit the Quantitative Reasoning Studio in Cole Library room 322. The Quantitative Reasoning Studio (QRS) offers free tutoring to all students at Cornell College. There will be at least 1 peer tutor that has taken this course and will be able to help you, if you arrive at a time they are working. Feel free to email Jessica Johanningmeier at QRS@cornellcollege.edu to ask when the tutor for this class will be available. They often will have a schedule posted on the wall in the studio.\n\n\nQRS Hours\n\n\n\nDay(s)\nTimes\n\n\n\n\nMonday-Thursday\n8 a.m. - 5 p.m. and 7 p.m. - 10 p.m.\n\n\nFriday\n8 a.m. - 5 p.m.\n\n\nSunday\n3 p.m. - 5 p.m. and 7 p.m. - 10 p.m.\n\n\n\n\n\nDungy Writing Studio\nFor help with your writing, visit the Dungy Writing Studio. You can make online appointments individual or groups to get help with items such as your group project. If you have any questions about the studio, email Dungy Writing Studio Director and Director of Fellowships and Scholarships, Laura Farmer, at lfarmer@cornellcollege.edu.\n\n\nWriting Studio Hours\n\n\n\nDay(s)\nTimes\n\n\n\n\nMonday-Thursday\n8 a.m. - 5 p.m. and 7 p.m. - 10 p.m.\n\n\nFriday\n8 a.m. - 5 p.m.\n\n\nSunday\n1 p.m. - 5 p.m.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#student-success-center",
    "href": "course-support.html#student-success-center",
    "title": "Course support",
    "section": "Student Success Center",
    "text": "Student Success Center\nThe Student Success Center is a resource for all students. Their staff serves as student success coaches for all students and welcome students to visit us to talk about academic concerns, study plans, finding their place at Cornell, or any questions you have and aren’t sure where to start! You can walk in to chat or contact a staff member directly to set up an appointment! See the website for more information.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#professor-email",
    "href": "course-support.html#professor-email",
    "title": "Course support",
    "section": "Professor Email",
    "text": "Professor Email\nIf you are not available during office hours times or have a questions later in the evening or other times outside of class, email your professor at tgeorge@cornellcollege.edu. If your question involves code - it is very likely you will need to meet with him to get help. Please reach out with any concerns you have during the course!",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#ebersole-health-and-wellbeing-center",
    "href": "course-support.html#ebersole-health-and-wellbeing-center",
    "title": "Course support",
    "section": "Ebersole Health and Wellbeing Center",
    "text": "Ebersole Health and Wellbeing Center\nThe mission of Cornell College Student Health Services complements the mission of the college by promoting the optimal well-being of students. We do this by:\n\nproviding and coordinating quality health care services\nadvocating for students in their pursuit of health and wellness\npreparing students to be their own health advocates and informed consumers of appropriate health care services\nproviding health education to promote the development of healthy lifestyles\n\nThe Student Health Center is located in the Ebersole Building, directly south of the Thomas Commons. Appointments are preferred. You can schedule an appointment online or by phone at 319-895-4292. Walk-ins will be accommodated as time permits. Appointments with the nurse are free.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#technology-support",
    "href": "course-support.html#technology-support",
    "title": "Course support",
    "section": "Technology Support",
    "text": "Technology Support\nIf you have issues with your computer during the block, IT may be able to help. Please submit a ticket.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#course-materials-costs",
    "href": "course-support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.).",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful links",
    "section": "",
    "text": "RStudio Server\n🔗 on Cornell College Cluster\n\n\nCourse GitHub organization\n🔗 on GitHub\n\n\nGradebook\n🔗 on Moodle",
    "crumbs": [
      "Course Contents",
      "Useful links"
    ]
  },
  {
    "objectID": "course-links.html#course-links",
    "href": "course-links.html#course-links",
    "title": "Useful links",
    "section": "",
    "text": "RStudio Server\n🔗 on Cornell College Cluster\n\n\nCourse GitHub organization\n🔗 on GitHub\n\n\nGradebook\n🔗 on Moodle",
    "crumbs": [
      "Course Contents",
      "Useful links"
    ]
  },
  {
    "objectID": "course-links.html#other-useful-links",
    "href": "course-links.html#other-useful-links",
    "title": "Useful links",
    "section": "Other Useful Links",
    "text": "Other Useful Links\n\nData Wrangling and Viz Interactive Tutorials\nRStudio Cheatsheets\nIntroduction to dplyr\nR Date Examples\nNY Times Cornell College Sign-up\nR for Data Science 2nd Ed\nQuarto Documentation\nData visualization\n\nggplot2 Reference\nggplot2: Elegant Graphics for Data Analysis\nData Visualization: A Practice Introduction\nPatchwork R Package",
    "crumbs": [
      "Course Contents",
      "Useful links"
    ]
  },
  {
    "objectID": "course-links.html#data-links",
    "href": "course-links.html#data-links",
    "title": "Useful links",
    "section": "Data Links",
    "text": "Data Links\n\nTidyTuesday\nR Data Sources for Regression Analysis\nFiveThirtyEight data\nAmazon Registry of Open Data\nOpen data StackExchange\nMicrosoft R Application Window\nData.gov\nUS Census\nNew York City data\nGeorge Mason University Data Link List\nToward Data Science list of Data Sources\nNHS Scotland Open Data\nEdinburgh Open Data\nOpen access to Scotland’s official statistics\nBikeshare data portal\nUK Gov Data\nKaggle datasets\nOpenIntro datasets\nAwesome public datasets\nYouth Risk Behavior Surveillance System (YRBSS)\nPRISM Data Archive Project\nHarvard Dataverse\nAndrew G. Reiter Poly Scie Datasets\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies\nU.S. Data\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nIf you know of others, let me know!",
    "crumbs": [
      "Course Contents",
      "Useful links"
    ]
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "To access computing resources the course, Introduction to Data Science, offered by the Cornell College Department of Mathematics and Statistics, go to the RStudio Server while on campus and connected to campus internet.\nYour account will be pre-created before the class begins and will use your Cornell College username. The default password will be shared in class and you will need to change it.",
    "crumbs": [
      "Course information",
      "R/RStudio Access"
    ]
  },
  {
    "objectID": "course-instructor.html",
    "href": "course-instructor.html",
    "title": "Instructor",
    "section": "",
    "text": "Dr. Tyler George (he/him) is a Assistant Professor of Statistics at Cornell College. He received his PhD in Statistics and Analytics from Central Michigan University. During his PhD he also studied mathematics and statistics education. His dissertation work involved creating a new lack of fit test for linear regression models. His interests are broadly in statistics, data science and best pedagogy to teach them.\n\n\n\nOffice hours\nLocation\n\n\n\n\nMonday - Thursday 3:05pm - 4:05pm\nWest 311\n\n\nOther Times by Appointment\nWest 311\n\n\n\nOffice hours are for STUDENTS. Please take advantage of them to get help with class, advising, and/or getting to know your professor! If you miss class, check out course calendar to verify there have been no changes.",
    "crumbs": [
      "Course information",
      "Instructor"
    ]
  },
  {
    "objectID": "course-instructor.html#instructor",
    "href": "course-instructor.html#instructor",
    "title": "Instructor",
    "section": "",
    "text": "Dr. Tyler George (he/him) is a Assistant Professor of Statistics at Cornell College. He received his PhD in Statistics and Analytics from Central Michigan University. During his PhD he also studied mathematics and statistics education. His dissertation work involved creating a new lack of fit test for linear regression models. His interests are broadly in statistics, data science and best pedagogy to teach them.\n\n\n\nOffice hours\nLocation\n\n\n\n\nMonday - Thursday 3:05pm - 4:05pm\nWest 311\n\n\nOther Times by Appointment\nWest 311\n\n\n\nOffice hours are for STUDENTS. Please take advantage of them to get help with class, advising, and/or getting to know your professor! If you miss class, check out course calendar to verify there have been no changes.",
    "crumbs": [
      "Course information",
      "Instructor"
    ]
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "Introduction to Statistical Learning",
    "section": "",
    "text": "Course Description\nThis course will introduce students to relatively new and powerful statistical techniques used to analyze data. The course will begin with a review of linear regression and an introduction to computer-based variable and model selection methods. Other topics will include classification methods, resampling methods for model-building, non-linear models, and tree-based methods. The computer software program R will be used throughout.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "hw/hw-01-intro-review.html",
    "href": "hw/hw-01-intro-review.html",
    "title": "Homework 1",
    "section": "",
    "text": "R is the name of the programming language itself and RStudio is a convenient interface.\nThe main goal of this homework is to re-introduce you to R and RStudio, which we will be using throughout the course both to learn the statistical concepts discussed in the course and to analyze real data and come to informed conclusions.\nAs the homework’s progress, you are encouraged to explore beyond what the homework dictates; a willingness to experiment will make you a much better programmer. Before we get to that stage, however, you need to build some basic fluency in R. Today we begin with the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands."
  },
  {
    "objectID": "hw/hw-01-intro-review.html#this-one-time",
    "href": "hw/hw-01-intro-review.html#this-one-time",
    "title": "Homework 1",
    "section": "This One Time",
    "text": "This One Time\n\nGo to our RStudio Server at http://turing.cornellcollege.edu:8787/\nClick File tab on the bottom right and then click the work Home.\nCreate a new folder using the little folder icon with the green plus on it. Use STA 362 in the folder name."
  },
  {
    "objectID": "hw/hw-01-intro-review.html#every-homeworklabactivity",
    "href": "hw/hw-01-intro-review.html#every-homeworklabactivity",
    "title": "Homework 1",
    "section": "Every Homework/lab/activity",
    "text": "Every Homework/lab/activity\nEach of your assignments will begin with the following steps.\n\nFinding the instructions on our website: https://sta362-sb8-24.github.io/STA362StatLearning/\nGoing to our RStudio Server at http://turing.cornellcollege.edu:8787/\nCreating a new project. and giving it a sensible name such as homework_1 and having that project in the course folder you created.\nCreate a new quarto document and give it a sensible name such as hw1.\nIn the YAML add the following (add what you don’t have). The embed-resources component will make your final rendered html self-contained.\n\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\n    embed-resources: true\n---"
  },
  {
    "objectID": "hw/hw-01-intro-review.html#yaml",
    "href": "hw/hw-01-intro-review.html#yaml",
    "title": "Homework 1",
    "section": "YAML",
    "text": "YAML\nIn your Quarto (qmd) file in your project, change the author name to your name, and render the document. Make sure that you also have added the extra YAML clode above."
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section",
    "href": "slides/01-02-welcome_to_sl.html#section",
    "title": "Chapter 1 and 2",
    "section": "👋",
    "text": "👋\nTyler George\n   tgeorge@cornellcollege.edu\n   MWTh 3:05pm-4:05pm and by appt."
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#course-website",
    "href": "slides/01-02-welcome_to_sl.html#course-website",
    "title": "Chapter 1 and 2",
    "section": "Course Website",
    "text": "Course Website\nhttps://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#intros",
    "href": "slides/01-02-welcome_to_sl.html#intros",
    "title": "Chapter 1 and 2",
    "section": "Intros",
    "text": "Intros\n\nName\nMajor\nFun OR boring fact"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#statistical-learning-problems",
    "href": "slides/01-02-welcome_to_sl.html#statistical-learning-problems",
    "title": "Chapter 1 and 2",
    "section": "Statistical Learning Problems",
    "text": "Statistical Learning Problems\n\n\n\nIdentify risk factors for breast cancer\n\n\n\n\n\n\nDr. Tyler George adapted from slides by Hastie & Tibshirani"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#statistical-learning-problems-1",
    "href": "slides/01-02-welcome_to_sl.html#statistical-learning-problems-1",
    "title": "Chapter 1 and 2",
    "section": "Statistical Learning Problems",
    "text": "Statistical Learning Problems\n\n\n\nCustomize an email spam detection system\n\n\n\nData: 4601 labeled emails sent to George who works at HP Labs\nInput features: frequencies of words and punctuation\n\n\n\n\n\n\n—\ngeorge\nyou\nhp\nfree\n!\nedu\nremove\n\n\n\n\nspam\n0.00\n2.26\n0.02\n0.52\n0.51\n0.01\n0.28\n\n\nemail\n2.27\n1.27\n0.90\n0.07\n0.11\n0.29\n0.01\n\n\n\n\nDr. Tyler George adapted from slides by Hastie & Tibshirani"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#statistical-learning-problems-2",
    "href": "slides/01-02-welcome_to_sl.html#statistical-learning-problems-2",
    "title": "Chapter 1 and 2",
    "section": "Statistical Learning Problems",
    "text": "Statistical Learning Problems\n\n\n\nIdentify numbers in handwritten zip code\n\n\n\n\n\n\n\nDr. Tyler George adapted from slides by Hastie & Tibshirani"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#statistical-learning-problems-3",
    "href": "slides/01-02-welcome_to_sl.html#statistical-learning-problems-3",
    "title": "Chapter 1 and 2",
    "section": "Statistical Learning Problems",
    "text": "Statistical Learning Problems\n\n\nEstablish the relationship between variables in population survey data\n\nIncome survey data for males from the central Atlantic region of US, 2009\n\n\n\n\nDr. Tyler George adapted from slides by Hastie & Tibshirani"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#types-of-statistical-learning",
    "href": "slides/01-02-welcome_to_sl.html#types-of-statistical-learning",
    "title": "Chapter 1 and 2",
    "section": "✌️ types of statistical learning",
    "text": "✌️ types of statistical learning\n\nSupervised Learning\nUnsupervised Learning"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#supervised-learning",
    "href": "slides/01-02-welcome_to_sl.html#supervised-learning",
    "title": "Chapter 1 and 2",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nDr. Tyler George adapted from slides by Hastie & Tibshirani\n\n\noutcome variable: \\(Y\\), (dependent variable, response, target)\npredictors: vector of \\(p\\) predictors, \\(X\\), (inputs, regressors, covariates, features, independent variables)\nIn the regression problem, \\(Y\\) is quantitative (e.g price, blood pressure)\nIn the classification problem, \\(Y\\) takes values in a finite, unordered set (survived/died, digit 0-9, cancer class of tissue sample)\nWe have training data \\((x_1, y_1), \\dots, (x_N, y_N)\\). These are observations (examples, instances) of these measurements"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#supervised-learning-1",
    "href": "slides/01-02-welcome_to_sl.html#supervised-learning-1",
    "title": "Chapter 1 and 2",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nWhat do you think are some objectives here?\n\nObjectives\n\nAccurately predict unseen test cases\nUnderstand which inputs affect the outcome, and how\nAssess the quality of our predictions and inferences\n\n\nDr. Tyler George adapted from slides by Hastie & Tibshirani"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#unsupervised-learning",
    "href": "slides/01-02-welcome_to_sl.html#unsupervised-learning",
    "title": "Chapter 1 and 2",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nDr. Tyler George adapted from slides by Hastie & Tibshirani\n\n\nNo outcome variable, just a set of predictors (features) measured on a set of samples\nobjective is more fuzzy – find groups of samples that behave similarly, find features that behave similarly, find linear combinations of features with the most variation\ndifficult to know how well your are doing\ndifferent from supervised learning, but can be useful as a pre-processing step for supervised learning"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#lets-take-a-tour---class-website",
    "href": "slides/01-02-welcome_to_sl.html#lets-take-a-tour---class-website",
    "title": "Chapter 1 and 2",
    "section": "Let’s take a tour - class website",
    "text": "Let’s take a tour - class website\n\n\n\n\n\nConcepts introduced:\n\nHow to find slides\nHow to find assignments\nHow to find RStudio\nHow to get help\nHow to find policies"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-and-classification",
    "href": "slides/01-02-welcome_to_sl.html#regression-and-classification",
    "title": "Chapter 1 and 2",
    "section": "Regression and Classification",
    "text": "Regression and Classification\n\nRegression: quantitative response\nClassification: qualitative (categorical) response"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-and-classification-1",
    "href": "slides/01-02-welcome_to_sl.html#regression-and-classification-1",
    "title": "Chapter 1 and 2",
    "section": "Regression and Classification",
    "text": "Regression and Classification\n\nWhat would be an example of a regression problem?\n\n\nRegression: quantitative response\nClassification: qualitative (categorical) response"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-and-classification-2",
    "href": "slides/01-02-welcome_to_sl.html#regression-and-classification-2",
    "title": "Chapter 1 and 2",
    "section": "Regression and Classification",
    "text": "Regression and Classification\n\nWhat would be an example of a classification problem?\n\n\nRegression: quantitative response\nClassification: qualitative (categorical) response"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#auto-data",
    "href": "slides/01-02-welcome_to_sl.html#auto-data",
    "title": "Chapter 1 and 2",
    "section": "Auto data",
    "text": "Auto data\n\n\n\n\n\n\n\n\n\n\n\nAbove are mpg vs horsepower, weight, and acceleration, with a blue linear-regression line fit separately to each. Can we predict mpg using these three?\n\nMaybe we can do better using a model:\n\\[\\texttt{mpg} \\approx f(\\texttt{horsepower}, \\texttt{weight}, \\texttt{acceleration})\\]"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#notation",
    "href": "slides/01-02-welcome_to_sl.html#notation",
    "title": "Chapter 1 and 2",
    "section": "Notation",
    "text": "Notation\n\nmpg is the response variable, the outcome variable, we refer to this as \\(Y\\)\nhorsepower is a feature, input, predictor, we refer to this as \\(X_1\\)\nweight is \\(X_2\\)\nacceleration is \\(X_3\\)\nOur input vector is:\n\n\\(X = \\begin{bmatrix} X_1 \\\\X_2 \\\\X_3\\end{bmatrix}\\)\n\nOur model is\n\n\\(Y = f(X) + \\varepsilon\\)\n\n\\(\\varepsilon\\) is our error"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#why-do-we-care-about-fx",
    "href": "slides/01-02-welcome_to_sl.html#why-do-we-care-about-fx",
    "title": "Chapter 1 and 2",
    "section": "Why do we care about \\(f(X)\\)?",
    "text": "Why do we care about \\(f(X)\\)?\n\nWe can use \\(f(X)\\) to make predictions of \\(Y\\) for new values of \\(X = x\\)\nWe can gain a better understanding of which components of \\(X = (X_1, X_2, \\dots, X_p)\\) are important for explaining \\(Y\\)\nDepending on how complex \\(f\\) is, maybe we can understand how each component ( \\(X_j\\) ) of \\(X\\) affects \\(Y\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx",
    "href": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx",
    "title": "Chapter 1 and 2",
    "section": "How do we choose \\(f(X)\\)?",
    "text": "How do we choose \\(f(X)\\)?\n\nWhat is a good value for \\(f(X)\\) at any selected value of \\(X\\), say \\(X = 100\\)? There can be many \\(Y\\) values at \\(X = 100\\)."
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx-1",
    "href": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx-1",
    "title": "Chapter 1 and 2",
    "section": "How do we choose \\(f(X)\\)?",
    "text": "How do we choose \\(f(X)\\)?\n\nWhat is a good value for \\(f(X)\\) at any selected value of \\(X\\), say \\(X = 100\\)? There can be many \\(Y\\) values at \\(X = 100\\)."
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx-2",
    "href": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx-2",
    "title": "Chapter 1 and 2",
    "section": "How do we choose \\(f(X)\\)?",
    "text": "How do we choose \\(f(X)\\)?\n\nWhat is a good value for \\(f(X)\\) at any selected value of \\(X\\), say \\(X = 100\\)? There can be many \\(Y\\) values at \\(X = 100\\).\n\nThere are 17 points here, what value should I choose for f(100). What do you think the blue dot represents?"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx-3",
    "href": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx-3",
    "title": "Chapter 1 and 2",
    "section": "How do we choose \\(f(X)\\)?",
    "text": "How do we choose \\(f(X)\\)?\n\nA good value is\n\\[f(100) = E(Y|X = 100)\\]\n\n\\(E(Y|X = 100)\\) means expected value (average) of \\(Y\\) given \\(X = 100\\)\n\n\nThis ideal \\(f(x) = E(Y | X = x)\\) is called the regression function"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-function-fx",
    "href": "slides/01-02-welcome_to_sl.html#regression-function-fx",
    "title": "Chapter 1 and 2",
    "section": "Regression function, \\(f(X)\\)",
    "text": "Regression function, \\(f(X)\\)\n\nAlso works or a vector, \\(X\\), for example,\n\n\\[f(x) = f(x_1, x_2, x_3) = E[Y | X_1 = x_1, X_2 = x_2, X_3 = x_3]\\]\n\nThis is the optimal predictor of \\(Y\\) in terms of mean-squared prediction error"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-function-fx-1",
    "href": "slides/01-02-welcome_to_sl.html#regression-function-fx-1",
    "title": "Chapter 1 and 2",
    "section": "Regression function, \\(f(X)\\)",
    "text": "Regression function, \\(f(X)\\)\n\n\\(f(x) = E(Y|X = x)\\) is the function that minimizes \\(E[(Y - g(X))^2 |X = x]\\) over all functions \\(g\\) at all points \\(X = x\\)\n\n\n\\(\\varepsilon = Y - f(x)\\) is the irreducible error\neven if we knew \\(f(x)\\), we would still make errors in prediction, since at each \\(X = x\\) there is typically a distribution of possible \\(Y\\) values"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-function-fx-2",
    "href": "slides/01-02-welcome_to_sl.html#regression-function-fx-2",
    "title": "Chapter 1 and 2",
    "section": "Regression function, \\(f(X)\\)",
    "text": "Regression function, \\(f(X)\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-function-fx-3",
    "href": "slides/01-02-welcome_to_sl.html#regression-function-fx-3",
    "title": "Chapter 1 and 2",
    "section": "Regression function, \\(f(X)\\)",
    "text": "Regression function, \\(f(X)\\)\n\n\nUsing these points, how would I calculate the regression function?\n\n\n\nTake the average! \\(f(100) = E[\\texttt{mpg}|\\texttt{horsepower} = 100] = 19.6\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-function-fx-4",
    "href": "slides/01-02-welcome_to_sl.html#regression-function-fx-4",
    "title": "Chapter 1 and 2",
    "section": "Regression function, \\(f(X)\\)",
    "text": "Regression function, \\(f(X)\\)\n\n\nThis point has a \\(Y\\) value of 32.9. What is \\(\\hat\\varepsilon\\)?\n\n\n\\(\\hat\\varepsilon = Y - \\hat{f}(X) = 32.9 - 19.6 = \\color{red}{13.3}\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#the-error",
    "href": "slides/01-02-welcome_to_sl.html#the-error",
    "title": "Chapter 1 and 2",
    "section": "The error",
    "text": "The error\nFor any estimate, \\(\\hat{f}(x)\\), of \\(f(x)\\), we have\n\\[E[(Y - \\hat{f}(x))^2 | X = x] = \\underbrace{[f(x) - \\hat{f}(x)]^2}_{\\textrm{reducible error}} + \\underbrace{Var(\\varepsilon)}_{\\textrm{irreducible error}}\\]\n\nAssume for a moment that both \\(\\hat{f}\\) and X are fixed.\n\\(E(Y − \\hat{Y})^2\\) represents the average, or expected value, of the squared difference between the predicted and actual value of Y, and Var( \\(\\varepsilon\\) ) represents the variance associated with the error term\nThe focus of this class is on techniques for estimating f with the aim of minimizing the reducible error.\nthe irreducible error will always provide an upper bound on the accuracy of our prediction for Y\nThis bound is almost always unknown in practice"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#estimating-f",
    "href": "slides/01-02-welcome_to_sl.html#estimating-f",
    "title": "Chapter 1 and 2",
    "section": "Estimating \\(f\\)",
    "text": "Estimating \\(f\\)\n\nTypically we have very few (if any!) data points at \\(X=x\\) exactly, so we cannot compute \\(E[Y|X=x]\\)\nFor example, what if we were interested in estimating miles per gallon when horsepower was 104.\n\n\n\n\n\n\n\n\n\n\n\n\n\n💡 We can relax the definition and let\n\\[\\hat{f}(x) = E[Y | X\\in \\mathcal{N}(x)]\\]\n\nWhere \\(\\mathcal{N}(x)\\) is some neighborhood of \\(x\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#notation-pause",
    "href": "slides/01-02-welcome_to_sl.html#notation-pause",
    "title": "Chapter 1 and 2",
    "section": "Notation pause!",
    "text": "Notation pause!\n\n\\[\\hat{f}(x) = \\underbrace{E}_{\\textrm{The expectation}}[\\underbrace{Y}_{\\textrm{of Y}} \\underbrace{|}_{\\textrm{given}} \\underbrace{X\\in \\mathcal{N}(x)}_{\\textrm{X is in the neighborhood of x}}]\\]\n\n\n🚨 If you need a notation pause at any point during this class, please let me know!"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#estimating-f-1",
    "href": "slides/01-02-welcome_to_sl.html#estimating-f-1",
    "title": "Chapter 1 and 2",
    "section": "Estimating \\(f\\)",
    "text": "Estimating \\(f\\)\n💡 We can relax the definition and let\n\\[\\hat{f}(x) = E[Y | X\\in \\mathcal{N}(x)]\\]\n\nNearest neighbor averaging does pretty well with small \\(p\\) ( \\(p\\leq 4\\) ) and large \\(n\\)\nNearest neighbor is not great when \\(p\\) is large because of the curse of dimensionality (because nearest neighbors tend to be far away in high dimensions)\n\n\n\nWhat do I mean by \\(p\\)? What do I mean by \\(n\\)?"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#parametric-models",
    "href": "slides/01-02-welcome_to_sl.html#parametric-models",
    "title": "Chapter 1 and 2",
    "section": "Parametric models",
    "text": "Parametric models\nA common parametric model is a linear model\n\\[f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p\\]\n\nA linear model has \\(p + 1\\) parameters ( \\(\\beta_0,\\dots,\\beta_p\\) )\nWe estimate these parameters by fitting a model to training data\nAlthough this model is almost never correct it can often be a good interpretable approximation to the unknown true function, \\(f(X)\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-1",
    "href": "slides/01-02-welcome_to_sl.html#section-1",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "The  red  points are simulated values for income from the model:\n\n\\[\\texttt{income} = f(\\texttt{education, senority}) + \\varepsilon\\]\n\n\\(f\\) is the  blue  surface"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-2",
    "href": "slides/01-02-welcome_to_sl.html#section-2",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Linear regression model fit to the simulated data\n\\[\\hat{f}_L(\\texttt{education, senority}) = \\hat{\\beta}_0 + \\hat{\\beta}_1\\texttt{education}+\\hat{\\beta}_2\\texttt{senority}\\]"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-3",
    "href": "slides/01-02-welcome_to_sl.html#section-3",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "More flexible regression model \\(\\hat{f}_S(\\texttt{education, seniority})\\) fit to the simulated data.\nHere we use a technique called a thin-plate spline to fit a flexible surface"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-4",
    "href": "slides/01-02-welcome_to_sl.html#section-4",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "And even MORE flexible 😱 model \\(\\hat{f}(\\texttt{education, seniority})\\).\n\nHere we’ve basically drawn the surface to hit every point, minimizing the error, but completely overfitting"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#finding-balance",
    "href": "slides/01-02-welcome_to_sl.html#finding-balance",
    "title": "Chapter 1 and 2",
    "section": "🤹 Finding balance",
    "text": "🤹 Finding balance\n\nPrediction accuracy versus interpretability\nLinear models are easy to interpret, thin-plate splines are not\nGood fit versus overfit or underfit\nHow do we know when the fit is just right?\nParsimony versus black-box\nWe often prefer a simpler model involving fewer variables over a black-box predictor involving them all"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy",
    "href": "slides/01-02-welcome_to_sl.html#accuracy",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\nWe’ve fit a model \\(\\hat{f}(x)\\) to some training data.\nWe can measure accuracy as the average squared prediction error over that train data\n\n\\[MSE_{\\texttt{train}} = \\textrm{Ave}_{train}[y_i-\\hat{f}(x_i)]^2\\]\n\n\nWhat can go wrong here?\n\n\nThis may be biased towards overfit models"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy-1",
    "href": "slides/01-02-welcome_to_sl.html#accuracy-1",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\n\nI have some train data, plotted above. What \\(\\hat{f}(x)\\) would minimize the \\(MSE_{\\texttt{train}}\\)?\n\n\\[MSE_{\\texttt{train}} = \\textrm{Ave}_{train}[y_i-\\hat{f}(x_i)]^2\\]"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy-2",
    "href": "slides/01-02-welcome_to_sl.html#accuracy-2",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\n\nI have some train data, plotted above. What \\(\\hat{f}(x)\\) would minimize the \\(MSE_{\\texttt{train}}\\)?\n\n\\[MSE_{train} = \\textrm{Ave}_{i\\in\\texttt{train}}[y_i-\\hat{f}(x_i)]^2\\]"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy-3",
    "href": "slides/01-02-welcome_to_sl.html#accuracy-3",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\n\nWhat is wrong with this?\n\n\nIt’s overfit!"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy-4",
    "href": "slides/01-02-welcome_to_sl.html#accuracy-4",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\nIf we get a new sample, that overfit model is probably going to be terrible!"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy-5",
    "href": "slides/01-02-welcome_to_sl.html#accuracy-5",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\nWe’ve fit a model \\(\\hat{f}(x)\\) to some training data.\nInstead of measuring accuracy as the average squared prediction error over that train data, we can compute it using fresh test data.\n\n\\[MSE_{\\texttt{test}} = \\textrm{Ave}_{test}[y_i-\\hat{f}(x_i)]^2\\]"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-6",
    "href": "slides/01-02-welcome_to_sl.html#section-6",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Black curve is the “truth” on the left.  Red  curve on right is \\(MSE_{\\texttt{test}}\\), grey curve is \\(MSE_{\\texttt{train}}\\). Orange, blue and green curves/squares correspond to fis of different flexibility."
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-7",
    "href": "slides/01-02-welcome_to_sl.html#section-7",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Here the truth is smoother, so the smoother fit and linear model do really well"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-8",
    "href": "slides/01-02-welcome_to_sl.html#section-8",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Here the truth is wiggly and the noise is low, so the more flexible fits do the best"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#bias-variance-trade-off",
    "href": "slides/01-02-welcome_to_sl.html#bias-variance-trade-off",
    "title": "Chapter 1 and 2",
    "section": "Bias-variance trade-off",
    "text": "Bias-variance trade-off\n\nWe’ve fit a model, \\(\\hat{f}(x)\\), to some training data\nLet’s pull a test observation from this population ( \\(x_0, y_0\\) )\nThe true model is \\(Y = f(x) + \\varepsilon\\)\n\\(f(x) = E[Y|X=x]\\)\n\n\n\\[E(y_0 - \\hat{f}(x_0))^2 = \\textrm{Var}(\\hat{f}(x_0)) + [\\textrm{Bias}(\\hat{f}(x_0))]^2 + \\textrm{Var}(\\varepsilon)\\]\n\n\nThe expectation averages over the variability of \\(y_0\\) as well as the variability of the training data. \\(\\textrm{Bias}(\\hat{f}(x_0)) =E[\\hat{f}(x_0)]-f(x_0)\\)\n\nAs flexibility of \\(\\hat{f}\\) \\(\\uparrow\\), its variance \\(\\uparrow\\) and its bias \\(\\downarrow\\)\nchoosing the flexibility based on average test error amounts to a bias-variance trade-off\n\n\n\nThat U-shape we see for the test MSE curves is due to this bias-variance trade-off\nThe expected test MSE for a given \\(x_0\\) can be decomposed into three components: the variance of \\(\\hat{f}(x_o)\\), the squared bias of \\(\\hat{f}(x_o)\\) and t4he variance of the error term \\(\\varepsilon\\)\nHere the notation \\(E[y_0 − \\hat{f}(x_0)]^2\\) defines the expected test MSE, and refers to the average test MSE that we would obtain if we repeatedly estimated \\(f\\) using a large number of training sets, and tested each at \\(x_0\\)\nThe overall expected test MSE can be computed by averaging \\(E[y_0 − \\hat{f}(x_0)]^2\\) over all possible values of \\(x_0\\) in the test set.\nSO we want to minimize the expected test error, so to do that we need to pick a statistical learning method to simultenously acheive low bias and low variance.\nSince both of these quantities are non-negative, the expected test MSE can never fall below Var( \\(\\varepsilon\\) )"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#bias-variance-trade-off-1",
    "href": "slides/01-02-welcome_to_sl.html#bias-variance-trade-off-1",
    "title": "Chapter 1 and 2",
    "section": "Bias-variance trade-off",
    "text": "Bias-variance trade-off"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#conceptual-idea",
    "href": "slides/01-02-welcome_to_sl.html#conceptual-idea",
    "title": "Chapter 1 and 2",
    "section": "Conceptual Idea",
    "text": "Conceptual Idea\nWatch StatQuest video: Machine Learning Fundamentals: Bias and Variance"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#notation-1",
    "href": "slides/01-02-welcome_to_sl.html#notation-1",
    "title": "Chapter 1 and 2",
    "section": "Notation",
    "text": "Notation\n\n\\(Y\\) is the response variable. It is qualitative\n\\(\\mathcal{C}(X)\\) is the classifier that assigns a class \\(\\mathcal{C}\\) to some future unlabeled observation, \\(X\\)\nExamples:\nEmail can be classified as \\(\\mathcal{C}=(\\texttt{spam, not spam})\\)\nWritten number is one of \\(\\mathcal{C}=\\{0, 1, 2, \\dots, 9\\}\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#classification-problem",
    "href": "slides/01-02-welcome_to_sl.html#classification-problem",
    "title": "Chapter 1 and 2",
    "section": "Classification Problem",
    "text": "Classification Problem\n\nWhat is the goal?\n\n\nBuild a classifier \\(\\mathcal{C}(X)\\) that assigns a class label from \\(\\mathcal{C}\\) to a future unlabeled observation \\(X\\)\n\nAssess the uncertainty in each classification\n\nUnderstand the roles of the different predictors among \\(X = (X_1, X_2, \\dots, X_p)\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-9",
    "href": "slides/01-02-welcome_to_sl.html#section-9",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Suppose there are \\(K\\) elements in \\(\\mathcal{C}\\), numbered \\(1, 2, \\dots, K\\)\n\\[p_k(x) = P(Y = k|X=x), k = 1, 2, \\dots, K\\] These are conditional class probabilities at \\(x\\)\n\n\nHow do you think we could calculate this?\n\n\n\n\nIn the plot, you could examine the mini-barplot at \\(x = 5\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-10",
    "href": "slides/01-02-welcome_to_sl.html#section-10",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Suppose there are \\(K\\) elements in \\(\\mathcal{C}\\), numbered \\(1, 2, \\dots, K\\)\n\\[p_k(x) = P(Y = k|X=x), k = 1, 2, \\dots, K\\] These are conditional class probabilities at \\(x\\)\n\nThe Bayes optimal classifier at \\(x\\) is\n\n\\[\\mathcal{C}(x) = j \\textrm{ if } p_j(x) = \\textrm{max}\\{p_1(x), p_2(x), \\dots, p_K(x)\\}\\]\n\n\nNotice that probability is a conditional probability\nIt is the probability that Y equals k given the observed preditor vector, \\(x\\)\nLet’s say we were using a Bayes Classifier for a two class problem, Y is 1 or 2. We would predict that the class is one if \\(P(Y=1|X=x_0)&gt;0.5\\) and 2 otherwise"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-11",
    "href": "slides/01-02-welcome_to_sl.html#section-11",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "What if this was our data and there were no points at exactly \\(x = 5\\)? Then how could we calculate this?\n\n\nNearest neighbor like before!\nThis does break down as the dimensions grow, but the impact of \\(\\mathcal{\\hat{C}}(x)\\) is less than on \\(\\hat{p}_k(x), k = 1,2,\\dots,K\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy-6",
    "href": "slides/01-02-welcome_to_sl.html#accuracy-6",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\nMisclassification error rate\n\n\\[Err_{\\texttt{test}} = \\frac{\\#correct predictions}{total predictions} = \\textrm{Ave}_{test}I[y_i\\neq \\mathcal{\\hat{C}}(x_i)]\\] &gt; * \\(I(\\cdot)\\) is an indicator function and will only be eitehr 0 or 1.\n\nThe Bayes Classifier using the true \\(p_k(x)\\) has the smallest error\nSome of the methods we (may) learn build structured models for \\(\\mathcal{C}(x)\\) (support vector machines, for example)\nSome build structured models for \\(p_k(x)\\) (logistic regression, for example)\n\n\n\nthe test error rate \\(\\textrm{Ave}_{i\\in\\texttt{test}}I[y_i\\neq \\mathcal{\\hat{C}}(x_i)]\\) is minimized on average by very simple classifier that assigns each observation to the most likely class, given its predictor values (that’s the Bayes classifier)\n\n\n\n\n\n\n🔗 https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#cross-validation-1",
    "href": "slides/05-cv-tidymodels.html#cross-validation-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Cross validation",
    "text": "Cross validation\n💡 Big idea\n\n\nWe have determined that it is sensible to use a test set to calculate metrics like prediction error\n\n\n\nWhy?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#cross-validation-2",
    "href": "slides/05-cv-tidymodels.html#cross-validation-2",
    "title": "Chapter 5 and tidymodels",
    "section": "Cross validation",
    "text": "Cross validation\n💡 Big idea\n\n\nWe have determined that it is sensible to use a test set to calculate metrics like prediction error\n\n\n\nHow could we do this?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#cross-validation-3",
    "href": "slides/05-cv-tidymodels.html#cross-validation-3",
    "title": "Chapter 5 and tidymodels",
    "section": "Cross validation",
    "text": "Cross validation\n💡 Big idea\n\n\nWe have determined that it is sensible to use a test set to calculate metrics like prediction error\nWhat if we don’t have a separate data set to test our model on?\n🎉 We can use resampling methods to estimate the test-set prediction error"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#training-error-versus-test-error",
    "href": "slides/05-cv-tidymodels.html#training-error-versus-test-error",
    "title": "Chapter 5 and tidymodels",
    "section": "Training error versus test error",
    "text": "Training error versus test error\n\nWhat is the difference? Which is typically larger?\n\n\nThe training error is calculated by using the same observations used to fit the statistical learning model\nThe test error is calculated by using a statistical learning method to predict the response of new observations\nThe training error rate typically underestimates the true prediction error rate"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#estimating-prediction-error",
    "href": "slides/05-cv-tidymodels.html#estimating-prediction-error",
    "title": "Chapter 5 and tidymodels",
    "section": "Estimating prediction error",
    "text": "Estimating prediction error\n\nBest case scenario: We have a large data set to test our model on\nThis is not always the case!\n\n\n💡 Let’s instead find a way to estimate the test error by holding out a subset of the training observations from the model fitting process, and then applying the statistical learning method to those held out observations"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set",
    "text": "Approach #1: Validation set\n\nRandomly divide the available set up samples into two parts: a training set and a validation set\nFit the model on the training set, calculate the prediction error on the validation set\n\n\n\nIf we have a quantitative predictor what metric would we use to calculate this test error?\n\n\nOften we use Mean Squared Error (MSE)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set-1",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set",
    "text": "Approach #1: Validation set\n\n\nRandomly divide the available set up samples into two parts: a training set and a validation set\nFit the model on the training set, calculate the prediction error on the validation set\n\n\n\nIf we have a qualitative predictor what metric would we use to calculate this test error?\n\n\nOften we use misclassification rate"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set-2",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set-2",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set",
    "text": "Approach #1: Validation set\n\n\n\\[\\Large\\color{orange}{MSE_{\\texttt{test-split}} = \\textrm{Ave}_{i\\in\\texttt{test-split}}[y_i-\\hat{f}(x_i)]^2}\\]\n\n\n\\[\\Large\\color{orange}{Err_{\\texttt{test-split}} = \\textrm{Ave}_{i\\in\\texttt{test-split}}I[y_i\\neq \\mathcal{\\hat{C}}(x_i)]}\\]"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set-3",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set-3",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set",
    "text": "Approach #1: Validation set\nAuto example:\n\n\nWe have 392 observations.\nTrying to predict mpg from horsepower.\nWe can split the data in half and use 196 to fit the model and 196 to test"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set-4",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set-4",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set",
    "text": "Approach #1: Validation set\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split}}}\\)\n\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split}}}\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split}}}\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split}}}\\)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set-5",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set-5",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set",
    "text": "Approach #1: Validation set\nAuto example:\n\n\nWe have 392 observations.\nTrying to predict mpg from horsepower.\nWe can split the data in half and use 196 to fit the model and 196 to test - what if we did this many times?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set-drawbacks",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set-drawbacks",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set (Drawbacks)",
    "text": "Approach #1: Validation set (Drawbacks)\n\nthe validation estimate of the test error can be highly variable, depending on which observations are included in the training set and which observations are included in the validation set\nIn the validation approach, only a subset of the observations (those that are included in the training set rather than in the validation set) are used to fit the model\nTherefore, the validation set error may tend to overestimate the test error for the model fit on the entire data set"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-2-k-fold-cross-validation",
    "href": "slides/05-cv-tidymodels.html#approach-2-k-fold-cross-validation",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #2: K-fold cross validation",
    "text": "Approach #2: K-fold cross validation\n💡 The idea is to do the following:\n\nRandomly divide the data into \\(K\\) equal-sized parts\nLeave out part \\(k\\), fit the model to the other \\(K - 1\\) parts (combined)\nObtain predictions for the left-out \\(k\\)th part\nDo this for each part \\(k = 1, 2,\\dots K\\), and then combine the result"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#k-fold-cross-validation",
    "href": "slides/05-cv-tidymodels.html#k-fold-cross-validation",
    "title": "Chapter 5 and tidymodels",
    "section": "K-fold cross validation",
    "text": "K-fold cross validation\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split-1}}}\\)\n\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split-2}}}\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split-3}}}\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split-4}}}\\)\nTake the mean of the \\(k\\) MSE values"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise",
    "href": "slides/05-cv-tidymodels.html#application-exercise",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\nIf we use 10 folds:\n\n\nWhat percentage of the training data is used in each analysis for each fold?\nWhat percentage of the training data is used in the assessment for each fold?\n\n\n\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#estimating-prediction-error-quantitative-outcome",
    "href": "slides/05-cv-tidymodels.html#estimating-prediction-error-quantitative-outcome",
    "title": "Chapter 5 and tidymodels",
    "section": "Estimating prediction error (quantitative outcome)",
    "text": "Estimating prediction error (quantitative outcome)\n\nSplit the data into K parts, where \\(C_1, C_2, \\dots, C_k\\) indicate the indices of observations in part \\(k\\)\n\\(CV_{(K)} = \\sum_{k=1}^K\\frac{n_k}{n}MSE_k\\)\n\\(MSE_k = \\sum_{i \\in C_k} (y_i - \\hat{y}_i)^2/n_k\\)\n\\(n_k\\) is the number of observations in group \\(k\\)\n\\(\\hat{y}_i\\) is the fit for observation \\(i\\) obtained from the data with the part \\(k\\) removed\nIf we set \\(K = n\\), we’d have \\(n-fold\\) cross validation which is the same as leave-one-out cross validation (LOOCV)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#estimating-prediction-error-quantitative-outcome-1",
    "href": "slides/05-cv-tidymodels.html#estimating-prediction-error-quantitative-outcome-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Estimating prediction error (quantitative outcome)",
    "text": "Estimating prediction error (quantitative outcome)\n\n\nSplit the data into K parts, where \\(C_1, C_2, \\dots, C_k\\) indicate the indices of observations in part \\(k\\)\n\\[CV_{(K)} = \\sum_{k=1}^K\\frac{n_k}{n}MSE_k\\]\n\\(MSE_k = \\sum_{i \\in C_k} (y_i - \\hat{y}_i)^2/n_k\\)\n\\(n_k\\) is the number of observations in group \\(k\\)\n\\(\\hat{y}_i\\) is the fit for observation \\(i\\) obtained from the data with the part \\(k\\) removed\nIf we set \\(K = n\\), we’d have \\(n-fold\\) cross validation which is the same as leave-one-out cross validation (LOOCV)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#leave-one-out-cross-validation",
    "href": "slides/05-cv-tidymodels.html#leave-one-out-cross-validation",
    "title": "Chapter 5 and tidymodels",
    "section": "Leave-one-out cross validation",
    "text": "Leave-one-out cross validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\dots\\]"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#special-case",
    "href": "slides/05-cv-tidymodels.html#special-case",
    "title": "Chapter 5 and tidymodels",
    "section": "Special Case!",
    "text": "Special Case!\n\nWith linear regression, you can actually calculate the LOOCV error without having to iterate!\n\\(CV_{(n)} = \\frac{1}{n}\\sum_{i=1}^n\\left(\\frac{y_i-\\hat{y}_i}{1-h_i}\\right)^2\\)\n\\(\\hat{y}_i\\) is the \\(i\\)th fitted value from the linear model\n\\(h_i\\) is the diagonal of the “hat” matrix (remember that! 🎓)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#picking-k",
    "href": "slides/05-cv-tidymodels.html#picking-k",
    "title": "Chapter 5 and tidymodels",
    "section": "Picking \\(K\\)",
    "text": "Picking \\(K\\)\n\n\\(K\\) can vary from 2 (splitting the data in half each time) to \\(n\\) (LOOCV)\nLOOCV is sometimes useful but usually the estimates from each fold are very correlated, so their average can have a high variance\nA better choice tends to be \\(K=5\\) or \\(K=10\\)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#bias-variance-trade-off",
    "href": "slides/05-cv-tidymodels.html#bias-variance-trade-off",
    "title": "Chapter 5 and tidymodels",
    "section": "Bias variance trade-off",
    "text": "Bias variance trade-off\n\nSince each training set is only \\((K - 1)/K\\) as big as the original training set, the estimates of prediction error will typically be biased upward\nThis bias is minimized when \\(K = n\\) (LOOCV), but this estimate has a high variance\n\\(K =5\\) or \\(K=10\\) provides a nice compromise for the bias-variance trade-off"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-2-k-fold-cross-validation-1",
    "href": "slides/05-cv-tidymodels.html#approach-2-k-fold-cross-validation-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #2: K-fold Cross Validation",
    "text": "Approach #2: K-fold Cross Validation\nAuto example:\n\n\nWe have 392 observations.\nTrying to predict mpg from horsepower"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#estimating-prediction-error-qualitative-outcome",
    "href": "slides/05-cv-tidymodels.html#estimating-prediction-error-qualitative-outcome",
    "title": "Chapter 5 and tidymodels",
    "section": "Estimating prediction error (qualitative outcome)",
    "text": "Estimating prediction error (qualitative outcome)\n\nThe premise is the same as cross valiation for quantitative outcomes\nSplit the data into K parts, where \\(C_1, C_2, \\dots, C_k\\) indicate the indices of observations in part \\(k\\)\n\\(CV_K = \\sum_{k=1}^K\\frac{n_k}{n}Err_k\\)\n\\(Err_k = \\sum_{i\\in C_k}I(y_i\\neq\\hat{y}_i)/n_k\\) (misclassification rate)\n\\(n_k\\) is the number of observations in group \\(k\\)\n\\(\\hat{y}_i\\) is the fit for observation \\(i\\) obtained from the data with the part \\(k\\) removed"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#estimating-prediction-error-qualitative-outcome-1",
    "href": "slides/05-cv-tidymodels.html#estimating-prediction-error-qualitative-outcome-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Estimating prediction error (qualitative outcome)",
    "text": "Estimating prediction error (qualitative outcome)\n\n\nThe premise is the same as cross valiation for quantitative outcomes\nSplit the data into K parts, where \\(C_1, C_2, \\dots, C_k\\) indicate the indices of observations in part \\(k\\)\n\\(CV_K = \\sum_{k=1}^K\\frac{n_k}{n}Err_k\\)\n\\(Err_k = \\sum_{i\\in C_k}I(y_i\\neq\\hat{y}_i)/n_k\\) (misclassification rate)\n\\(n_k\\) is the number of observations in group \\(k\\)\n\\(\\hat{y}_i\\) is the fit for observation \\(i\\) obtained from the data with the part \\(k\\) removed"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#big-idea",
    "href": "slides/05-cv-tidymodels.html#big-idea",
    "title": "Chapter 5 and tidymodels",
    "section": "💡 Big idea",
    "text": "💡 Big idea\n\n\nWe have determined that it is sensible to use a test set to calculate metrics like prediction error"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#big-idea-1",
    "href": "slides/05-cv-tidymodels.html#big-idea-1",
    "title": "Chapter 5 and tidymodels",
    "section": "💡 Big idea",
    "text": "💡 Big idea\n\n\nWe have determined that it is sensible to use a test set to calculate metrics like prediction error\n\n\n\nWhy?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#big-idea-2",
    "href": "slides/05-cv-tidymodels.html#big-idea-2",
    "title": "Chapter 5 and tidymodels",
    "section": "💡 Big idea",
    "text": "💡 Big idea\n\n\nWe have determined that it is sensible to use a test set to calculate metrics like prediction error\n\n\n\nHow could we do this?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#big-idea-3",
    "href": "slides/05-cv-tidymodels.html#big-idea-3",
    "title": "Chapter 5 and tidymodels",
    "section": "💡 Big idea",
    "text": "💡 Big idea\n\n\nWe have determined that it is sensible to use a test set to calculate metrics like prediction error\nWhat if we don’t have a separate data set to test our model on?\n🎉 We can use resampling methods to estimate the test-set prediction error"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#cross-validation",
    "href": "slides/05-cv-tidymodels.html#cross-validation",
    "title": "Chapter 5 and tidymodels",
    "section": "Cross validation",
    "text": "Cross validation\n💡 Big idea\n\n\nWe have determined that it is sensible to use a test set to calculate metrics like prediction error"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#setup",
    "href": "slides/05-cv-tidymodels.html#setup",
    "title": "Chapter 5 and tidymodels",
    "section": "Setup",
    "text": "Setup"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#estimating-prediction-error-quant.-outcome",
    "href": "slides/05-cv-tidymodels.html#estimating-prediction-error-quant.-outcome",
    "title": "Chapter 5 and tidymodels",
    "section": "Estimating prediction error (quant. outcome)",
    "text": "Estimating prediction error (quant. outcome)\n\nSplit the data into K parts, where \\(C_1, C_2, \\dots, C_k\\) indicate the indices of observations in part \\(k\\)\n\\(CV_{(K)} = \\sum_{k=1}^K\\frac{n_k}{n}MSE_k\\)\n\\(MSE_k = \\sum_{i \\in C_k} (y_i - \\hat{y}_i)^2/n_k\\)\n\\(n_k\\) is the number of observations in group \\(k\\)\n\\(\\hat{y}_i\\) is the fit for observation \\(i\\) obtained from the data with the part \\(k\\) removed\nIf we set \\(K = n\\), we’d have \\(n-fold\\) cross validation which is the same as leave-one-out cross validation (LOOCV)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#estimating-prediction-error-quant.-outcome-1",
    "href": "slides/05-cv-tidymodels.html#estimating-prediction-error-quant.-outcome-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Estimating prediction error (quant. outcome)",
    "text": "Estimating prediction error (quant. outcome)\n\n\nSplit the data into K parts, where \\(C_1, C_2, \\dots, C_k\\) indicate the indices of observations in part \\(k\\)\n\\[CV_{(K)} = \\sum_{k=1}^K\\frac{n_k}{n}MSE_k\\]\n\\(MSE_k = \\sum_{i \\in C_k} (y_i - \\hat{y}_i)^2/n_k\\)\n\\(n_k\\) is the number of observations in group \\(k\\)\n\\(\\hat{y}_i\\) is the fit for observation \\(i\\) obtained from the data with the part \\(k\\) removed\nIf we set \\(K = n\\), we’d have \\(n-fold\\) cross validation which is the same as leave-one-out cross validation (LOOCV)"
  }
]