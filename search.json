[
  {
    "objectID": "slides/04-logistic.html#recap",
    "href": "slides/04-logistic.html#recap",
    "title": "Chapter 4 Part 1",
    "section": "Recap",
    "text": "Recap\n\nWe had a linear regression refresher\nLinear regression is a great tool when we have a continuous outcome\nWe are going to learn some fancy ways to do even better in the future\n\nSetup:\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR)"
  },
  {
    "objectID": "slides/04-logistic.html#classification-1",
    "href": "slides/04-logistic.html#classification-1",
    "title": "Chapter 4 Part 1",
    "section": "Classification",
    "text": "Classification\n\nWhat are some examples of classification problems?\n\n\nQualitative response variable in an unordered set, \\(\\mathcal{C}\\)\neye color \\(\\in\\) {blue, brown, green}\nemail \\(\\in\\) {spam, not spam}\nResponse, \\(Y\\) takes on values in \\(\\mathcal{C}\\)\nPredictors are a vector, \\(X\\)\nThe task: build a function \\(C(X)\\) that takes \\(X\\) and predicts \\(Y\\), \\(C(X)\\in\\mathcal{C}\\)\nMany times we are actually more interested in the probabilities that \\(X\\) belongs to each category in \\(\\mathcal{C}\\)"
  },
  {
    "objectID": "slides/04-logistic.html#example-credit-card-default",
    "href": "slides/04-logistic.html#example-credit-card-default",
    "title": "Chapter 4 Part 1",
    "section": "Example: Credit card default",
    "text": "Example: Credit card default"
  },
  {
    "objectID": "slides/04-logistic.html#can-we-use-linear-regression",
    "href": "slides/04-logistic.html#can-we-use-linear-regression",
    "title": "Chapter 4 Part 1",
    "section": "Can we use linear regression?",
    "text": "Can we use linear regression?\nWe can code Default as\n\\[Y = \\begin{cases} 0 & \\textrm{if }\\texttt{No}\\\\ 1&\\textrm{if }\\texttt{Yes}\\end{cases}\\] Can we fit a linear regression of \\(Y\\) on \\(X\\) and classify as Yes if \\(\\hat{Y}&gt; 0.5\\)?\n\nIn this case of a binary outcome, linear regression is okay (it is equivalent to linear discriminant analysis, you can read more about that in your book!)\n\\(E[Y|X=x] = P(Y=1|X=x)\\), so it seems like this is a pretty good idea!\nThe problem: Linear regression can produce probabilities less than 0 or greater than 1 üò±"
  },
  {
    "objectID": "slides/04-logistic.html#can-we-use-linear-regression-1",
    "href": "slides/04-logistic.html#can-we-use-linear-regression-1",
    "title": "Chapter 4 Part 1",
    "section": "Can we use linear regression?",
    "text": "Can we use linear regression?\nWe can code Default as\n\\[Y = \\begin{cases} 0 & \\textrm{if }\\texttt{No}\\\\ 1&\\textrm{if }\\texttt{Yes}\\end{cases}\\] Can we fit a linear regression of \\(Y\\) on \\(X\\) and classify as Yes if \\(\\hat{Y}&gt; 0.5\\)?\n\nWhat may do a better job?\n\n\nLogistic regression!"
  },
  {
    "objectID": "slides/04-logistic.html#linear-versus-logistic-regression",
    "href": "slides/04-logistic.html#linear-versus-logistic-regression",
    "title": "Chapter 4 Part 1",
    "section": "Linear versus logistic regression",
    "text": "Linear versus logistic regression\n\n\nWhich does a better job at predicting the probability of default?\n\n\nThe orange marks represent the response \\(Y\\in\\{0,1\\}\\)"
  },
  {
    "objectID": "slides/04-logistic.html#linear-regression",
    "href": "slides/04-logistic.html#linear-regression",
    "title": "Chapter 4 Part 1",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhat if we have \\(&gt;2\\) possible outcomes? For example, someone comes to the emergency room and we need to classify them according to their symptoms\n\\[\n\\begin{align}\nY = \\begin{cases} 1& \\textrm{if }\\texttt{stroke}\\\\2&\\textrm{if }\\texttt{drug overdose}\\\\3&\\textrm{if }\\texttt{epileptic seizure}\\end{cases}\n\\end{align}\n\\]\n\nWhat could go wrong here?\n\n\nThe coding implies an ordering\nThe coding implies equal spacing (that is the difference between stroke and drug overdose is the same as drug overdose and epileptic seizure)"
  },
  {
    "objectID": "slides/04-logistic.html#linear-regression-1",
    "href": "slides/04-logistic.html#linear-regression-1",
    "title": "Chapter 4 Part 1",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhat if we have \\(&gt;2\\) possible outcomes? For example, someone comes to the emergency room and we need to classify them according to their symptoms\n\\[\n\\begin{align}\nY = \\begin{cases} 1& \\textrm{if }\\texttt{stroke}\\\\2&\\textrm{if }\\texttt{drug overdose}\\\\3&\\textrm{if }\\texttt{epileptic seizure}\\end{cases}\n\\end{align}\n\\]\n\nLinear regression is not appropriate here\nMutliclass logistic regression or discriminant analysis are more appropriate"
  },
  {
    "objectID": "slides/04-logistic.html#logistic-regression",
    "href": "slides/04-logistic.html#logistic-regression",
    "title": "Chapter 4 Part 1",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\\[\np(X) = \\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}\n\\]\n\nNote: \\(p(X)\\) is shorthand for \\(P(Y=1|X)\\)\nNo matter what values \\(\\beta_0\\), \\(\\beta_1\\), or \\(X\\) take \\(p(X)\\) will always be between 0 and 1"
  },
  {
    "objectID": "slides/04-logistic.html#logistic-regression-1",
    "href": "slides/04-logistic.html#logistic-regression-1",
    "title": "Chapter 4 Part 1",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\\[\np(X) = \\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}\n\\]\nWe can rearrange this into the following form:\n\\[\n\\log\\left(\\frac{p(X)}{1-p(X)}\\right) = \\beta_0 + \\beta_1 X\n\\]\n\nWhat is this transformation called?\n\n\nThis is a log odds or logit transformation of \\(p(X)\\)"
  },
  {
    "objectID": "slides/04-logistic.html#linear-versus-logistic-regression-1",
    "href": "slides/04-logistic.html#linear-versus-logistic-regression-1",
    "title": "Chapter 4 Part 1",
    "section": "Linear versus logistic regression",
    "text": "Linear versus logistic regression\n\nLogistic regression ensures that our estimates for \\(p(X)\\) are between 0 and 1 üéâ"
  },
  {
    "objectID": "slides/04-logistic.html#maximum-likelihood",
    "href": "slides/04-logistic.html#maximum-likelihood",
    "title": "Chapter 4 Part 1",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\nRefresher: How did we estimate \\(\\hat\\beta\\) in linear regression?"
  },
  {
    "objectID": "slides/04-logistic.html#maximum-likelihood-1",
    "href": "slides/04-logistic.html#maximum-likelihood-1",
    "title": "Chapter 4 Part 1",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\nRefresher: How did we estimate \\(\\hat\\beta\\) in linear regression?\n\nIn logistic regression, we use maximum likelihood to estimate the parameters\n\\[\\mathcal{l}(\\beta_0,\\beta_1)=\\prod_{i:y_i=1}p(x_i)\\prod_{i:y_i=0}(1-p(x_i))\\]\n\nThis likelihood give the probability of the observed ones and zeros in the data\nWe pick \\(\\beta_0\\) and \\(\\beta_1\\) to maximize the likelihood\nWe‚Äôll let R do the heavy lifting here"
  },
  {
    "objectID": "slides/04-logistic.html#lets-see-it-in-r",
    "href": "slides/04-logistic.html#lets-see-it-in-r",
    "title": "Chapter 4 Part 1",
    "section": "Let‚Äôs see it in R",
    "text": "Let‚Äôs see it in R\n\n\n# A tibble: 2 √ó 5\n  term         estimate std.error statistic   p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) -10.7      0.361        -29.5 3.62e-191\n2 balance       0.00550  0.000220      25.0 1.98e-137\n\n\n\nUse the logistic_reg() function in R with the glm engine"
  },
  {
    "objectID": "slides/04-logistic.html#making-predictions",
    "href": "slides/04-logistic.html#making-predictions",
    "title": "Chapter 4 Part 1",
    "section": "Making predictions",
    "text": "Making predictions\n\nWhat is our estimated probability of default for someone with a balance of $1000?\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.6513306\n0.3611574\n-29.49221\n0\n\n\nbalance\n0.0054989\n0.0002204\n24.95309\n0\n\n\n\n\n\n\n\n\n\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}{1+e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}=\\frac{e^{-10.65+0.0055\\times 1000}}{1+e^{-10.65+0.0055\\times 1000}}=0.006\n\\]"
  },
  {
    "objectID": "slides/04-logistic.html#making-predictions-1",
    "href": "slides/04-logistic.html#making-predictions-1",
    "title": "Chapter 4 Part 1",
    "section": "Making predictions",
    "text": "Making predictions\n\nWhat is our estimated probability of default for someone with a balance of $2000?\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.6513306\n0.3611574\n-29.49221\n0\n\n\nbalance\n0.0054989\n0.0002204\n24.95309\n0\n\n\n\n\n\n\n\n\n\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}{1+e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}=\\frac{e^{-10.65+0.0055\\times 2000}}{1+e^{-10.65+0.0055\\times 2000}}=0.586\n\\]"
  },
  {
    "objectID": "slides/04-logistic.html#logistic-regression-example",
    "href": "slides/04-logistic.html#logistic-regression-example",
    "title": "Chapter 4 Part 1",
    "section": "Logistic regression example",
    "text": "Logistic regression example\nLet‚Äôs refit the model to predict the probability of default given the customer is a student\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-3.5041278\n0.0707130\n-49.554219\n0.0000000\n\n\nstudentYes\n0.4048871\n0.1150188\n3.520181\n0.0004313\n\n\n\n\n\n\n\n\n\\[P(\\texttt{default = Yes}|\\texttt{student = Yes}) = \\frac{e^{-3.5041+0.4049\\times1}}{1+e^{-3.5041+0.4049\\times1}}=0.0431\\]\n\n\nHow will this change if student = No?\n\n\n\n\\[P(\\texttt{default = Yes}|\\texttt{student = No}) = \\frac{e^{-3.5041+0.4049\\times0}}{1+e^{-3.5041+0.4049\\times0}}=0.0292\\]"
  },
  {
    "objectID": "slides/04-logistic.html#multiple-logistic-regression",
    "href": "slides/04-logistic.html#multiple-logistic-regression",
    "title": "Chapter 4 Part 1",
    "section": "Multiple logistic regression",
    "text": "Multiple logistic regression\n\\[\\log\\left(\\frac{p(X)}{1-p(X)}\\right)=\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p\\] \\[p(X) = \\frac{e^{\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p}}{1+e^{\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p}}\\]\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.8690452\n0.4922555\n-22.080088\n0.0000000\n\n\nbalance\n0.0057365\n0.0002319\n24.737563\n0.0000000\n\n\nincome\n0.0000030\n0.0000082\n0.369815\n0.7115203\n\n\nstudentYes\n-0.6467758\n0.2362525\n-2.737646\n0.0061881\n\n\n\n\n\n\n\n\n\nWhy is the coefficient for student negative now when it was positive before?"
  },
  {
    "objectID": "slides/04-logistic.html#confounding",
    "href": "slides/04-logistic.html#confounding",
    "title": "Chapter 4 Part 1",
    "section": "Confounding",
    "text": "Confounding\n\n\nWhat is going on here?"
  },
  {
    "objectID": "slides/04-logistic.html#confounding-1",
    "href": "slides/04-logistic.html#confounding-1",
    "title": "Chapter 4 Part 1",
    "section": "Confounding",
    "text": "Confounding\n\n\nStudents tend to have higher balances than non-students\nTheir marginal default rate is higher\nFor each level of balance, students default less\nTheir conditional default rate is lower"
  },
  {
    "objectID": "slides/04-logistic.html#logistic-regression-for-more-than-two-classes",
    "href": "slides/04-logistic.html#logistic-regression-for-more-than-two-classes",
    "title": "Chapter 4 Part 1",
    "section": "Logistic regression for more than two classes",
    "text": "Logistic regression for more than two classes\n\\[P(Y=k|X) = \\frac{e ^{\\beta_{0k}+\\beta_{1k}X_1+\\dots+\\beta_{pk}X_p}}{\\sum_{l=1}^Ke^{\\beta_{0l}+\\beta_{1l}X_1+\\dots+\\beta_{pl}X_p}}\\]\n\nSo far we‚Äôve discussed binary outcome data\nWe can generalize this to situations with multiple classes\nHere we have a linear function for each of the \\(K\\) classes\nThis is known as multinomial logistic regression"
  },
  {
    "objectID": "slides/04-logistic.html#a-bit-about-odds",
    "href": "slides/04-logistic.html#a-bit-about-odds",
    "title": "Chapter 4 Part 1",
    "section": "A bit about ‚Äúodds‚Äù",
    "text": "A bit about ‚Äúodds‚Äù\n\nThe ‚Äúodds‚Äù tell you how likely an event is\nüåÇ Let‚Äôs say there is a 60% chance of rain today * What is the probability that it will rain?\n\\(p = 0.6\\)\nWhat is the probability that it won‚Äôt rain?\n\\(1-p = 0.4\\)\nWhat are the odds that it will rain?\n3 to 2, 3:2, \\(\\frac{0.6}{0.4} = 1.5\\)"
  },
  {
    "objectID": "slides/04-logistic.html#transforming-logs",
    "href": "slides/04-logistic.html#transforming-logs",
    "title": "Chapter 4 Part 1",
    "section": "Transforming logs",
    "text": "Transforming logs\n\nHow do you ‚Äúundo‚Äù a \\(\\log\\) base \\(e\\)?\nUse \\(e\\)! For example:\n\\(e^{\\log(10)} = 10\\)\n\\(e^{\\log(1283)} = 1283\\)\n\\(e^{\\log(x)} = x\\)"
  },
  {
    "objectID": "slides/04-logistic.html#transforming-logs-1",
    "href": "slides/04-logistic.html#transforming-logs-1",
    "title": "Chapter 4 Part 1",
    "section": "Transforming logs",
    "text": "Transforming logs\n\nHow would you get the odds from the log(odds)?\n\n\n\nHow do you ‚Äúundo‚Äù a \\(\\log\\) base \\(e\\)?\nUse \\(e\\)! For example:\n\\(e^{\\log(10)} = 10\\)\n\\(e^{\\log(1283)} = 1283\\)\n\\(e^{\\log(x)} = x\\)\n\n\n\n\\(e^{\\log(odds)}\\) = odds"
  },
  {
    "objectID": "slides/04-logistic.html#transforming-odds",
    "href": "slides/04-logistic.html#transforming-odds",
    "title": "Chapter 4 Part 1",
    "section": "Transforming odds",
    "text": "Transforming odds\n\nodds = \\(\\frac{\\pi}{1-\\pi}\\)\nSolving for \\(\\pi\\)\n\\(\\pi = \\frac{\\textrm{odds}}{1+\\textrm{odds}}\\)\nPlugging in \\(e^{\\log(odds)}\\) = odds\n\\(\\pi = \\frac{e^{\\log(odds)}}{1+e^{\\log(odds)}}\\)\nPlugging in \\(\\log(odds) = \\beta_0 + \\beta_1x\\)\n\\(\\pi = \\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}\\)"
  },
  {
    "objectID": "slides/04-logistic.html#the-logistic-model",
    "href": "slides/04-logistic.html#the-logistic-model",
    "title": "Chapter 4 Part 1",
    "section": "The logistic model",
    "text": "The logistic model\n\n‚úåÔ∏è forms\n\n\n\n\n\n\n\n\nForm\nModel\n\n\n\n\nLogit form\n\\(\\log\\left(\\frac{\\pi}{1-\\pi}\\right) = \\beta_0 + \\beta_1x\\)\n\n\nProbability form\n\\(\\Large\\pi = \\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}\\)"
  },
  {
    "objectID": "slides/04-logistic.html#the-logistic-model-1",
    "href": "slides/04-logistic.html#the-logistic-model-1",
    "title": "Chapter 4 Part 1",
    "section": "The logistic model",
    "text": "The logistic model\n\n\n\nprobability\nodds\nlog(odds)\n\n\n\n\n\\(\\pi\\)\n\\(\\frac{\\pi}{1-\\pi}\\)\n\\(\\log\\left(\\frac{\\pi}{1-\\pi}\\right)=l\\)\n\n\n\n‚¨ÖÔ∏è\n\n\n\nlog(odds)\nodds\nprobability\n\n\n\n\n\\(l\\)\n\\(e^l\\)\n\\(\\frac{e^l}{1+e^l} = \\pi\\)"
  },
  {
    "objectID": "slides/04-logistic.html#the-logistic-model-2",
    "href": "slides/04-logistic.html#the-logistic-model-2",
    "title": "Chapter 4 Part 1",
    "section": "The logistic model",
    "text": "The logistic model\n\n‚úåÔ∏è forms\nlog(odds): \\(l = \\beta_0 + \\beta_1x\\)\nP(Outcome = Yes): \\(\\Large\\pi =\\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}\\)"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios",
    "href": "slides/04-logistic.html#odds-ratios",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later. (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-1",
    "href": "slides/04-logistic.html#odds-ratios-1",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat is the explanatory variable?\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-2",
    "href": "slides/04-logistic.html#odds-ratios-2",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat type of variable is this?\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-3",
    "href": "slides/04-logistic.html#odds-ratios-3",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat is the outcome variable?\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-4",
    "href": "slides/04-logistic.html#odds-ratios-4",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat type of variable is this?\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-5",
    "href": "slides/04-logistic.html#odds-ratios-5",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\n\n\nTMS\nPlacebo\nTotal\n\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\nWe can compare the results using odds\nWhat are the odds of being pain-free for the placebo group?\n\\((22/100)/(78/100) = 22/78 = 0.282\\)\nWhat are the odds of being pain-free for the treatment group?\n\\(39/61 = 0.639\\)\nComparing the odds what can we conclude?\nTMS increases the likelihood of success"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-6",
    "href": "slides/04-logistic.html#odds-ratios-6",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\n\n\nTMS\nPlacebo\nTotal\n\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\nWe can summarize this relationship with an odds ratio: the ratio of the two odds\n\\(\\Large OR = \\frac{39/61}{22/78} = \\frac{0.639}{0.282} = 2.27\\)\n‚Äúthe odds of being pain free were 2.27 times higher with TMS than with the placebo‚Äù"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-7",
    "href": "slides/04-logistic.html#odds-ratios-7",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat if we wanted to calculate this in terms of Not pain-free (with pain-free) as the referent?\n\n\n\n\nTMS\nPlacebo\nTotal\n\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\n\\(\\Large OR = \\frac{61/39}{78/22} = \\frac{1.564}{3.545} = 0.441\\)\nthe odds for still being in pain for the TMS group are 0.441 times the odds of being in pain for the placebo group"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-8",
    "href": "slides/04-logistic.html#odds-ratios-8",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat changed here?\n\n\n\n\nTMS\nPlacebo\nTotal\n\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\n\\(\\Large OR = \\frac{78/22}{61/39} = \\frac{3.545}{1.564} = 2.27\\)\nthe odds for still being in pain for the placebo group are 2.27 times the odds of being in pain for the TMS group"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-9",
    "href": "slides/04-logistic.html#odds-ratios-9",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nIn general, it‚Äôs more natural to interpret odds ratios &gt; 1, you can flip the referent to do so\n\n\n\nTMS\nPlacebo\nTotal\n\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\\(\\Large OR = \\frac{78/22}{61/39} = \\frac{3.545}{1.564} = 2.27\\)\nthe odds for still being in pain for the placebo group are 2.27 times the odds of being in pain for the TMS group"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-10",
    "href": "slides/04-logistic.html#odds-ratios-10",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nLet‚Äôs look at some Titanic data. We are interested in whether the passenger reported being female is related to whether they survived.\n\n\n\nFemale\nMale\nTotal\n\n\n\n\n\nSurvived\n308\n142\n450\n\n\nDied\n154\n709\n863\n\n\nTotal\n462\n851\n1313"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-11",
    "href": "slides/04-logistic.html#odds-ratios-11",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat are the odds of surviving for females versus males?\n\n\n\n\nFemale\nMale\nTotal\n\n\n\n\n\nSurvived\n308\n142\n450\n\n\nDied\n154\n709\n863\n\n\nTotal\n462\n851\n1313\n\n\n\n\\[\\Large OR = \\frac{308/154}{142/709} = \\frac{2}{0.2} = 9.99\\]"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-12",
    "href": "slides/04-logistic.html#odds-ratios-12",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nHow do you interpret this?\n\n\n\n\nFemale\nMale\nTotal\n\n\n\n\n\nSurvived\n308\n142\n450\n\n\nDied\n154\n709\n863\n\n\nTotal\n462\n851\n1313\n\n\n\n\\[\\Large OR = \\frac{308/154}{142/709} = \\frac{2}{0.2} = 9.99\\] the odds of surviving for the female passengers was 9.99 times the odds of surviving for the male passengers"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-13",
    "href": "slides/04-logistic.html#odds-ratios-13",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat if we wanted to fit a model? What would the equation be?\n\n\n\n\nFemale\nMale\nTotal\n\n\n\n\n\nSurvived\n308\n142\n450\n\n\nDied\n154\n709\n863\n\n\nTotal\n462\n851\n1313\n\n\n\n\n\\[\\Large \\log(\\textrm{odds of survival}) = \\beta_0 + \\beta_1 \\textrm{Female}\\]"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-14",
    "href": "slides/04-logistic.html#odds-ratios-14",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\\[\\Large \\log(\\textrm{odds of survival}) = \\beta_0 + \\beta_1 \\textrm{Female}\\]\n\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -1.61    0.0919     -17.5 1.70e-68\n2 Sexfemale       2.30    0.135       17.1 2.91e-65"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-15",
    "href": "slides/04-logistic.html#odds-ratios-15",
    "title": "Chapter 4 Part 1",
    "section": "Odds Ratios",
    "text": "Odds Ratios\n\nHow do you interpret this result?\n\n\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -1.61    0.0919     -17.5 1.70e-68\n2 Sexfemale       2.30    0.135       17.1 2.91e-65"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-16",
    "href": "slides/04-logistic.html#odds-ratios-16",
    "title": "Chapter 4 Part 1",
    "section": "Odds Ratios",
    "text": "Odds Ratios\n\nHow do you interpret this result?\n\n\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.200    0.0919     -17.5 1.70e-68\n2 Sexfemale      9.99     0.135       17.1 2.91e-65\n\n\n[1] 9.99"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-17",
    "href": "slides/04-logistic.html#odds-ratios-17",
    "title": "Chapter 4 Part 1",
    "section": "Odds Ratios",
    "text": "Odds Ratios\n\nHow do you interpret this result?\n\n\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.200    0.0919     -17.5 1.70e-68\n2 Sexfemale      9.99     0.135       17.1 2.91e-65\n\n\n[1] 9.99\n\n\nthe odds of surviving for the female passengers was 9.99 times the odds of surviving for the male passengers"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-18",
    "href": "slides/04-logistic.html#odds-ratios-18",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nWhat if the explanatory variable is continuous?\n\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -19.2       5.63     -3.41 0.000644\n2 GPA             5.45      1.58      3.45 0.000553\n\n\nA one unit increase in GPA yields a 5.45 increase in the log odds of acceptance"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-19",
    "href": "slides/04-logistic.html#odds-ratios-19",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nWhat if the explanatory variable is continuous?\n\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  4.56e-9      5.63     -3.41 0.000644\n2 GPA          2.34e+2      1.58      3.45 0.000553\n\n\nA one unit increase in GPA yields a 234-fold increase in the odds of acceptance\n\nüò± that seems huge! Remember: the interpretation of these coefficients depends on your units (the same as in ordinary linear regression)."
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-20",
    "href": "slides/04-logistic.html#odds-ratios-20",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nHow could we get the odds associated with increasing GPA by 0.1?\n\n\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -19.2       5.63     -3.41 0.000644\n2 GPA             5.45      1.58      3.45 0.000553\n\n\n\n\n[1] 234\n\n\n[1] 1.73\n\n\nA one-tenth unit increase in GPA yields a 1.73-fold increase in the odds of acceptance"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-21",
    "href": "slides/04-logistic.html#odds-ratios-21",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nHow could we get the odds associated with increasing GPA by 0.1?\n\n\n\n# A tibble: 2 √ó 5\n  term             estimate std.error statistic  p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) 0.00000000456     5.63      -3.41 0.000644\n2 GPA_10      1.73              0.158      3.45 0.000553\n\n\nA one-tenth unit increase in GPA yields a 1.73-fold increase in the odds of acceptance"
  },
  {
    "objectID": "slides/04-logistic.html#application-exercise",
    "href": "slides/04-logistic.html#application-exercise",
    "title": "Chapter 4 Part 1",
    "section": " Application Exercise",
    "text": "Application Exercise\nUsing the Default data from the ISLR package, fit a logistic regression model predicting whether a customer defaults with whether they are a student and their current balance.\nHere is some code to get you started:\n\n\n\n‚àí+\n05:00\n\n\n\n\n\n\n\nüîó https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "labs/01-cv-tm.html",
    "href": "labs/01-cv-tm.html",
    "title": "Lab 01",
    "section": "",
    "text": "Go to our RStudio and create a new R project inside your class folder."
  },
  {
    "objectID": "labs/01-cv-tm.html#yaml",
    "href": "labs/01-cv-tm.html#yaml",
    "title": "Lab 01",
    "section": "YAML:",
    "text": "YAML:\nOpen the .qmd file in your project, make sure the author is your name, and Render the document."
  },
  {
    "objectID": "labs/01-cv-tm.html#conceptual-questions",
    "href": "labs/01-cv-tm.html#conceptual-questions",
    "title": "Lab 01",
    "section": "Conceptual questions",
    "text": "Conceptual questions\n\nExplain how k-fold Cross Validation is implemented.\nWhat are the advantages / disadvantages of k-fold Cross Validation compared to the Validation Set approach? What are the advantages / disadvantages of k-fold Cross Validation compared to Leave-one-out Cross Validation?"
  },
  {
    "objectID": "labs/01-cv-tm.html#data-exploration",
    "href": "labs/01-cv-tm.html#data-exploration",
    "title": "Lab 01",
    "section": "Data exploration",
    "text": "Data exploration\n\nFor this analysis, we are using the Auto dataset from the ISLR package. How many rows are in this dataset? How many columns? Is there any missing data?\nOur outcome of interest is miles per gallon: mpg. Create a publication-ready figure examining the distribution of this variable.\nOur main predictor of interest is horsepower. Create a publication-ready figure looking at the relationship between miles per gallon and horsepower."
  },
  {
    "objectID": "labs/01-cv-tm.html#k-fold-cross-validation",
    "href": "labs/01-cv-tm.html#k-fold-cross-validation",
    "title": "Lab 01",
    "section": "K-fold cross validation",
    "text": "K-fold cross validation\nWe are trying to decide between three models of varying flexibility:\n\nModel 1: \\(\\texttt{mpg} = \\beta_0 + \\beta_1 \\texttt{horsepower} + \\epsilon\\)\nModel 2: \\(\\texttt{mpg} = \\beta_0 + \\beta_1 \\texttt{horsepower} + \\beta_2 \\texttt{horsepower}^2 + \\epsilon\\)\nModel 3: \\(\\texttt{mpg} = \\beta_0 + \\beta_1 \\texttt{horsepower} + \\beta_2 \\texttt{horsepower}^2 + \\beta_3 \\texttt{horsepower}^3 + \\epsilon\\)\n\n\nUsing the Auto data, split the data into two groups a training data set, saved as Auto_train and a testing data set, saved as Auto_test. Be sure to set a seed to ensure that you get the same result each time you Render your document.\n\n\n\nYou can use the poly() function to fit a model with a polynomial term. For example, to fit the model \\(y = \\beta_0 + \\beta_1 \\texttt{x} + \\beta_2 \\texttt{x}^2 + \\beta_3 \\texttt{x}^3 + \\epsilon\\), you would run fit(lm_spec, y ~ poly(x, 3), data = data)\n\nFit the three models outlined above on the training data. Using the model created on the training data, predict mpg in the test data set for each model. What is the test RMSE for the three models? Which model would you choose?\nFit the same three models, but instead of the validation set approach, perform 5-fold cross validation. Make sure to set a seed so you get the same answer each time you run the analysis. Calculate the overall 5-fold cross validation error for each of the three models. Which model would you chose?\nThe tidymodels package allows you to do this faster! Instead of having a fit 3 (or more!) different models to determine the best flexibility, you can (1) create a recipe to specify how you would like to fit a model and then (2) tune this model to determine the best output. Copy the code below. What do you think the line step_poly(horsepower, degree = tune()) does? Hint: you can run ?step_poly in the Console to learn more about this function.\n\n\nauto_prep &lt;- Auto |&gt;\n  recipe(mpg ~ horsepower) |&gt;\n  step_poly(horsepower, degree = tune())\n\n\nTo tune this model, you will replace fit_resamples with tune_grid. The pseudo code to do this is below - you may need to update some names to match what you have named objects in your document. Add the code to tune your model based on the code below.\n\n\nauto_tune &lt;- tune_grid(lm_spec,\n          auto_prep,\n          resamples = auto_cv)\n\n\nUsing the collect_metrics function, look at the RMSE for auto_tune. Which degree is preferable?\nYou can plot the output from Exercise 11 to make it a bit easier to determine. First, save your output from Exercise 11 as auto_metrics. Then filter this data frame to only include rows where .metric == \"rmse\". Save this filtered data frame as auto_rmse. Edit the code below to plot the degree on the x-axis and mean on the y-axis. Describe what this plot shows.\n\n\nggplot(auto_rmse, aes(x = ----, y = ----)) + \n  geom_line() +\n  geom_pointrange(aes(ymin = mean - std_err, ymax = mean + std_err)) + \n  labs(x = \"Degree\",\n       y = \"Cross validation error\",\n       title = ---)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html",
    "href": "slides/05-cv-tidymodels.html",
    "title": "Chapter 5 and tidymodels",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ISLR)\nlibrary(tidymodels)\nlibrary(gridExtra)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#setup",
    "href": "slides/05-cv-tidymodels.html#setup",
    "title": "Chapter 5 and tidymodels",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(ISLR)\nlibrary(tidymodels)\nlibrary(gridExtra)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#cross-validation",
    "href": "slides/05-cv-tidymodels.html#cross-validation",
    "title": "Chapter 5 and tidymodels",
    "section": "Cross validation",
    "text": "Cross validation\nüí° Big idea\n\n\nWe have determined that it is sensible to use a test set to calculate metrics like prediction error"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#cross-validation-1",
    "href": "slides/05-cv-tidymodels.html#cross-validation-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Cross validation",
    "text": "Cross validation\nüí° Big idea\n\n\nWe have determined that it is sensible to use a test set to calculate metrics like prediction error\n\n\n\nWhy?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#cross-validation-2",
    "href": "slides/05-cv-tidymodels.html#cross-validation-2",
    "title": "Chapter 5 and tidymodels",
    "section": "Cross validation",
    "text": "Cross validation\nüí° Big idea\n\n\nWe have determined that it is sensible to use a test set to calculate metrics like prediction error\n\n\n\nHow could we do this?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#cross-validation-3",
    "href": "slides/05-cv-tidymodels.html#cross-validation-3",
    "title": "Chapter 5 and tidymodels",
    "section": "Cross validation",
    "text": "Cross validation\nüí° Big idea\n\n\nWe have determined that it is sensible to use a test set to calculate metrics like prediction error\nWhat if we don‚Äôt have a separate data set to test our model on?\nüéâ We can use resampling methods to estimate the test-set prediction error"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#training-error-versus-test-error",
    "href": "slides/05-cv-tidymodels.html#training-error-versus-test-error",
    "title": "Chapter 5 and tidymodels",
    "section": "Training error versus test error",
    "text": "Training error versus test error\n\nWhat is the difference? Which is typically larger?\n\n\nThe training error is calculated by using the same observations used to fit the statistical learning model\nThe test error is calculated by using a statistical learning method to predict the response of new observations\nThe training error rate typically underestimates the true prediction error rate"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#estimating-prediction-error",
    "href": "slides/05-cv-tidymodels.html#estimating-prediction-error",
    "title": "Chapter 5 and tidymodels",
    "section": "Estimating prediction error",
    "text": "Estimating prediction error\n\nBest case scenario: We have a large data set to test our model on\nThis is not always the case!\n\n\nüí° Let‚Äôs instead find a way to estimate the test error by holding out a subset of the training observations from the model fitting process, and then applying the statistical learning method to those held out observations"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set",
    "text": "Approach #1: Validation set\n\nRandomly divide the available set up samples into two parts: a training set and a validation set\nFit the model on the training set, calculate the prediction error on the validation set\n\n\n\nIf we have a quantitative predictor what metric would we use to calculate this test error?\n\n\nOften we use Mean Squared Error (MSE)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set-1",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set",
    "text": "Approach #1: Validation set\n\n\nRandomly divide the available set up samples into two parts: a training set and a validation set\nFit the model on the training set, calculate the prediction error on the validation set\n\n\n\nIf we have a qualitative predictor what metric would we use to calculate this test error?\n\n\nOften we use misclassification rate"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set-2",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set-2",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set",
    "text": "Approach #1: Validation set\n\n\n\\[\\Large\\color{orange}{MSE_{\\texttt{test-split}} = \\textrm{Ave}_{i\\in\\texttt{test-split}}[y_i-\\hat{f}(x_i)]^2}\\]\n\n\n\\[\\Large\\color{orange}{Err_{\\texttt{test-split}} = \\textrm{Ave}_{i\\in\\texttt{test-split}}I[y_i\\neq \\mathcal{\\hat{C}}(x_i)]}\\]"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set-3",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set-3",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set",
    "text": "Approach #1: Validation set\nAuto example:\n\n\nWe have 392 observations.\nTrying to predict mpg from horsepower.\nWe can split the data in half and use 196 to fit the model and 196 to test"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set-4",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set-4",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set",
    "text": "Approach #1: Validation set\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split}}}\\)\n\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split}}}\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split}}}\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split}}}\\)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set-5",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set-5",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set",
    "text": "Approach #1: Validation set\nAuto example:\n\n\nWe have 392 observations.\nTrying to predict mpg from horsepower.\nWe can split the data in half and use 196 to fit the model and 196 to test - what if we did this many times?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set-drawbacks",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set-drawbacks",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set (Drawbacks)",
    "text": "Approach #1: Validation set (Drawbacks)\n\nthe validation estimate of the test error can be highly variable, depending on which observations are included in the training set and which observations are included in the validation set\nIn the validation approach, only a subset of the observations (those that are included in the training set rather than in the validation set) are used to fit the model\nTherefore, the validation set error may tend to overestimate the test error for the model fit on the entire data set"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-2-k-fold-cross-validation",
    "href": "slides/05-cv-tidymodels.html#approach-2-k-fold-cross-validation",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #2: K-fold cross validation",
    "text": "Approach #2: K-fold cross validation\nüí° The idea is to do the following:\n\nRandomly divide the data into \\(K\\) equal-sized parts\nLeave out part \\(k\\), fit the model to the other \\(K - 1\\) parts (combined)\nObtain predictions for the left-out \\(k\\)th part\nDo this for each part \\(k = 1, 2,\\dots K\\), and then combine the result"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#k-fold-cross-validation",
    "href": "slides/05-cv-tidymodels.html#k-fold-cross-validation",
    "title": "Chapter 5 and tidymodels",
    "section": "K-fold cross validation",
    "text": "K-fold cross validation\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split-1}}}\\)\n\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split-2}}}\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split-3}}}\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split-4}}}\\)\nTake the mean of the \\(k\\) MSE values"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise",
    "href": "slides/05-cv-tidymodels.html#application-exercise",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\nCreate a new R project, then a new quarto file with cv in its name in that project. Answer the questions in that file.\nIf we use 10 folds:\n\n\nWhat percentage of the training data is used in each analysis for each fold?\nWhat percentage of the training data is used in the assessment for each fold?\n\n\n\n\n\n‚àí+\n02:00"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#estimating-prediction-error-quantitative-outcome",
    "href": "slides/05-cv-tidymodels.html#estimating-prediction-error-quantitative-outcome",
    "title": "Chapter 5 and tidymodels",
    "section": "Estimating prediction error (quantitative outcome)",
    "text": "Estimating prediction error (quantitative outcome)\n\nSplit the data into K parts, where \\(C_1, C_2, \\dots, C_k\\) indicate the indices of observations in part \\(k\\)\n\\(CV_{(K)} = \\sum_{k=1}^K\\frac{n_k}{n}MSE_k\\)\n\\(MSE_k = \\sum_{i \\in C_k} (y_i - \\hat{y}_i)^2/n_k\\)\n\\(n_k\\) is the number of observations in group \\(k\\)\n\\(\\hat{y}_i\\) is the fit for observation \\(i\\) obtained from the data with the part \\(k\\) removed\nIf we set \\(K = n\\), we‚Äôd have \\(n-fold\\) cross validation which is the same as leave-one-out cross validation (LOOCV)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#estimating-prediction-error-quantitative-outcome-1",
    "href": "slides/05-cv-tidymodels.html#estimating-prediction-error-quantitative-outcome-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Estimating prediction error (quantitative outcome)",
    "text": "Estimating prediction error (quantitative outcome)\n\n\nSplit the data into K parts, where \\(C_1, C_2, \\dots, C_k\\) indicate the indices of observations in part \\(k\\)\n\\(CV_{(K)} = \\sum_{k=1}^K\\frac{n_k}{n}MSE_k\\)\n\\(MSE_k = \\sum_{i \\in C_k} (y_i - \\hat{y}_i)^2/n_k\\)\n\\(n_k\\) is the number of observations in group \\(k\\)\n\\(\\hat{y}_i\\) is the fit for observation \\(i\\) obtained from the data with the part \\(k\\) removed\nIf we set \\(K = n\\), we‚Äôd have \\(n-fold\\) cross validation which is the same as leave-one-out cross validation (LOOCV)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#leave-one-out-cross-validation",
    "href": "slides/05-cv-tidymodels.html#leave-one-out-cross-validation",
    "title": "Chapter 5 and tidymodels",
    "section": "Leave-one-out cross validation",
    "text": "Leave-one-out cross validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\dots\\]"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#special-case",
    "href": "slides/05-cv-tidymodels.html#special-case",
    "title": "Chapter 5 and tidymodels",
    "section": "Special Case!",
    "text": "Special Case!\n\nWith linear regression, you can actually calculate the LOOCV error without having to iterate!\n\\(CV_{(n)} = \\frac{1}{n}\\sum_{i=1}^n\\left(\\frac{y_i-\\hat{y}_i}{1-h_i}\\right)^2\\)\n\\(\\hat{y}_i\\) is the \\(i\\)th fitted value from the linear model\n\\(h_i\\) is the diagonal of the ‚Äúhat‚Äù matrix (remember that! üéì)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#picking-k",
    "href": "slides/05-cv-tidymodels.html#picking-k",
    "title": "Chapter 5 and tidymodels",
    "section": "Picking \\(K\\)",
    "text": "Picking \\(K\\)\n\n\\(K\\) can vary from 2 (splitting the data in half each time) to \\(n\\) (LOOCV)\nLOOCV is sometimes useful but usually the estimates from each fold are very correlated, so their average can have a high variance\nA better choice tends to be \\(K=5\\) or \\(K=10\\)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#bias-variance-trade-off",
    "href": "slides/05-cv-tidymodels.html#bias-variance-trade-off",
    "title": "Chapter 5 and tidymodels",
    "section": "Bias variance trade-off",
    "text": "Bias variance trade-off\n\nSince each training set is only \\((K - 1)/K\\) as big as the original training set, the estimates of prediction error will typically be biased upward\nThis bias is minimized when \\(K = n\\) (LOOCV), but this estimate has a high variance\n\\(K =5\\) or \\(K=10\\) provides a nice compromise for the bias-variance trade-off"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-2-k-fold-cross-validation-1",
    "href": "slides/05-cv-tidymodels.html#approach-2-k-fold-cross-validation-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #2: K-fold Cross Validation",
    "text": "Approach #2: K-fold Cross Validation\nAuto example:\n\n\nWe have 392 observations.\nTrying to predict mpg from horsepower"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#estimating-prediction-error-qualitative-outcome",
    "href": "slides/05-cv-tidymodels.html#estimating-prediction-error-qualitative-outcome",
    "title": "Chapter 5 and tidymodels",
    "section": "Estimating prediction error (qualitative outcome)",
    "text": "Estimating prediction error (qualitative outcome)\n\nThe premise is the same as cross valiation for quantitative outcomes\nSplit the data into K parts, where \\(C_1, C_2, \\dots, C_k\\) indicate the indices of observations in part \\(k\\)\n\\(CV_K = \\sum_{k=1}^K\\frac{n_k}{n}Err_k\\)\n\\(Err_k = \\sum_{i\\in C_k}I(y_i\\neq\\hat{y}_i)/n_k\\) (misclassification rate)\n\\(n_k\\) is the number of observations in group \\(k\\)\n\\(\\hat{y}_i\\) is the fit for observation \\(i\\) obtained from the data with the part \\(k\\) removed"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#estimating-prediction-error-qualitative-outcome-1",
    "href": "slides/05-cv-tidymodels.html#estimating-prediction-error-qualitative-outcome-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Estimating prediction error (qualitative outcome)",
    "text": "Estimating prediction error (qualitative outcome)\n\n\nThe premise is the same as cross valiation for quantitative outcomes\nSplit the data into K parts, where \\(C_1, C_2, \\dots, C_k\\) indicate the indices of observations in part \\(k\\)\n\\(CV_K = \\sum_{k=1}^K\\frac{n_k}{n}Err_k\\)\n\\(Err_k = \\sum_{i\\in C_k}I(y_i\\neq\\hat{y}_i)/n_k\\) (misclassification rate)\n\\(n_k\\) is the number of observations in group \\(k\\)\n\\(\\hat{y}_i\\) is the fit for observation \\(i\\) obtained from the data with the part \\(k\\) removed"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise-1",
    "href": "slides/05-cv-tidymodels.html#application-exercise-1",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nCreate a new quarto file in your project and add tidymodels in the name.\nLoad the packages by running the top chunk of R code\n\n\n\nlibrary(tidymodels)\nlibrary(broom)\nlibrary(ISLR)\nlibrary(countdown)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#tidymodels-1",
    "href": "slides/05-cv-tidymodels.html#tidymodels-1",
    "title": "Chapter 5 and tidymodels",
    "section": "tidymodels",
    "text": "tidymodels\n\n\n\n\ntidymodels.org\n\ntidymodels is an opinionated collection of R packages designed for modeling and statistical analysis.\nAll packages share an underlying philosophy and a common grammar."
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#step-1-specify-the-model",
    "href": "slides/05-cv-tidymodels.html#step-1-specify-the-model",
    "title": "Chapter 5 and tidymodels",
    "section": "Step 1: Specify the model",
    "text": "Step 1: Specify the model\n\nPick the model\nSet the engine"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#specify-the-model",
    "href": "slides/05-cv-tidymodels.html#specify-the-model",
    "title": "Chapter 5 and tidymodels",
    "section": "Specify the model",
    "text": "Specify the model\n\nlinear_reg() |&gt;\n  set_engine(\"lm\")"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#specify-the-model-1",
    "href": "slides/05-cv-tidymodels.html#specify-the-model-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Specify the model",
    "text": "Specify the model\n\nlinear_reg() |&gt;\n  set_engine(\"glmnet\")"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#specify-the-model-2",
    "href": "slides/05-cv-tidymodels.html#specify-the-model-2",
    "title": "Chapter 5 and tidymodels",
    "section": "Specify the model",
    "text": "Specify the model\n\nlinear_reg() |&gt;\n  set_engine(\"spark\")"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#specify-the-model-3",
    "href": "slides/05-cv-tidymodels.html#specify-the-model-3",
    "title": "Chapter 5 and tidymodels",
    "section": "Specify the model",
    "text": "Specify the model\n\ndecision_tree() |&gt;\n  set_engine(\"rpart\")"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#specify-the-model-4",
    "href": "slides/05-cv-tidymodels.html#specify-the-model-4",
    "title": "Chapter 5 and tidymodels",
    "section": "Specify the model",
    "text": "Specify the model\n\n\nAll available models:\n\ntidymodels.org"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise-2",
    "href": "slides/05-cv-tidymodels.html#application-exercise-2",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nWrite a pipe that creates a model that uses lm() to fit a linear regression using tidymodels. Save it as lm_spec and look at the object. What does it return?\n\n\nHint: you‚Äôll need https://www.tidymodels.org\n\n\n\n‚àí+\n05:00"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#answer",
    "href": "slides/05-cv-tidymodels.html#answer",
    "title": "Chapter 5 and tidymodels",
    "section": "Answer",
    "text": "Answer\n\nlm_spec &lt;- \n  linear_reg() |&gt; # Pick linear regression\n  set_engine(engine = \"lm\") # set engine\nlm_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#fit-the-data",
    "href": "slides/05-cv-tidymodels.html#fit-the-data",
    "title": "Chapter 5 and tidymodels",
    "section": "Fit the data",
    "text": "Fit the data\n\n\nYou can train your model using the fit() function\n\n\nfit(lm_spec,\n    mpg ~ horsepower,\n    data = Auto)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = mpg ~ horsepower, data = data)\n\nCoefficients:\n(Intercept)   horsepower  \n    39.9359      -0.1578"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise-3",
    "href": "slides/05-cv-tidymodels.html#application-exercise-3",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nFit the model:\n\n\nlibrary(ISLR)\nlm_fit &lt;- fit(lm_spec,\n              mpg ~ horsepower,\n              data = Auto)\nlm_fit\n\nDoes this give the same results as\n\nlm(mpg ~ horsepower, data = Auto)\n\n\n\n\n‚àí+\n03:00"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#answer-1",
    "href": "slides/05-cv-tidymodels.html#answer-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Answer",
    "text": "Answer\n\nlm_fit &lt;- fit(lm_spec,\n              mpg ~ horsepower,\n              data = Auto)\nlm_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = mpg ~ horsepower, data = data)\n\nCoefficients:\n(Intercept)   horsepower  \n    39.9359      -0.1578  \n\nlm(mpg ~ horsepower, data = Auto)\n\n\nCall:\nlm(formula = mpg ~ horsepower, data = Auto)\n\nCoefficients:\n(Intercept)   horsepower  \n    39.9359      -0.1578"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#get-predictions",
    "href": "slides/05-cv-tidymodels.html#get-predictions",
    "title": "Chapter 5 and tidymodels",
    "section": "Get predictions",
    "text": "Get predictions\n\nlm_fit |&gt;\n  predict(new_data = Auto)\n\n\nUses the predict() function\n‚ÄºÔ∏è new_data has an underscore\nüòÑ This automagically creates a data frame"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#get-predictions-1",
    "href": "slides/05-cv-tidymodels.html#get-predictions-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Get predictions",
    "text": "Get predictions\n\nlm_fit |&gt;\n  predict(new_data = Auto) |&gt;\n  bind_cols(Auto)\n\n# A tibble: 392 √ó 10\n   .pred   mpg cylinders displacement horsepower weight acceleration  year origin name    \n * &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;   \n 1 19.4     18         8          307        130   3504         12      70      1 chevrol‚Ä¶\n 2 13.9     15         8          350        165   3693         11.5    70      1 buick s‚Ä¶\n 3 16.3     18         8          318        150   3436         11      70      1 plymout‚Ä¶\n 4 16.3     16         8          304        150   3433         12      70      1 amc reb‚Ä¶\n 5 17.8     17         8          302        140   3449         10.5    70      1 ford to‚Ä¶\n 6  8.68    15         8          429        198   4341         10      70      1 ford ga‚Ä¶\n 7  5.21    14         8          454        220   4354          9      70      1 chevrol‚Ä¶\n 8  6.00    14         8          440        215   4312          8.5    70      1 plymout‚Ä¶\n 9  4.42    14         8          455        225   4425         10      70      1 pontiac‚Ä¶\n10  9.95    15         8          390        190   3850          8.5    70      1 amc amb‚Ä¶\n# ‚Ñπ 382 more rows\n\n\n\n\nWhat does bind_cols do?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#get-predictions-2",
    "href": "slides/05-cv-tidymodels.html#get-predictions-2",
    "title": "Chapter 5 and tidymodels",
    "section": "Get predictions",
    "text": "Get predictions\n\nlm_fit |&gt;\n  predict(new_data = Auto) |&gt;\n  bind_cols(Auto)\n\n# A tibble: 392 √ó 10\n   .pred   mpg cylinders displacement horsepower weight acceleration  year origin name    \n * &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;   \n 1 19.4     18         8          307        130   3504         12      70      1 chevrol‚Ä¶\n 2 13.9     15         8          350        165   3693         11.5    70      1 buick s‚Ä¶\n 3 16.3     18         8          318        150   3436         11      70      1 plymout‚Ä¶\n 4 16.3     16         8          304        150   3433         12      70      1 amc reb‚Ä¶\n 5 17.8     17         8          302        140   3449         10.5    70      1 ford to‚Ä¶\n 6  8.68    15         8          429        198   4341         10      70      1 ford ga‚Ä¶\n 7  5.21    14         8          454        220   4354          9      70      1 chevrol‚Ä¶\n 8  6.00    14         8          440        215   4312          8.5    70      1 plymout‚Ä¶\n 9  4.42    14         8          455        225   4425         10      70      1 pontiac‚Ä¶\n10  9.95    15         8          390        190   3850          8.5    70      1 amc amb‚Ä¶\n# ‚Ñπ 382 more rows\n\n\n\nWhich column has the predicted values?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise-4",
    "href": "slides/05-cv-tidymodels.html#application-exercise-4",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\n\n‚àí+\n03:00\n\n\n\n\n\nEdit the code below to add the original data to the predicted data.\n\n\n\nmpg_pred &lt;- lm_fit |&gt; \n  predict(new_data = Auto) |&gt; \n  ---"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#get-predictions-3",
    "href": "slides/05-cv-tidymodels.html#get-predictions-3",
    "title": "Chapter 5 and tidymodels",
    "section": "Get predictions",
    "text": "Get predictions\n\nmpg_pred &lt;- lm_fit |&gt;\n  predict(new_data = Auto) |&gt;\n  bind_cols(Auto)\n\nmpg_pred\n\n# A tibble: 392 √ó 10\n   .pred   mpg cylinders displacement horsepower weight acceleration  year origin name    \n * &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;   \n 1 19.4     18         8          307        130   3504         12      70      1 chevrol‚Ä¶\n 2 13.9     15         8          350        165   3693         11.5    70      1 buick s‚Ä¶\n 3 16.3     18         8          318        150   3436         11      70      1 plymout‚Ä¶\n 4 16.3     16         8          304        150   3433         12      70      1 amc reb‚Ä¶\n 5 17.8     17         8          302        140   3449         10.5    70      1 ford to‚Ä¶\n 6  8.68    15         8          429        198   4341         10      70      1 ford ga‚Ä¶\n 7  5.21    14         8          454        220   4354          9      70      1 chevrol‚Ä¶\n 8  6.00    14         8          440        215   4312          8.5    70      1 plymout‚Ä¶\n 9  4.42    14         8          455        225   4425         10      70      1 pontiac‚Ä¶\n10  9.95    15         8          390        190   3850          8.5    70      1 amc amb‚Ä¶\n# ‚Ñπ 382 more rows"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#calculate-the-error",
    "href": "slides/05-cv-tidymodels.html#calculate-the-error",
    "title": "Chapter 5 and tidymodels",
    "section": "Calculate the error",
    "text": "Calculate the error\n\n\nRoot mean square error\n\n\n\nmpg_pred |&gt;\n  rmse(truth = mpg, estimate = .pred)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        4.89\n\n\n\n\nWhat is this estimate? (training error? testing error?)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#validation-set-approach",
    "href": "slides/05-cv-tidymodels.html#validation-set-approach",
    "title": "Chapter 5 and tidymodels",
    "section": "Validation set approach",
    "text": "Validation set approach\n\nAuto_split &lt;- initial_split(Auto, prop = 0.5)\nAuto_split\n\n&lt;Training/Testing/Total&gt;\n&lt;196/196/392&gt;\n\n\n\n\nHow many observations are in the training set?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#validation-set-approach-1",
    "href": "slides/05-cv-tidymodels.html#validation-set-approach-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Validation set approach",
    "text": "Validation set approach\n\nAuto_split &lt;- initial_split(Auto, prop = 0.5)\nAuto_split\n\n&lt;Training/Testing/Total&gt;\n&lt;196/196/392&gt;\n\n\n\nHow many observations are in the test set?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#validation-set-approach-2",
    "href": "slides/05-cv-tidymodels.html#validation-set-approach-2",
    "title": "Chapter 5 and tidymodels",
    "section": "Validation set approach",
    "text": "Validation set approach\n\nAuto_split &lt;- initial_split(Auto, prop = 0.5)\nAuto_split\n\n&lt;Training/Testing/Total&gt;\n&lt;196/196/392&gt;\n\n\n\nHow many observations are there in total?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#validation-set-approach-3",
    "href": "slides/05-cv-tidymodels.html#validation-set-approach-3",
    "title": "Chapter 5 and tidymodels",
    "section": "Validation set approach",
    "text": "Validation set approach\n\nAuto_split &lt;- initial_split(Auto, prop = 0.5)\nAuto_split\n\n&lt;Training/Testing/Total&gt;\n&lt;196/196/392&gt;\n\n\n\n\nExtract the training and testing data\n\n\n\ntraining(Auto_split)\ntesting(Auto_split)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#validation-set-approach-4",
    "href": "slides/05-cv-tidymodels.html#validation-set-approach-4",
    "title": "Chapter 5 and tidymodels",
    "section": "Validation set approach",
    "text": "Validation set approach\n\nAuto_train &lt;- training(Auto_split)\n\n\nAuto_train\n\n\n\n# A tibble: 196 √ó 9\n     mpg cylinders displacement horsepower weight acceleration  year origin name          \n   &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;         \n 1  37.7         4           89         62   2050         17.3    81      3 toyota tercel \n 2  27           4           97         60   1834         19      71      2 volkswagen mo‚Ä¶\n 3  22           6          232        112   2835         14.7    82      1 ford granada l\n 4  16           6          250        100   3781         17      74      1 chevrolet che‚Ä¶\n 5  25           4           90         71   2223         16.5    75      2 volkswagen da‚Ä¶\n 6  18           6          232        100   2945         16      73      1 amc hornet    \n 7  38.1         4           89         60   1968         18.8    80      3 toyota coroll‚Ä¶\n 8  23           4           97         54   2254         23.5    72      2 volkswagen ty‚Ä¶\n 9  15           8          302        130   4295         14.9    77      1 mercury couga‚Ä¶\n10  34           4          108         70   2245         16.9    82      3 toyota corolla\n# ‚Ñπ 186 more rows"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise-5",
    "href": "slides/05-cv-tidymodels.html#application-exercise-5",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nCopy the code below, fill in the blanks to fit a model on the training data then calculate the test RMSE.\n\n\nset.seed(100)\nAuto_split  &lt;- ________\nAuto_train  &lt;- ________\nAuto_test   &lt;- ________\nlm_fit      &lt;- fit(lm_spec, \n                   mpg ~ horsepower, \n                   data = ________)\nmpg_pred  &lt;- ________ |&gt; \n  predict(new_data = ________) |&gt; \n  bind_cols(________)\nrmse(________, truth = ________, estimate = ________)\n\n\n\n\n‚àí+\n06:00"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#a-faster-way",
    "href": "slides/05-cv-tidymodels.html#a-faster-way",
    "title": "Chapter 5 and tidymodels",
    "section": "A faster way!",
    "text": "A faster way!\n\nYou can use last_fit() and specify the split\nThis will automatically train the data on the train data from the split\nInstead of specifying which metric to calculate (with rmse as before) you can just use collect_metrics() and it will automatically calculate the metrics on the test data from the split"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#a-faster-way-1",
    "href": "slides/05-cv-tidymodels.html#a-faster-way-1",
    "title": "Chapter 5 and tidymodels",
    "section": "A faster way!",
    "text": "A faster way!\n\nset.seed(100)\n\nAuto_split &lt;- initial_split(Auto, prop = 0.5)\nlm_fit &lt;- last_fit(lm_spec,\n                   mpg ~ horsepower,\n                   split = Auto_split) \n\nlm_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 √ó 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       4.96  Preprocessor1_Model1\n2 rsq     standard       0.613 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#a-faster-way-2",
    "href": "slides/05-cv-tidymodels.html#a-faster-way-2",
    "title": "Chapter 5 and tidymodels",
    "section": "A faster way!",
    "text": "A faster way!\n\nset.seed(100)\n\nAuto_split &lt;- initial_split(Auto, prop = 0.5)\nlm_fit &lt;- last_fit(lm_spec,\n                   mpg ~ horsepower,\n                   split = Auto_split) \n\nlm_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 √ó 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       4.96  Preprocessor1_Model1\n2 rsq     standard       0.613 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#what-about-cross-validation",
    "href": "slides/05-cv-tidymodels.html#what-about-cross-validation",
    "title": "Chapter 5 and tidymodels",
    "section": "What about cross validation?",
    "text": "What about cross validation?\n\nAuto_cv &lt;- vfold_cv(Auto, v = 5)\nAuto_cv\n\n#  5-fold cross-validation \n# A tibble: 5 √ó 2\n  splits           id   \n  &lt;list&gt;           &lt;chr&gt;\n1 &lt;split [313/79]&gt; Fold1\n2 &lt;split [313/79]&gt; Fold2\n3 &lt;split [314/78]&gt; Fold3\n4 &lt;split [314/78]&gt; Fold4\n5 &lt;split [314/78]&gt; Fold5"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#what-about-cross-validation-1",
    "href": "slides/05-cv-tidymodels.html#what-about-cross-validation-1",
    "title": "Chapter 5 and tidymodels",
    "section": "What about cross validation?",
    "text": "What about cross validation?\n\nInstead of fit we will use fit_resamples\n\n\n\nfit_resamples(lm_spec, \n              mpg ~ horsepower,\n              resamples = Auto_cv)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#what-about-cross-validation-2",
    "href": "slides/05-cv-tidymodels.html#what-about-cross-validation-2",
    "title": "Chapter 5 and tidymodels",
    "section": "What about cross validation?",
    "text": "What about cross validation?\n\nHow do we get the metrics out? With collect_metrics() again!\n\n\n\nresults &lt;- fit_resamples(lm_spec,\n                         mpg ~ horsepower,\n                         resamples = Auto_cv)\n\nresults |&gt;\n  collect_metrics()\n\n# A tibble: 2 √ó 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4.88      5  0.385  Preprocessor1_Model1\n2 rsq     standard   0.616     5  0.0220 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise-6",
    "href": "slides/05-cv-tidymodels.html#application-exercise-6",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\n\n\n‚àí+\n05:00\n\n\n\n\nEdit the code below to get the 5-fold cross validation error rate for the following model:\n\n\\(mpg = \\beta_0 + \\beta_1 horsepower + \\beta_2 horsepower^2+ \\epsilon\\)\n\nAuto_cv &lt;- vfold_cv(Auto, v = 5)\n\nresults &lt;- fit_resamples(lm_spec,\n                         ----,\n                         resamples = ---)\n\nresults |&gt;\n  collect_metrics()\n\n\nWhat do you think rsq is?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#answer-2",
    "href": "slides/05-cv-tidymodels.html#answer-2",
    "title": "Chapter 5 and tidymodels",
    "section": "Answer",
    "text": "Answer\n\nAuto_cv &lt;- vfold_cv(Auto, v = 5)\n\nresults &lt;- fit_resamples(lm_spec,\n                         mpg ~ horsepower + I(horsepower^2),\n                         resamples = Auto_cv)\n\nresults |&gt;\n  collect_metrics()\n\n# A tibble: 2 √ó 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4.38      5  0.110  Preprocessor1_Model1\n2 rsq     standard   0.688     5  0.0177 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise-7",
    "href": "slides/05-cv-tidymodels.html#application-exercise-7",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nFit 3 models on the data using 5 fold cross validation:\n\n\\(mpg = \\beta_0 + \\beta_1 horsepower + \\epsilon\\)\n\\(mpg = \\beta_0 + \\beta_1 horsepower + \\beta_2 horsepower^2+ \\epsilon\\)\n\\(mpg = \\beta_0 + \\beta_1 horsepower + \\beta_2 horsepower^2+ \\beta_3 horsepower^3 +\\epsilon\\)\n\nCollect the metrics from each model, saving the results as results_1, results_2, results_3\nWhich model is ‚Äúbest‚Äù?\n\n\n\n\n\n‚àí+\n08:00\n\n\n\n\n\n\n\nüîó https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "slides/03-linear-regression.html",
    "href": "slides/03-linear-regression.html",
    "title": "Chapter 3 - Linear Regresion",
    "section": "",
    "text": "Create a new quarto file for this homework in your exercises R project."
  },
  {
    "objectID": "slides/03-linear-regression.html#application-exercise",
    "href": "slides/03-linear-regression.html#application-exercise",
    "title": "Chapter 3 - Linear Regresion",
    "section": " Application Exercise",
    "text": "Application Exercise\n\nCreate a new quarto file for this homework in your exercises R project."
  },
  {
    "objectID": "slides/03-linear-regression.html#lets-look-at-an-example",
    "href": "slides/03-linear-regression.html#lets-look-at-an-example",
    "title": "Chapter 3 - Linear Regresion",
    "section": "Let‚Äôs look at an example",
    "text": "Let‚Äôs look at an example\nLet‚Äôs look at a sample of 116 sparrows from Kent Island. We are interested in the relationship between Weight and Wing Length\n\n\nthe standard error of \\(\\hat{\\beta_1}\\) ( \\(SE_{\\hat{\\beta}_1}\\) ) is how much we expect the sample slope to vary from one random sample to another."
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows",
    "href": "slides/03-linear-regression.html#sparrows",
    "title": "Chapter 3 - Linear Regresion",
    "section": "Sparrows",
    "text": "Sparrows\n\nHow can we quantify how much we‚Äôd expect the slope to differ from one random sample to another?\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Weight ~ WingLength, data = Sparrows) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1\n2 WingLength     0.467    0.0347     13.5  2.62e-25"
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-1",
    "href": "slides/03-linear-regression.html#sparrows-1",
    "title": "Chapter 3 - Linear Regresion",
    "section": "Sparrows",
    "text": "Sparrows\n\nHow do we interpret this?\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Weight ~ WingLength, data = Sparrows) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1\n2 WingLength     0.467    0.0347     13.5  2.62e-25\n\n\n\n‚Äúthe sample slope is more than 13 standard errors above a slope of zero‚Äù"
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-2",
    "href": "slides/03-linear-regression.html#sparrows-2",
    "title": "Chapter 3 - Linear Regresion",
    "section": "Sparrows",
    "text": "Sparrows\n\nHow do we know what values of this statistic are worth paying attention to?\n\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Weight ~ WingLength, data = Sparrows) |&gt;\n  tidy(conf.int = TRUE)\n\n# A tibble: 2 √ó 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1   -0.531     3.26 \n2 WingLength     0.467    0.0347     13.5  2.62e-25    0.399     0.536\n\n\n\nconfidence intervals\np-values"
  },
  {
    "objectID": "slides/03-linear-regression.html#application-exercise-1",
    "href": "slides/03-linear-regression.html#application-exercise-1",
    "title": "Chapter 3 - Linear Regresion",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nFit a linear model using the mtcars data frame predicting miles per gallon (mpg) from weight and horsepower (wt and hp).\nPull out the coefficients and confidence intervals using the tidy() function demonstrated. How do you interpret these?"
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-3",
    "href": "slides/03-linear-regression.html#sparrows-3",
    "title": "Chapter 3 - Linear Regresion",
    "section": "Sparrows",
    "text": "Sparrows\n\nHow are these statistics distributed under the null hypothesis?\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Weight ~ WingLength, data = Sparrows) |&gt;\n  tidy() \n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1\n2 WingLength     0.467    0.0347     13.5  2.62e-25"
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-4",
    "href": "slides/03-linear-regression.html#sparrows-4",
    "title": "Chapter 3 - Linear Regresion",
    "section": "Sparrows",
    "text": "Sparrows\n\n\nI‚Äôve generated some data under a null hypothesis where \\(n = 20\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-5",
    "href": "slides/03-linear-regression.html#sparrows-5",
    "title": "Chapter 3 - Linear Regresion",
    "section": "Sparrows",
    "text": "Sparrows\n\n\nthis is a t-distribution with n-p-1 degrees of freedom."
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-6",
    "href": "slides/03-linear-regression.html#sparrows-6",
    "title": "Chapter 3 - Linear Regresion",
    "section": "Sparrows",
    "text": "Sparrows\nThe distribution of test statistics we would expect given the null hypothesis is true, \\(\\beta_1 = 0\\), is t-distribution with n-2 degrees of freedom."
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-7",
    "href": "slides/03-linear-regression.html#sparrows-7",
    "title": "Chapter 3 - Linear Regresion",
    "section": "Sparrows",
    "text": "Sparrows"
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-8",
    "href": "slides/03-linear-regression.html#sparrows-8",
    "title": "Chapter 3 - Linear Regresion",
    "section": "Sparrows",
    "text": "Sparrows\n\nHow can we compare this line to the distribution under the null?\n\n\n\np-value"
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-9",
    "href": "slides/03-linear-regression.html#sparrows-9",
    "title": "Chapter 3 - Linear Regresion",
    "section": "Sparrows",
    "text": "Sparrows\n\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Weight ~ WingLength, data = Sparrows) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1\n2 WingLength     0.467    0.0347     13.5  2.62e-25"
  },
  {
    "objectID": "slides/03-linear-regression.html#return-to-generated-data-n-20",
    "href": "slides/03-linear-regression.html#return-to-generated-data-n-20",
    "title": "Chapter 3 - Linear Regresion",
    "section": "Return to generated data, n = 20",
    "text": "Return to generated data, n = 20\n\n\nLet‚Äôs say we get a statistic of 1.5 in a sample"
  },
  {
    "objectID": "slides/03-linear-regression.html#lets-do-it-in-r",
    "href": "slides/03-linear-regression.html#lets-do-it-in-r",
    "title": "Chapter 3 - Linear Regresion",
    "section": "Let‚Äôs do it in R!",
    "text": "Let‚Äôs do it in R!\nThe proportion of area less than 1.5\n\n\npt(1.5, df = 18)\n\n[1] 0.9245248"
  },
  {
    "objectID": "slides/03-linear-regression.html#lets-do-it-in-r-1",
    "href": "slides/03-linear-regression.html#lets-do-it-in-r-1",
    "title": "Chapter 3 - Linear Regresion",
    "section": "Let‚Äôs do it in R!",
    "text": "Let‚Äôs do it in R!\nThe proportion of area greater than 1.5\n\n\npt(1.5, df = 18, lower.tail = FALSE)\n\n[1] 0.07547523"
  },
  {
    "objectID": "slides/03-linear-regression.html#lets-do-it-in-r-2",
    "href": "slides/03-linear-regression.html#lets-do-it-in-r-2",
    "title": "Chapter 3 - Linear Regresion",
    "section": "Let‚Äôs do it in R!",
    "text": "Let‚Äôs do it in R!\nThe proportion of area greater than 1.5 or less than -1.5.\n\n\n\npt(1.5, df = 18, lower.tail = FALSE) * 2\n\n[1] 0.1509505"
  },
  {
    "objectID": "slides/03-linear-regression.html#hypothesis-test",
    "href": "slides/03-linear-regression.html#hypothesis-test",
    "title": "Chapter 3 - Linear Regresion",
    "section": "Hypothesis test",
    "text": "Hypothesis test\n\nnull hypothesis \\(H_0: \\beta_1 = 0\\)\nalternative hypothesis \\(H_A: \\beta_1 \\ne 0\\)\np-value: 0.15\nOften, we have an \\(\\alpha\\)-level cutoff to compare this to, for example 0.05. Since this is greater than 0.05, we fail to reject the null hypothesis"
  },
  {
    "objectID": "slides/03-linear-regression.html#application-exercise-2",
    "href": "slides/03-linear-regression.html#application-exercise-2",
    "title": "Chapter 3 - Linear Regresion",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nUsing the linear model you fit previously (mpg from wt and hp) - calculate the p-value for the coefficient for weight\nInterpret this value. What is the null hypothesis? What is the alternative hypothesis? Do you reject the null?"
  },
  {
    "objectID": "slides/03-linear-regression.html#lets-do-it-in-r-3",
    "href": "slides/03-linear-regression.html#lets-do-it-in-r-3",
    "title": "Chapter 3 - Linear Regresion",
    "section": "Let‚Äôs do it in R!",
    "text": "Let‚Äôs do it in R!\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Weight ~ WingLength, data = Sparrows) |&gt;\n  tidy(conf.int = TRUE)\n\n# A tibble: 2 √ó 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1   -0.531     3.26 \n2 WingLength     0.467    0.0347     13.5  2.62e-25    0.399     0.536\n\n\n\n\\(t^* = t_{n-p-1} = t_{114} = 1.98\\)\n\\(LB = 0.47 - 1.98\\times 0.0347 = 0.399\\)\n\\(UB = 0.47+1.98 \\times 0.0347 = 0.536\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#linear-regression-questions",
    "href": "slides/03-linear-regression.html#linear-regression-questions",
    "title": "Chapter 3 - Linear Regresion",
    "section": "Linear Regression Questions",
    "text": "Linear Regression Questions\n\n‚úîÔ∏è Is there a relationship between a response variable and predictors?\n‚úîÔ∏è How strong is the relationship?\n‚úîÔ∏è What is the uncertainty?\nHow accurately can we predict a future outcome?"
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-10",
    "href": "slides/03-linear-regression.html#sparrows-10",
    "title": "Chapter 3 - Linear Regresion",
    "section": "Sparrows",
    "text": "Sparrows\n\nUsing the information here, how could I predict a new sparrow‚Äôs weight if I knew the wing length was 30?\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Weight ~ WingLength, data = Sparrows) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1\n2 WingLength     0.467    0.0347     13.5  2.62e-25\n\n\n\n\\(1.37 + 0.467 \\times 30 = 15.38\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#linear-regression-accuracy",
    "href": "slides/03-linear-regression.html#linear-regression-accuracy",
    "title": "Chapter 3 - Linear Regresion",
    "section": "Linear Regression Accuracy",
    "text": "Linear Regression Accuracy\n\nWhat is the residual sum of squares again?\n\n\nNote: In previous classes, this may have been referred to as SSE (sum of squares error), the book uses RSS, so we will stick with that!\n\n\n\\[RSS = \\sum(y_i - \\hat{y}_i)^2\\]"
  },
  {
    "objectID": "slides/03-linear-regression.html#linear-regression-accuracy-1",
    "href": "slides/03-linear-regression.html#linear-regression-accuracy-1",
    "title": "Chapter 3 - Linear Regresion",
    "section": "Linear Regression Accuracy",
    "text": "Linear Regression Accuracy\n\n\nThe total sum of squares represents the variability of the outcome, it is equivalent to the variability described by the model plus the remaining residual sum of squares\n\n\\[TSS = \\sum(y_i - \\bar{y})^2\\]"
  },
  {
    "objectID": "slides/03-linear-regression.html#linear-regression-accuracy-2",
    "href": "slides/03-linear-regression.html#linear-regression-accuracy-2",
    "title": "Chapter 3 - Linear Regresion",
    "section": "Linear Regression Accuracy",
    "text": "Linear Regression Accuracy\n\nThere are many ways ‚Äúmodel fit‚Äù can be assessed. Two common ones are:\n\nResidual Standard Error (RSE)\n\\(R^2\\) - the fraction of the variance explained\n\n\\(RSE = \\sqrt{\\frac{1}{n-p-1}RSS}\\)\n\\(R^2 = 1 - \\frac{RSS}{TSS}\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#lets-do-it-in-r-4",
    "href": "slides/03-linear-regression.html#lets-do-it-in-r-4",
    "title": "Chapter 3 - Linear Regresion",
    "section": "Let‚Äôs do it in R!",
    "text": "Let‚Äôs do it in R!\n\nlm_fit &lt;- linear_reg() |&gt; \n  set_engine(\"lm\") |&gt;\n  fit(Weight ~ WingLength, data = Sparrows)\n\nlm_fit |&gt;\n  predict(new_data = Sparrows) |&gt;\n  bind_cols(Sparrows) |&gt;\n  rsq(truth = Weight, estimate = .pred) \n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.614\n\n\n\n\nIs this testing \\(R^2\\) or training \\(R^2\\)?"
  },
  {
    "objectID": "slides/03-linear-regression.html#application-exercise-3",
    "href": "slides/03-linear-regression.html#application-exercise-3",
    "title": "Chapter 3 - Linear Regresion",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nFit a linear model using the mtcars data frame predicting miles per gallon (mpg) from weight and horsepower (wt and hp), using polynomials with 4 degrees of freedom for both.\nEstimate the training \\(R^2\\) using the rsq function.\nInterpret this values."
  },
  {
    "objectID": "slides/03-linear-regression.html#application-exercise-4",
    "href": "slides/03-linear-regression.html#application-exercise-4",
    "title": "Chapter 3 - Linear Regresion",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nCreate a cross validation object to do 5 fold cross validation using the mtcars data\nRefit the model on this object (using fit_resamples)\nUse collect_metrics to estimate the test \\(R^2\\) - how does this compare to the training \\(R^2\\) calculated in the previous exercise?"
  },
  {
    "objectID": "slides/03-linear-regression.html#additional-linear-regression-topics",
    "href": "slides/03-linear-regression.html#additional-linear-regression-topics",
    "title": "Chapter 3 - Linear Regresion",
    "section": "Additional Linear Regression Topics",
    "text": "Additional Linear Regression Topics\n\nPolynomial terms\nInteractions\nOutliers\nNon-constant variance of error terms\nHigh leverage points\nCollinearity\n\nRefer to Chapter 3 for more details on these topics if you need a refresher.\n\n\n\n\nüîó https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "labs/03-logistic.html",
    "href": "labs/03-logistic.html",
    "title": "Lab 03 - Logistic Regression",
    "section": "",
    "text": "Due: Tuesday 2023-02-21"
  },
  {
    "objectID": "labs/03-logistic.html#conceptual-questions",
    "href": "labs/03-logistic.html#conceptual-questions",
    "title": "Lab 03 - Logistic Regression",
    "section": "Conceptual questions",
    "text": "Conceptual questions\n\nUsing algebra, rearrange equation (1) below to get equation (2). Show all steps. Be sure to use Latex, remember you can insert a Latex equation by wrapping it in $$.\n\nEquation (1)\n\\[p(X)=\\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}\\]\nEquation (2)\n\\[\\textrm{log}\\left(\\frac{p(X)}{1-p(X)}\\right)=\\beta_0+\\beta_1X\\]\n\nSuppose that an individual has a 23% chance of defaulting on their credit card payment. What are the odds that they will default?"
  },
  {
    "objectID": "labs/03-logistic.html#logistic-regression",
    "href": "labs/03-logistic.html#logistic-regression",
    "title": "Lab 03 - Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nFor this lab we are using the Smarket data. Examine this data set - how many observations are there? How many columns? What are the variables?\nLet‚Äôs look at the correlation between all of the variables. To do this, if you haven‚Äôt done so already, we need to install the GGally package. Run the following code in your Console one time.\n\n\ninstall.packages(\"GGally\")\n\nThen add the code below to your .qmd file. What can you learn from this visualization? Which pair of variables have the highest correlation?\n\nggpairs(Smarket, \n        lower = list(combo = wrap(ggally_facethist, binwidth = 0.5)), \n        progress = FALSE)\n\n\nInference Fit a logistic regression model to predict Direction using Lag1, Lag2, Lag3, Lag4, Lag5, and Volume. Show a table that contains the coefficients and p-values along with the confidence intervals for each of the 6 predictors. Which predictor has the smallest p-value? Interpret the coefficient, confidence interval, and p-value for this predictor.\nInference Exponentiate the results from Exercise 5. Interpret the odds ratio for the same predictor you selected in Exercise 5.\nPrediction Using 5-fold cross validation, fit the same logistic regression model as Exercise 5. What is the test Accuracy for this model? Interpret this result.\nInference Fit a logistic regression model to predict Direction using only Lag1 and Lag2. Show a table that contains the coefficients and p-values along with the confidence intervals for each of the 2 predictors. Which predictor has the smallest p-value? Interpret the coefficient, confidence interval, and p-value for this predictor.\nInference Exponentiate the results from Exercise 8. Interpret the odds ratio for the same predictor you selected in Exercise 8.\nPrediction Using 5-fold cross validation, fit the same logistic regression model as Exercise 8. What is the test Accuracy for this model? Interpret this result.\nIf you had to choose between the model fit in Exercise 5 and the one fit in Exercise 8, which would you choose? Why?"
  },
  {
    "objectID": "hw/hw-01-intro-review.html",
    "href": "hw/hw-01-intro-review.html",
    "title": "Homework 1",
    "section": "",
    "text": "R is the name of the programming language itself and RStudio is a convenient interface.\nThe main goal of this homework is to re-introduce you to R and RStudio, which we will be using throughout the course both to learn the statistical concepts discussed in the course and to analyze real data and come to informed conclusions.\nAs the homework‚Äôs progress, you are encouraged to explore beyond what the homework dictates; a willingness to experiment will make you a much better programmer. Before we get to that stage, however, you need to build some basic fluency in R. Today we begin with the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands."
  },
  {
    "objectID": "hw/hw-01-intro-review.html#this-one-time",
    "href": "hw/hw-01-intro-review.html#this-one-time",
    "title": "Homework 1",
    "section": "This One Time",
    "text": "This One Time\n\nGo to our RStudio Server at http://turing.cornellcollege.edu:8787/\nClick File tab on the bottom right and then click the work Home.\nCreate a new folder using the little folder icon with the green plus on it. Use STA 362 in the folder name."
  },
  {
    "objectID": "hw/hw-01-intro-review.html#every-homeworklabactivity",
    "href": "hw/hw-01-intro-review.html#every-homeworklabactivity",
    "title": "Homework 1",
    "section": "Every Homework/lab/activity",
    "text": "Every Homework/lab/activity\nEach of your assignments will begin with the following steps.\n\nFinding the instructions on our website: https://sta362-sb8-24.github.io/STA362StatLearning/\nGoing to our RStudio Server at http://turing.cornellcollege.edu:8787/\nCreating a new project. and giving it a sensible name such as homework_1 and having that project in the course folder you created.\nCreate a new quarto document and give it a sensible name such as hw1.\nIn the YAML add the following (add what you don‚Äôt have). The embed-resources component will make your final rendered html self-contained.\n\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\n    embed-resources: true\n---"
  },
  {
    "objectID": "hw/hw-01-intro-review.html#yaml",
    "href": "hw/hw-01-intro-review.html#yaml",
    "title": "Homework 1",
    "section": "YAML",
    "text": "YAML\nIn your Quarto (qmd) file in your project, change the author name to your name, and render the document. Make sure that you also have added the extra YAML clode above."
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html",
    "href": "slides/01-02-welcome_to_sl.html",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "tgeorge@cornellcollege.edu\n ¬† MWTh 3:05pm-4:05pm and by appt.\n\n\n\n\nhttps://sta362-sb8-24.github.io/STA362StatLearning/\n\n\n\n\nName\nMajor\nFun OR boring fact\n\n\n\n\n\n\n\nIdentify risk factors for breast cancer\n\n\n\n\n\n\nDr.¬†Tyler George adapted from slides by Hastie & Tibshirani\n\n\n\n\n\n\n\nCustomize an email spam detection system\n\n\n\nData: 4601 labeled emails sent to George who works at HP Labs\nInput features: frequencies of words and punctuation\n\n\n\n\n\n\n‚Äî\ngeorge\nyou\nhp\nfree\n!\nedu\nremove\n\n\n\n\nspam\n0.00\n2.26\n0.02\n0.52\n0.51\n0.01\n0.28\n\n\nemail\n2.27\n1.27\n0.90\n0.07\n0.11\n0.29\n0.01\n\n\n\n\nDr.¬†Tyler George adapted from slides by Hastie & Tibshirani\n\n\n\n\n\n\n\nIdentify numbers in handwritten zip code\n\n\n\n\n\n\n\nDr.¬†Tyler George adapted from slides by Hastie & Tibshirani\n\n\n\n\n\n\nEstablish the relationship between variables in population survey data\n\nIncome survey data for males from the central Atlantic region of US, 2009\n\n\n\n\nDr.¬†Tyler George adapted from slides by Hastie & Tibshirani"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section",
    "href": "slides/01-02-welcome_to_sl.html#section",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "tgeorge@cornellcollege.edu\n ¬† MWTh 3:05pm-4:05pm and by appt."
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#course-website",
    "href": "slides/01-02-welcome_to_sl.html#course-website",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#intros",
    "href": "slides/01-02-welcome_to_sl.html#intros",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Name\nMajor\nFun OR boring fact"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#statistical-learning-problems",
    "href": "slides/01-02-welcome_to_sl.html#statistical-learning-problems",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Identify risk factors for breast cancer\n\n\n\n\n\n\nDr.¬†Tyler George adapted from slides by Hastie & Tibshirani"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#statistical-learning-problems-1",
    "href": "slides/01-02-welcome_to_sl.html#statistical-learning-problems-1",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Customize an email spam detection system\n\n\n\nData: 4601 labeled emails sent to George who works at HP Labs\nInput features: frequencies of words and punctuation\n\n\n\n\n\n\n‚Äî\ngeorge\nyou\nhp\nfree\n!\nedu\nremove\n\n\n\n\nspam\n0.00\n2.26\n0.02\n0.52\n0.51\n0.01\n0.28\n\n\nemail\n2.27\n1.27\n0.90\n0.07\n0.11\n0.29\n0.01\n\n\n\n\nDr.¬†Tyler George adapted from slides by Hastie & Tibshirani"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#statistical-learning-problems-2",
    "href": "slides/01-02-welcome_to_sl.html#statistical-learning-problems-2",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Identify numbers in handwritten zip code\n\n\n\n\n\n\n\nDr.¬†Tyler George adapted from slides by Hastie & Tibshirani"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#statistical-learning-problems-3",
    "href": "slides/01-02-welcome_to_sl.html#statistical-learning-problems-3",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Establish the relationship between variables in population survey data\n\nIncome survey data for males from the central Atlantic region of US, 2009\n\n\n\n\nDr.¬†Tyler George adapted from slides by Hastie & Tibshirani"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#types-of-statistical-learning",
    "href": "slides/01-02-welcome_to_sl.html#types-of-statistical-learning",
    "title": "Chapter 1 and 2",
    "section": "‚úåÔ∏è types of statistical learning",
    "text": "‚úåÔ∏è types of statistical learning\n\nSupervised Learning\nUnsupervised Learning"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#supervised-learning",
    "href": "slides/01-02-welcome_to_sl.html#supervised-learning",
    "title": "Chapter 1 and 2",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nDr.¬†Tyler George adapted from slides by Hastie & Tibshirani\n\n\n\noutcome variable: \\(Y\\), (dependent variable, response, target)\npredictors: vector of \\(p\\) predictors, \\(X\\), (inputs, regressors, covariates, features, independent variables)\nIn the regression problem, \\(Y\\) is quantitative (e.g price, blood pressure)\nIn the classification problem, \\(Y\\) takes values in a finite, unordered set (survived/died, digit 0-9, cancer class of tissue sample)\nWe have training data \\((x_1, y_1), \\dots, (x_N, y_N)\\). These are observations (examples, instances) of these measurements"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#supervised-learning-1",
    "href": "slides/01-02-welcome_to_sl.html#supervised-learning-1",
    "title": "Chapter 1 and 2",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nWhat do you think are some objectives here?\n\n\nObjectives\n\nAccurately predict unseen test cases\nUnderstand which inputs affect the outcome, and how\nAssess the quality of our predictions and inferences\n\n\nDr.¬†Tyler George adapted from slides by Hastie & Tibshirani"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#unsupervised-learning",
    "href": "slides/01-02-welcome_to_sl.html#unsupervised-learning",
    "title": "Chapter 1 and 2",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nDr.¬†Tyler George adapted from slides by Hastie & Tibshirani\n\n\n\nNo outcome variable, just a set of predictors (features) measured on a set of samples\nobjective is more fuzzy ‚Äì find groups of samples that behave similarly, find features that behave similarly, find linear combinations of features with the most variation\ndifficult to know how well your are doing\ndifferent from supervised learning, but can be useful as a pre-processing step for supervised learning"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#lets-take-a-tour---class-website",
    "href": "slides/01-02-welcome_to_sl.html#lets-take-a-tour---class-website",
    "title": "Chapter 1 and 2",
    "section": "Let‚Äôs take a tour - class website",
    "text": "Let‚Äôs take a tour - class website\n\n\n\n\n\nConcepts introduced:\n\nHow to find slides\nHow to find assignments\nHow to find RStudio\nHow to get help\nHow to find policies"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-and-classification",
    "href": "slides/01-02-welcome_to_sl.html#regression-and-classification",
    "title": "Chapter 1 and 2",
    "section": "Regression and Classification",
    "text": "Regression and Classification\n\nRegression: quantitative response\nClassification: qualitative (categorical) response"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-and-classification-1",
    "href": "slides/01-02-welcome_to_sl.html#regression-and-classification-1",
    "title": "Chapter 1 and 2",
    "section": "Regression and Classification",
    "text": "Regression and Classification\n\nWhat would be an example of a regression problem?\n\n\nRegression: quantitative response\nClassification: qualitative (categorical) response"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-and-classification-2",
    "href": "slides/01-02-welcome_to_sl.html#regression-and-classification-2",
    "title": "Chapter 1 and 2",
    "section": "Regression and Classification",
    "text": "Regression and Classification\n\nWhat would be an example of a classification problem?\n\n\nRegression: quantitative response\nClassification: qualitative (categorical) response"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#auto-data",
    "href": "slides/01-02-welcome_to_sl.html#auto-data",
    "title": "Chapter 1 and 2",
    "section": "Auto data",
    "text": "Auto data\n\n\n\n\n\n\n\n\n\n\n\nAbove are mpg vs horsepower, weight, and acceleration, with a blue linear-regression line fit separately to each. Can we predict mpg using these three?\n. . .\nMaybe we can do better using a model:\n\\[\\texttt{mpg} \\approx f(\\texttt{horsepower}, \\texttt{weight}, \\texttt{acceleration})\\]"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#notation",
    "href": "slides/01-02-welcome_to_sl.html#notation",
    "title": "Chapter 1 and 2",
    "section": "Notation",
    "text": "Notation\n\n\nmpg is the response variable, the outcome variable, we refer to this as \\(Y\\)\nhorsepower is a feature, input, predictor, we refer to this as \\(X_1\\)\nweight is \\(X_2\\)\nacceleration is \\(X_3\\)\nOur input vector is:\n\n\\(X = \\begin{bmatrix} X_1 \\\\X_2 \\\\X_3\\end{bmatrix}\\)\n\nOur model is\n\n\\(Y = f(X) + \\varepsilon\\)\n\n\\(\\varepsilon\\) is our error"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#why-do-we-care-about-fx",
    "href": "slides/01-02-welcome_to_sl.html#why-do-we-care-about-fx",
    "title": "Chapter 1 and 2",
    "section": "Why do we care about \\(f(X)\\)?",
    "text": "Why do we care about \\(f(X)\\)?\n\n\nWe can use \\(f(X)\\) to make predictions of \\(Y\\) for new values of \\(X = x\\)\nWe can gain a better understanding of which components of \\(X = (X_1, X_2, \\dots, X_p)\\) are important for explaining \\(Y\\)\nDepending on how complex \\(f\\) is, maybe we can understand how each component ( \\(X_j\\) ) of \\(X\\) affects \\(Y\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx",
    "href": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx",
    "title": "Chapter 1 and 2",
    "section": "How do we choose \\(f(X)\\)?",
    "text": "How do we choose \\(f(X)\\)?\n\n\n\n\n\n\n\n\n\nWhat is a good value for \\(f(X)\\) at any selected value of \\(X\\), say \\(X = 100\\)? There can be many \\(Y\\) values at \\(X = 100\\)."
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx-1",
    "href": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx-1",
    "title": "Chapter 1 and 2",
    "section": "How do we choose \\(f(X)\\)?",
    "text": "How do we choose \\(f(X)\\)?\n\n\n\n\n\n\n\n\n\nWhat is a good value for \\(f(X)\\) at any selected value of \\(X\\), say \\(X = 100\\)? There can be many \\(Y\\) values at \\(X = 100\\)."
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx-2",
    "href": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx-2",
    "title": "Chapter 1 and 2",
    "section": "How do we choose \\(f(X)\\)?",
    "text": "How do we choose \\(f(X)\\)?\n\n\n\n\n\n\n\n\n\nWhat is a good value for \\(f(X)\\) at any selected value of \\(X\\), say \\(X = 100\\)? There can be many \\(Y\\) values at \\(X = 100\\).\n\n\nThere are 17 points here, what value should I choose for f(100). What do you think the blue dot represents?"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx-3",
    "href": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx-3",
    "title": "Chapter 1 and 2",
    "section": "How do we choose \\(f(X)\\)?",
    "text": "How do we choose \\(f(X)\\)?\n\n\n\n\n\n\n\n\n\nA good value is\n\\[f(100) = E(Y|X = 100)\\]\n. . .\n\\(E(Y|X = 100)\\) means expected value (average) of \\(Y\\) given \\(X = 100\\)\n. . .\nThis ideal \\(f(x) = E(Y | X = x)\\) is called the regression function"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-function-fx",
    "href": "slides/01-02-welcome_to_sl.html#regression-function-fx",
    "title": "Chapter 1 and 2",
    "section": "Regression function, \\(f(X)\\)",
    "text": "Regression function, \\(f(X)\\)\n\nAlso works or a vector, \\(X\\), for example,\n\n\\[f(x) = f(x_1, x_2, x_3) = E[Y | X_1 = x_1, X_2 = x_2, X_3 = x_3]\\]\n\nThis is the optimal predictor of \\(Y\\) in terms of mean-squared prediction error"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-function-fx-1",
    "href": "slides/01-02-welcome_to_sl.html#regression-function-fx-1",
    "title": "Chapter 1 and 2",
    "section": "Regression function, \\(f(X)\\)",
    "text": "Regression function, \\(f(X)\\)\n\n\\(f(x) = E(Y|X = x)\\) is the function that minimizes \\(E[(Y - g(X))^2 |X = x]\\) over all functions \\(g\\) at all points \\(X = x\\)\n\n\n\n\\(\\varepsilon = Y - f(x)\\) is the irreducible error\neven if we knew \\(f(x)\\), we would still make errors in prediction, since at each \\(X = x\\) there is typically a distribution of possible \\(Y\\) values"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-function-fx-2",
    "href": "slides/01-02-welcome_to_sl.html#regression-function-fx-2",
    "title": "Chapter 1 and 2",
    "section": "Regression function, \\(f(X)\\)",
    "text": "Regression function, \\(f(X)\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-function-fx-3",
    "href": "slides/01-02-welcome_to_sl.html#regression-function-fx-3",
    "title": "Chapter 1 and 2",
    "section": "Regression function, \\(f(X)\\)",
    "text": "Regression function, \\(f(X)\\)\n\n\n\n\n\n\n\n\n\n\nUsing these points, how would I calculate the regression function?\n\n. . .\n\nTake the average! \\(f(100) = E[\\texttt{mpg}|\\texttt{horsepower} = 100] = 19.6\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-function-fx-4",
    "href": "slides/01-02-welcome_to_sl.html#regression-function-fx-4",
    "title": "Chapter 1 and 2",
    "section": "Regression function, \\(f(X)\\)",
    "text": "Regression function, \\(f(X)\\)\n\n\n\n\n\n\n\n\n\n\nThis point has a \\(Y\\) value of 32.9. What is \\(\\hat\\varepsilon\\)?\n\n\n\n\\(\\hat\\varepsilon = Y - \\hat{f}(X) = 32.9 - 19.6 = \\color{red}{13.3}\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#the-error",
    "href": "slides/01-02-welcome_to_sl.html#the-error",
    "title": "Chapter 1 and 2",
    "section": "The error",
    "text": "The error\nFor any estimate, \\(\\hat{f}(x)\\), of \\(f(x)\\), we have\n\\[E[(Y - \\hat{f}(x))^2 | X = x] = \\underbrace{[f(x) - \\hat{f}(x)]^2}_{\\textrm{reducible error}} + \\underbrace{Var(\\varepsilon)}_{\\textrm{irreducible error}}\\]\n\n\nAssume for a moment that both \\(\\hat{f}\\) and X are fixed.\n\\(E(Y ‚àí \\hat{Y})^2\\) represents the average, or expected value, of the squared difference between the predicted and actual value of Y, and Var( \\(\\varepsilon\\) ) represents the variance associated with the error term\nThe focus of this class is on techniques for estimating f with the aim of minimizing the reducible error.\nthe irreducible error will always provide an upper bound on the accuracy of our prediction for Y\nThis bound is almost always unknown in practice"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#estimating-f",
    "href": "slides/01-02-welcome_to_sl.html#estimating-f",
    "title": "Chapter 1 and 2",
    "section": "Estimating \\(f\\)",
    "text": "Estimating \\(f\\)\n\nTypically we have very few (if any!) data points at \\(X=x\\) exactly, so we cannot compute \\(E[Y|X=x]\\)\nFor example, what if we were interested in estimating miles per gallon when horsepower was 104.\n\n\n\n\n\n\n\n\n\n\n\n\n. . .\nüí° We can relax the definition and let\n\\[\\hat{f}(x) = E[Y | X\\in \\mathcal{N}(x)]\\]\n\n\nWhere \\(\\mathcal{N}(x)\\) is some neighborhood of \\(x\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#notation-pause",
    "href": "slides/01-02-welcome_to_sl.html#notation-pause",
    "title": "Chapter 1 and 2",
    "section": "Notation pause!",
    "text": "Notation pause!\n\n\\[\\hat{f}(x) = \\underbrace{E}_{\\textrm{The expectation}}[\\underbrace{Y}_{\\textrm{of Y}} \\underbrace{|}_{\\textrm{given}} \\underbrace{X\\in \\mathcal{N}(x)}_{\\textrm{X is in the neighborhood of x}}]\\]\n. . .\n\nüö® If you need a notation pause at any point during this class, please let me know!"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#estimating-f-1",
    "href": "slides/01-02-welcome_to_sl.html#estimating-f-1",
    "title": "Chapter 1 and 2",
    "section": "Estimating \\(f\\)",
    "text": "Estimating \\(f\\)\nüí° We can relax the definition and let\n\\[\\hat{f}(x) = E[Y | X\\in \\mathcal{N}(x)]\\]\n\n\nNearest neighbor averaging does pretty well with small \\(p\\) ( \\(p\\leq 4\\) ) and large \\(n\\)\nNearest neighbor is not great when \\(p\\) is large because of the curse of dimensionality (because nearest neighbors tend to be far away in high dimensions)\n\n\n. . .\n\nWhat do I mean by \\(p\\)? What do I mean by \\(n\\)?"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#parametric-models",
    "href": "slides/01-02-welcome_to_sl.html#parametric-models",
    "title": "Chapter 1 and 2",
    "section": "Parametric models",
    "text": "Parametric models\nA common parametric model is a linear model\n\\[f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p\\]\n\n\nA linear model has \\(p + 1\\) parameters ( \\(\\beta_0,\\dots,\\beta_p\\) )\nWe estimate these parameters by fitting a model to training data\nAlthough this model is almost never correct it can often be a good interpretable approximation to the unknown true function, \\(f(X)\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-1",
    "href": "slides/01-02-welcome_to_sl.html#section-1",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "The  red  points are simulated values for income from the model:\n\n\\[\\texttt{income} = f(\\texttt{education, senority}) + \\varepsilon\\]\n\n\\(f\\) is the  blue  surface"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-2",
    "href": "slides/01-02-welcome_to_sl.html#section-2",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Linear regression model fit to the simulated data\n\\[\\hat{f}_L(\\texttt{education, senority}) = \\hat{\\beta}_0 + \\hat{\\beta}_1\\texttt{education}+\\hat{\\beta}_2\\texttt{senority}\\]"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-3",
    "href": "slides/01-02-welcome_to_sl.html#section-3",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "More flexible regression model \\(\\hat{f}_S(\\texttt{education, seniority})\\) fit to the simulated data.\nHere we use a technique called a thin-plate spline to fit a flexible surface"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-4",
    "href": "slides/01-02-welcome_to_sl.html#section-4",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "And even MORE flexible üò± model \\(\\hat{f}(\\texttt{education, seniority})\\).\n\nHere we‚Äôve basically drawn the surface to hit every point, minimizing the error, but completely overfitting"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#finding-balance",
    "href": "slides/01-02-welcome_to_sl.html#finding-balance",
    "title": "Chapter 1 and 2",
    "section": "ü§π Finding balance",
    "text": "ü§π Finding balance\n\n\nPrediction accuracy versus interpretability\nLinear models are easy to interpret, thin-plate splines are not\nGood fit versus overfit or underfit\nHow do we know when the fit is just right?\nParsimony versus black-box\nWe often prefer a simpler model involving fewer variables over a black-box predictor involving them all"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy",
    "href": "slides/01-02-welcome_to_sl.html#accuracy",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\nWe‚Äôve fit a model \\(\\hat{f}(x)\\) to some training data.\nWe can measure accuracy as the average squared prediction error over that train data\n\n\\[MSE_{\\texttt{train}} = \\textrm{Ave}_{train}[y_i-\\hat{f}(x_i)]^2\\]\n. . .\n\nWhat can go wrong here?\n\n\n\nThis may be biased towards overfit models"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy-1",
    "href": "slides/01-02-welcome_to_sl.html#accuracy-1",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\n\n\n\n\n\n\n\n\n\nI have some train data, plotted above. What \\(\\hat{f}(x)\\) would minimize the \\(MSE_{\\texttt{train}}\\)?\n\n\\[MSE_{\\texttt{train}} = \\textrm{Ave}_{train}[y_i-\\hat{f}(x_i)]^2\\]"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy-2",
    "href": "slides/01-02-welcome_to_sl.html#accuracy-2",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\n\n\n\n\n\n\n\n\n\nI have some train data, plotted above. What \\(\\hat{f}(x)\\) would minimize the \\(MSE_{\\texttt{train}}\\)?\n\n\\[MSE_{train} = \\textrm{Ave}_{i\\in\\texttt{train}}[y_i-\\hat{f}(x_i)]^2\\]"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy-3",
    "href": "slides/01-02-welcome_to_sl.html#accuracy-3",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\n\n\n\n\n\n\n\n\n\nWhat is wrong with this?\n\n. . .\nIt‚Äôs overfit!"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy-4",
    "href": "slides/01-02-welcome_to_sl.html#accuracy-4",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\n\n\n\n\n\n\n\n\nIf we get a new sample, that overfit model is probably going to be terrible!"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy-5",
    "href": "slides/01-02-welcome_to_sl.html#accuracy-5",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\nWe‚Äôve fit a model \\(\\hat{f}(x)\\) to some training data.\nInstead of measuring accuracy as the average squared prediction error over that train data, we can compute it using fresh test data.\n\n\\[MSE_{\\texttt{test}} = \\textrm{Ave}_{test}[y_i-\\hat{f}(x_i)]^2\\]"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-6",
    "href": "slides/01-02-welcome_to_sl.html#section-6",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Black curve is the ‚Äútruth‚Äù on the left.  Red  curve on right is \\(MSE_{\\texttt{test}}\\), grey curve is \\(MSE_{\\texttt{train}}\\). Orange, blue and green curves/squares correspond to fis of different flexibility."
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-7",
    "href": "slides/01-02-welcome_to_sl.html#section-7",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Here the truth is smoother, so the smoother fit and linear model do really well"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-8",
    "href": "slides/01-02-welcome_to_sl.html#section-8",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Here the truth is wiggly and the noise is low, so the more flexible fits do the best"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#bias-variance-trade-off",
    "href": "slides/01-02-welcome_to_sl.html#bias-variance-trade-off",
    "title": "Chapter 1 and 2",
    "section": "Bias-variance trade-off",
    "text": "Bias-variance trade-off\n\n\nWe‚Äôve fit a model, \\(\\hat{f}(x)\\), to some training data\nLet‚Äôs pull a test observation from this population ( \\(x_0, y_0\\) )\nThe true model is \\(Y = f(x) + \\varepsilon\\)\n\\(f(x) = E[Y|X=x]\\)\n\n\n. . .\n\\[E(y_0 - \\hat{f}(x_0))^2 = \\textrm{Var}(\\hat{f}(x_0)) + [\\textrm{Bias}(\\hat{f}(x_0))]^2 + \\textrm{Var}(\\varepsilon)\\]\n. . .\nThe expectation averages over the variability of \\(y_0\\) as well as the variability of the training data. \\(\\textrm{Bias}(\\hat{f}(x_0)) =E[\\hat{f}(x_0)]-f(x_0)\\)\n\n\nAs flexibility of \\(\\hat{f}\\) \\(\\uparrow\\), its variance \\(\\uparrow\\) and its bias \\(\\downarrow\\)\nchoosing the flexibility based on average test error amounts to a bias-variance trade-off\n\n\n\n\nThat U-shape we see for the test MSE curves is due to this bias-variance trade-off\nThe expected test MSE for a given \\(x_0\\) can be decomposed into three components: the variance of \\(\\hat{f}(x_o)\\), the squared bias of \\(\\hat{f}(x_o)\\) and t4he variance of the error term \\(\\varepsilon\\)\nHere the notation \\(E[y_0 ‚àí \\hat{f}(x_0)]^2\\) defines the expected test MSE, and refers to the average test MSE that we would obtain if we repeatedly estimated \\(f\\) using a large number of training sets, and tested each at \\(x_0\\)\nThe overall expected test MSE can be computed by averaging \\(E[y_0 ‚àí \\hat{f}(x_0)]^2\\) over all possible values of \\(x_0\\) in the test set.\nSO we want to minimize the expected test error, so to do that we need to pick a statistical learning method to simultenously acheive low bias and low variance.\nSince both of these quantities are non-negative, the expected test MSE can never fall below Var( \\(\\varepsilon\\) )"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#bias-variance-trade-off-1",
    "href": "slides/01-02-welcome_to_sl.html#bias-variance-trade-off-1",
    "title": "Chapter 1 and 2",
    "section": "Bias-variance trade-off",
    "text": "Bias-variance trade-off"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#conceptual-idea",
    "href": "slides/01-02-welcome_to_sl.html#conceptual-idea",
    "title": "Chapter 1 and 2",
    "section": "Conceptual Idea",
    "text": "Conceptual Idea\nWatch StatQuest video: Machine Learning Fundamentals: Bias and Variance"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#notation-1",
    "href": "slides/01-02-welcome_to_sl.html#notation-1",
    "title": "Chapter 1 and 2",
    "section": "Notation",
    "text": "Notation\n\n\n\\(Y\\) is the response variable. It is qualitative\n\\(\\mathcal{C}(X)\\) is the classifier that assigns a class \\(\\mathcal{C}\\) to some future unlabeled observation, \\(X\\)\nExamples:\nEmail can be classified as \\(\\mathcal{C}=(\\texttt{spam, not spam})\\)\nWritten number is one of \\(\\mathcal{C}=\\{0, 1, 2, \\dots, 9\\}\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#classification-problem",
    "href": "slides/01-02-welcome_to_sl.html#classification-problem",
    "title": "Chapter 1 and 2",
    "section": "Classification Problem",
    "text": "Classification Problem\n\nWhat is the goal?\n\n\n\nBuild a classifier \\(\\mathcal{C}(X)\\) that assigns a class label from \\(\\mathcal{C}\\) to a future unlabeled observation \\(X\\)\n\nAssess the uncertainty in each classification\n\nUnderstand the roles of the different predictors among \\(X = (X_1, X_2, \\dots, X_p)\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-9",
    "href": "slides/01-02-welcome_to_sl.html#section-9",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Suppose there are \\(K\\) elements in \\(\\mathcal{C}\\), numbered \\(1, 2, \\dots, K\\)\n\\[p_k(x) = P(Y = k|X=x), k = 1, 2, \\dots, K\\] These are conditional class probabilities at \\(x\\)\n. . .\n\nHow do you think we could calculate this?\n\n. . .\n\nIn the plot, you could examine the mini-barplot at \\(x = 5\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-10",
    "href": "slides/01-02-welcome_to_sl.html#section-10",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Suppose there are \\(K\\) elements in \\(\\mathcal{C}\\), numbered \\(1, 2, \\dots, K\\)\n\\[p_k(x) = P(Y = k|X=x), k = 1, 2, \\dots, K\\] These are conditional class probabilities at \\(x\\)\n\nThe Bayes optimal classifier at \\(x\\) is\n\n\\[\\mathcal{C}(x) = j \\textrm{ if } p_j(x) = \\textrm{max}\\{p_1(x), p_2(x), \\dots, p_K(x)\\}\\]\n\n\nNotice that probability is a conditional probability\nIt is the probability that Y equals k given the observed preditor vector, \\(x\\)\nLet‚Äôs say we were using a Bayes Classifier for a two class problem, Y is 1 or 2. We would predict that the class is one if \\(P(Y=1|X=x_0)&gt;0.5\\) and 2 otherwise"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-11",
    "href": "slides/01-02-welcome_to_sl.html#section-11",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "What if this was our data and there were no points at exactly \\(x = 5\\)? Then how could we calculate this?\n\n\n\nNearest neighbor like before!\nThis does break down as the dimensions grow, but the impact of \\(\\mathcal{\\hat{C}}(x)\\) is less than on \\(\\hat{p}_k(x), k = 1,2,\\dots,K\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy-6",
    "href": "slides/01-02-welcome_to_sl.html#accuracy-6",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\nMisclassification error rate\n\n\\[Err_{\\texttt{test}} = \\frac{\\#correct predictions}{total predictions} = \\textrm{Ave}_{test}I[y_i\\neq \\mathcal{\\hat{C}}(x_i)]\\] &gt; * \\(I(\\cdot)\\) is an indicator function and will only be eitehr 0 or 1.\n\n\nThe Bayes Classifier using the true \\(p_k(x)\\) has the smallest error\nSome of the methods we (may) learn build structured models for \\(\\mathcal{C}(x)\\) (support vector machines, for example)\nSome build structured models for \\(p_k(x)\\) (logistic regression, for example)\n\n\n\n\nthe test error rate \\(\\textrm{Ave}_{i\\in\\texttt{test}}I[y_i\\neq \\mathcal{\\hat{C}}(x_i)]\\) is minimized on average by very simple classifier that assigns each observation to the most likely class, given its predictor values (that‚Äôs the Bayes classifier)"
  },
  {
    "objectID": "slides/04-logistic.html",
    "href": "slides/04-logistic.html",
    "title": "Chapter 4 - Logistic Regression",
    "section": "",
    "text": "We had a linear regression refresher\nLinear regression is a great tool when we have a continuous outcome\nWe are going to learn some fancy ways to do even better in the future\n\nSetup:\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR)"
  },
  {
    "objectID": "labs/02-logistic.html",
    "href": "labs/02-logistic.html",
    "title": "Lab 02",
    "section": "",
    "text": "Go to our RStudio and create a new R project inside your class folder.\n\n\nCreate a .qmd file for your lab, make sure the author is your name, and Render the document."
  },
  {
    "objectID": "labs/02-logistic.html#yaml",
    "href": "labs/02-logistic.html#yaml",
    "title": "Lab 02",
    "section": "",
    "text": "Create a .qmd file for your lab, make sure the author is your name, and Render the document."
  },
  {
    "objectID": "labs/02-logistic.html#conceptual-questions",
    "href": "labs/02-logistic.html#conceptual-questions",
    "title": "Lab 02",
    "section": "Conceptual questions",
    "text": "Conceptual questions\n\nSuppose that an individual has a 23% chance of defaulting on their credit card payment. What are the odds that they will default?"
  },
  {
    "objectID": "labs/02-logistic.html#logistic-regression",
    "href": "labs/02-logistic.html#logistic-regression",
    "title": "Lab 02",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nFor this lab we are using the Smarket data. Examine this data set - how many observations are there? How many columns? What are the variables?\nLet‚Äôs look at the correlation between all of the variables. Add the code below to your .qmd file. What can you learn from this visualization? Which pair of variables have the highest correlation?\n\n\nggpairs(Smarket, \n        lower = list(combo = wrap(ggally_facethist, binwidth = 0.5)), \n        progress = FALSE)\n\n\nInference Fit a logistic regression model to predict Direction using Lag1, Lag2, Lag3, Lag4, Lag5, and Volume. Show a table that contains the coefficients and p-values along with the confidence intervals for each of the 6 predictors. Which predictor has the smallest p-value? Interpret the coefficient, confidence interval, and p-value for this predictor.\nInference Exponentiate the results from Exercise 4. Interpret the odds ratio for the same predictor you selected in Exercise 4.\nPrediction Using 5-fold cross validation, fit the same logistic regression model as Exercise 5. What is the test Accuracy for this model? Interpret this result.\nInference Fit a logistic regression model to predict Direction using only Lag1 and Lag2. Show a table that contains the coefficients and p-values along with the confidence intervals for each of the 2 predictors. Which predictor has the smallest p-value?\nInference Exponentiate the results from the previous Exercise. Interpret the odds ratio for the same predictor you selected in the previous exercise.\nPrediction Using 5-fold cross validation, fit the same logistic regression model as Exercise 7. What is the test Accuracy for this model? Interpret this result.\nIf you had to choose between the model fit in Exercise 4 and the one fit in Exercise 7, which would you choose? Why?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Statistical Learning Schedule",
    "section": "",
    "text": "Note: The timeline of topics and assignments might be updated throughout the semester.\n\n\n\n\n\n\n\n\n\nDay\nDate\nTopic\nNotes\nLab\nHomework\n\n\n\n\n1\n15 April\nCourse Intro, Ch 1 - Stat Learning Examples, Ch 2 - SL, Bias-Variance Tradeoff\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\n16 April\nCh 5 - Cross Validation, Tidy Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\n17 April\nCh 5 - Cross Validation, Tidy Models, Ch 3 - LR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4\n18 April\nCh 4 - Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5\n19 April\n\n\n\n\n\n\n\n\n\n\n\n\n6\n22 April\n\n\n\n\n\n\n\n\n\n\n\n\n7\n23 April\n\n\n\n\n\n\n\n\n\n\n\n\n8\n24 April\n\n\n\n\n\n\n\n\n\n\n\n\n9\n25 April\n\n\n\n\n\n\n\n\n\n\n\n\n10\n26 April\nExam\n\n\n\n\n\n\n\n\n\n\n\n11\n29 April\n\n\n\n\n\n\n\n\n\n\n\n\n12\n30 April\n\n\n\n\n\n\n\n\n\n\n\n\n13\n1 May\n\n\n\n\n\n\n\n\n\n\n\n\n14\n2 May\n\n\n\n\n\n\n\n\n\n\n\n\n15\n3 May\n\n\n\n\n\n\n\n\n\n\n\n\n16\n6 May\n\n\n\n\n\n\n\n\n\n\n\n\n17\n7 May\n\n\n\n\n\n\n\n\n\n\n\n\n18\n8 May\nExam",
    "crumbs": [
      "Course Contents",
      "Schedule & Assignments"
    ]
  },
  {
    "objectID": "slides/04-logistic_slides.html#recap",
    "href": "slides/04-logistic_slides.html#recap",
    "title": "Chapter 4 Part 1",
    "section": "Recap",
    "text": "Recap\n\nWe had a linear regression refresher\nLinear regression is a great tool when we have a continuous outcome\nWe are going to learn some fancy ways to do even better in the future\n\nSetup:\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR)"
  },
  {
    "objectID": "slides/04-logistic_slides.html#classification-1",
    "href": "slides/04-logistic_slides.html#classification-1",
    "title": "Chapter 4 Part 1",
    "section": "Classification",
    "text": "Classification\n\nWhat are some examples of classification problems?\n\n\nQualitative response variable in an unordered set, \\(\\mathcal{C}\\)\neye color \\(\\in\\) {blue, brown, green}\nemail \\(\\in\\) {spam, not spam}\nResponse, \\(Y\\) takes on values in \\(\\mathcal{C}\\)\nPredictors are a vector, \\(X\\)\nThe task: build a function \\(C(X)\\) that takes \\(X\\) and predicts \\(Y\\), \\(C(X)\\in\\mathcal{C}\\)\nMany times we are actually more interested in the probabilities that \\(X\\) belongs to each category in \\(\\mathcal{C}\\)"
  },
  {
    "objectID": "slides/04-logistic_slides.html#example-credit-card-default",
    "href": "slides/04-logistic_slides.html#example-credit-card-default",
    "title": "Chapter 4 Part 1",
    "section": "Example: Credit card default",
    "text": "Example: Credit card default"
  },
  {
    "objectID": "slides/04-logistic_slides.html#can-we-use-linear-regression",
    "href": "slides/04-logistic_slides.html#can-we-use-linear-regression",
    "title": "Chapter 4 Part 1",
    "section": "Can we use linear regression?",
    "text": "Can we use linear regression?\nWe can code Default as\n\\[Y = \\begin{cases} 0 & \\textrm{if }\\texttt{No}\\\\ 1&\\textrm{if }\\texttt{Yes}\\end{cases}\\] Can we fit a linear regression of \\(Y\\) on \\(X\\) and classify as Yes if \\(\\hat{Y}&gt; 0.5\\)?\n\nIn this case of a binary outcome, linear regression is okay (it is equivalent to linear discriminant analysis, you can read more about that in your book!)\n\\(E[Y|X=x] = P(Y=1|X=x)\\), so it seems like this is a pretty good idea!\nThe problem: Linear regression can produce probabilities less than 0 or greater than 1 üò±"
  },
  {
    "objectID": "slides/04-logistic_slides.html#can-we-use-linear-regression-1",
    "href": "slides/04-logistic_slides.html#can-we-use-linear-regression-1",
    "title": "Chapter 4 Part 1",
    "section": "Can we use linear regression?",
    "text": "Can we use linear regression?\nWe can code Default as\n\\[Y = \\begin{cases} 0 & \\textrm{if }\\texttt{No}\\\\ 1&\\textrm{if }\\texttt{Yes}\\end{cases}\\] Can we fit a linear regression of \\(Y\\) on \\(X\\) and classify as Yes if \\(\\hat{Y}&gt; 0.5\\)?\n\nWhat may do a better job?\n\n\nLogistic regression!"
  },
  {
    "objectID": "slides/04-logistic_slides.html#linear-versus-logistic-regression",
    "href": "slides/04-logistic_slides.html#linear-versus-logistic-regression",
    "title": "Chapter 4 Part 1",
    "section": "Linear versus logistic regression",
    "text": "Linear versus logistic regression\n\n\nWhich does a better job at predicting the probability of default?\n\n\nThe orange marks represent the response \\(Y\\in\\{0,1\\}\\)"
  },
  {
    "objectID": "slides/04-logistic_slides.html#linear-regression",
    "href": "slides/04-logistic_slides.html#linear-regression",
    "title": "Chapter 4 Part 1",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhat if we have \\(&gt;2\\) possible outcomes? For example, someone comes to the emergency room and we need to classify them according to their symptoms\n\\[\n\\begin{align}\nY = \\begin{cases} 1& \\textrm{if }\\texttt{stroke}\\\\2&\\textrm{if }\\texttt{drug overdose}\\\\3&\\textrm{if }\\texttt{epileptic seizure}\\end{cases}\n\\end{align}\n\\]\n\nWhat could go wrong here?\n\n\nThe coding implies an ordering\nThe coding implies equal spacing (that is the difference between stroke and drug overdose is the same as drug overdose and epileptic seizure)"
  },
  {
    "objectID": "slides/04-logistic_slides.html#linear-regression-1",
    "href": "slides/04-logistic_slides.html#linear-regression-1",
    "title": "Chapter 4 Part 1",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhat if we have \\(&gt;2\\) possible outcomes? For example, someone comes to the emergency room and we need to classify them according to their symptoms\n\\[\n\\begin{align}\nY = \\begin{cases} 1& \\textrm{if }\\texttt{stroke}\\\\2&\\textrm{if }\\texttt{drug overdose}\\\\3&\\textrm{if }\\texttt{epileptic seizure}\\end{cases}\n\\end{align}\n\\]\n\nLinear regression is not appropriate here\nMutliclass logistic regression or discriminant analysis are more appropriate"
  },
  {
    "objectID": "slides/04-logistic_slides.html#logistic-regression",
    "href": "slides/04-logistic_slides.html#logistic-regression",
    "title": "Chapter 4 Part 1",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\\[\np(X) = \\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}\n\\]\n\nNote: \\(p(X)\\) is shorthand for \\(P(Y=1|X)\\)\nNo matter what values \\(\\beta_0\\), \\(\\beta_1\\), or \\(X\\) take \\(p(X)\\) will always be between 0 and 1"
  },
  {
    "objectID": "slides/04-logistic_slides.html#logistic-regression-1",
    "href": "slides/04-logistic_slides.html#logistic-regression-1",
    "title": "Chapter 4 Part 1",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\\[\np(X) = \\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}\n\\]\nWe can rearrange this into the following form:\n\\[\n\\log\\left(\\frac{p(X)}{1-p(X)}\\right) = \\beta_0 + \\beta_1 X\n\\]\n\nWhat is this transformation called?\n\n\nThis is a log odds or logit transformation of \\(p(X)\\)"
  },
  {
    "objectID": "slides/04-logistic_slides.html#linear-versus-logistic-regression-1",
    "href": "slides/04-logistic_slides.html#linear-versus-logistic-regression-1",
    "title": "Chapter 4 Part 1",
    "section": "Linear versus logistic regression",
    "text": "Linear versus logistic regression\n\nLogistic regression ensures that our estimates for \\(p(X)\\) are between 0 and 1 üéâ"
  },
  {
    "objectID": "slides/04-logistic_slides.html#maximum-likelihood",
    "href": "slides/04-logistic_slides.html#maximum-likelihood",
    "title": "Chapter 4 Part 1",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\nRefresher: How did we estimate \\(\\hat\\beta\\) in linear regression?"
  },
  {
    "objectID": "slides/04-logistic_slides.html#maximum-likelihood-1",
    "href": "slides/04-logistic_slides.html#maximum-likelihood-1",
    "title": "Chapter 4 Part 1",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\nRefresher: How did we estimate \\(\\hat\\beta\\) in linear regression?\n\nIn logistic regression, we use maximum likelihood to estimate the parameters\n\\[\\mathcal{l}(\\beta_0,\\beta_1)=\\prod_{i:y_i=1}p(x_i)\\prod_{i:y_i=0}(1-p(x_i))\\]\n\nThis likelihood give the probability of the observed ones and zeros in the data\nWe pick \\(\\beta_0\\) and \\(\\beta_1\\) to maximize the likelihood\nWe‚Äôll let R do the heavy lifting here"
  },
  {
    "objectID": "slides/04-logistic_slides.html#lets-see-it-in-r",
    "href": "slides/04-logistic_slides.html#lets-see-it-in-r",
    "title": "Chapter 4 Part 1",
    "section": "Let‚Äôs see it in R",
    "text": "Let‚Äôs see it in R\n\n\n# A tibble: 2 √ó 5\n  term         estimate std.error statistic   p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) -10.7      0.361        -29.5 3.62e-191\n2 balance       0.00550  0.000220      25.0 1.98e-137\n\n\n\nUse the logistic_reg() function in R with the glm engine"
  },
  {
    "objectID": "slides/04-logistic_slides.html#making-predictions",
    "href": "slides/04-logistic_slides.html#making-predictions",
    "title": "Chapter 4 Part 1",
    "section": "Making predictions",
    "text": "Making predictions\n\nWhat is our estimated probability of default for someone with a balance of $1000?\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.6513306\n0.3611574\n-29.49221\n0\n\n\nbalance\n0.0054989\n0.0002204\n24.95309\n0\n\n\n\n\n\n\n\n\n\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}{1+e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}=\\frac{e^{-10.65+0.0055\\times 1000}}{1+e^{-10.65+0.0055\\times 1000}}=0.006\n\\]"
  },
  {
    "objectID": "slides/04-logistic_slides.html#making-predictions-1",
    "href": "slides/04-logistic_slides.html#making-predictions-1",
    "title": "Chapter 4 Part 1",
    "section": "Making predictions",
    "text": "Making predictions\n\nWhat is our estimated probability of default for someone with a balance of $2000?\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.6513306\n0.3611574\n-29.49221\n0\n\n\nbalance\n0.0054989\n0.0002204\n24.95309\n0\n\n\n\n\n\n\n\n\n\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}{1+e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}=\\frac{e^{-10.65+0.0055\\times 2000}}{1+e^{-10.65+0.0055\\times 2000}}=0.586\n\\]"
  },
  {
    "objectID": "slides/04-logistic_slides.html#logistic-regression-example",
    "href": "slides/04-logistic_slides.html#logistic-regression-example",
    "title": "Chapter 4 Part 1",
    "section": "Logistic regression example",
    "text": "Logistic regression example\nLet‚Äôs refit the model to predict the probability of default given the customer is a student\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-3.5041278\n0.0707130\n-49.554219\n0.0000000\n\n\nstudentYes\n0.4048871\n0.1150188\n3.520181\n0.0004313\n\n\n\n\n\n\n\n\n\\[P(\\texttt{default = Yes}|\\texttt{student = Yes}) = \\frac{e^{-3.5041+0.4049\\times1}}{1+e^{-3.5041+0.4049\\times1}}=0.0431\\]\n\n\nHow will this change if student = No?\n\n\n\n\\[P(\\texttt{default = Yes}|\\texttt{student = No}) = \\frac{e^{-3.5041+0.4049\\times0}}{1+e^{-3.5041+0.4049\\times0}}=0.0292\\]"
  },
  {
    "objectID": "slides/04-logistic_slides.html#multiple-logistic-regression",
    "href": "slides/04-logistic_slides.html#multiple-logistic-regression",
    "title": "Chapter 4 Part 1",
    "section": "Multiple logistic regression",
    "text": "Multiple logistic regression\n\\[\\log\\left(\\frac{p(X)}{1-p(X)}\\right)=\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p\\] \\[p(X) = \\frac{e^{\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p}}{1+e^{\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p}}\\]\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.8690452\n0.4922555\n-22.080088\n0.0000000\n\n\nbalance\n0.0057365\n0.0002319\n24.737563\n0.0000000\n\n\nincome\n0.0000030\n0.0000082\n0.369815\n0.7115203\n\n\nstudentYes\n-0.6467758\n0.2362525\n-2.737646\n0.0061881\n\n\n\n\n\n\n\n\n\nWhy is the coefficient for student negative now when it was positive before?"
  },
  {
    "objectID": "slides/04-logistic_slides.html#confounding",
    "href": "slides/04-logistic_slides.html#confounding",
    "title": "Chapter 4 Part 1",
    "section": "Confounding",
    "text": "Confounding\n\n\nWhat is going on here?"
  },
  {
    "objectID": "slides/04-logistic_slides.html#confounding-1",
    "href": "slides/04-logistic_slides.html#confounding-1",
    "title": "Chapter 4 Part 1",
    "section": "Confounding",
    "text": "Confounding\n\n\nStudents tend to have higher balances than non-students\nTheir marginal default rate is higher\nFor each level of balance, students default less\nTheir conditional default rate is lower"
  },
  {
    "objectID": "slides/04-logistic_slides.html#logistic-regression-for-more-than-two-classes",
    "href": "slides/04-logistic_slides.html#logistic-regression-for-more-than-two-classes",
    "title": "Chapter 4 Part 1",
    "section": "Logistic regression for more than two classes",
    "text": "Logistic regression for more than two classes\n\\[P(Y=k|X) = \\frac{e ^{\\beta_{0k}+\\beta_{1k}X_1+\\dots+\\beta_{pk}X_p}}{\\sum_{l=1}^Ke^{\\beta_{0l}+\\beta_{1l}X_1+\\dots+\\beta_{pl}X_p}}\\]\n\nSo far we‚Äôve discussed binary outcome data\nWe can generalize this to situations with multiple classes\nHere we have a linear function for each of the \\(K\\) classes\nThis is known as multinomial logistic regression"
  },
  {
    "objectID": "slides/04-logistic_slides.html#a-bit-about-odds",
    "href": "slides/04-logistic_slides.html#a-bit-about-odds",
    "title": "Chapter 4 Part 1",
    "section": "A bit about ‚Äúodds‚Äù",
    "text": "A bit about ‚Äúodds‚Äù\n\nThe ‚Äúodds‚Äù tell you how likely an event is\nüåÇ Let‚Äôs say there is a 60% chance of rain today * What is the probability that it will rain?\n\\(p = 0.6\\)\nWhat is the probability that it won‚Äôt rain?\n\\(1-p = 0.4\\)\nWhat are the odds that it will rain?\n3 to 2, 3:2, \\(\\frac{0.6}{0.4} = 1.5\\)"
  },
  {
    "objectID": "slides/04-logistic_slides.html#transforming-logs",
    "href": "slides/04-logistic_slides.html#transforming-logs",
    "title": "Chapter 4 Part 1",
    "section": "Transforming logs",
    "text": "Transforming logs\n\nHow do you ‚Äúundo‚Äù a \\(\\log\\) base \\(e\\)?\nUse \\(e\\)! For example:\n\\(e^{\\log(10)} = 10\\)\n\\(e^{\\log(1283)} = 1283\\)\n\\(e^{\\log(x)} = x\\)"
  },
  {
    "objectID": "slides/04-logistic_slides.html#transforming-logs-1",
    "href": "slides/04-logistic_slides.html#transforming-logs-1",
    "title": "Chapter 4 Part 1",
    "section": "Transforming logs",
    "text": "Transforming logs\n\nHow would you get the odds from the log(odds)?\n\n\n\nHow do you ‚Äúundo‚Äù a \\(\\log\\) base \\(e\\)?\nUse \\(e\\)! For example:\n\\(e^{\\log(10)} = 10\\)\n\\(e^{\\log(1283)} = 1283\\)\n\\(e^{\\log(x)} = x\\)\n\n\n\n\\(e^{\\log(odds)}\\) = odds"
  },
  {
    "objectID": "slides/04-logistic_slides.html#transforming-odds",
    "href": "slides/04-logistic_slides.html#transforming-odds",
    "title": "Chapter 4 Part 1",
    "section": "Transforming odds",
    "text": "Transforming odds\n\nodds = \\(\\frac{\\pi}{1-\\pi}\\)\nSolving for \\(\\pi\\)\n\\(\\pi = \\frac{\\textrm{odds}}{1+\\textrm{odds}}\\)\nPlugging in \\(e^{\\log(odds)}\\) = odds\n\\(\\pi = \\frac{e^{\\log(odds)}}{1+e^{\\log(odds)}}\\)\nPlugging in \\(\\log(odds) = \\beta_0 + \\beta_1x\\)\n\\(\\pi = \\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}\\)"
  },
  {
    "objectID": "slides/04-logistic_slides.html#the-logistic-model",
    "href": "slides/04-logistic_slides.html#the-logistic-model",
    "title": "Chapter 4 Part 1",
    "section": "The logistic model",
    "text": "The logistic model\n\n‚úåÔ∏è forms\n\n\n\n\n\n\n\n\nForm\nModel\n\n\n\n\nLogit form\n\\(\\log\\left(\\frac{\\pi}{1-\\pi}\\right) = \\beta_0 + \\beta_1x\\)\n\n\nProbability form\n\\(\\Large\\pi = \\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}\\)"
  },
  {
    "objectID": "slides/04-logistic_slides.html#the-logistic-model-1",
    "href": "slides/04-logistic_slides.html#the-logistic-model-1",
    "title": "Chapter 4 Part 1",
    "section": "The logistic model",
    "text": "The logistic model\n\n\n\nprobability\nodds\nlog(odds)\n\n\n\n\n\\(\\pi\\)\n\\(\\frac{\\pi}{1-\\pi}\\)\n\\(\\log\\left(\\frac{\\pi}{1-\\pi}\\right)=l\\)\n\n\n\n‚¨ÖÔ∏è\n\n\n\nlog(odds)\nodds\nprobability\n\n\n\n\n\\(l\\)\n\\(e^l\\)\n\\(\\frac{e^l}{1+e^l} = \\pi\\)"
  },
  {
    "objectID": "slides/04-logistic_slides.html#the-logistic-model-2",
    "href": "slides/04-logistic_slides.html#the-logistic-model-2",
    "title": "Chapter 4 Part 1",
    "section": "The logistic model",
    "text": "The logistic model\n\n‚úåÔ∏è forms\nlog(odds): \\(l = \\beta_0 + \\beta_1x\\)\nP(Outcome = Yes): \\(\\Large\\pi =\\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}\\)"
  },
  {
    "objectID": "slides/04-logistic_slides.html#odds-ratios",
    "href": "slides/04-logistic_slides.html#odds-ratios",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later. (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic_slides.html#odds-ratios-1",
    "href": "slides/04-logistic_slides.html#odds-ratios-1",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat is the explanatory variable?\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic_slides.html#odds-ratios-2",
    "href": "slides/04-logistic_slides.html#odds-ratios-2",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat type of variable is this?\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic_slides.html#odds-ratios-3",
    "href": "slides/04-logistic_slides.html#odds-ratios-3",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat is the outcome variable?\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic_slides.html#odds-ratios-4",
    "href": "slides/04-logistic_slides.html#odds-ratios-4",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat type of variable is this?\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic_slides.html#odds-ratios-5",
    "href": "slides/04-logistic_slides.html#odds-ratios-5",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\n\n\nTMS\nPlacebo\nTotal\n\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\nWe can compare the results using odds\nWhat are the odds of being pain-free for the placebo group?\n\\((22/100)/(78/100) = 22/78 = 0.282\\)\nWhat are the odds of being pain-free for the treatment group?\n\\(39/61 = 0.639\\)\nComparing the odds what can we conclude?\nTMS increases the likelihood of success"
  },
  {
    "objectID": "slides/04-logistic_slides.html#odds-ratios-6",
    "href": "slides/04-logistic_slides.html#odds-ratios-6",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\n\n\nTMS\nPlacebo\nTotal\n\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\nWe can summarize this relationship with an odds ratio: the ratio of the two odds\n\\(\\Large OR = \\frac{39/61}{22/78} = \\frac{0.639}{0.282} = 2.27\\)\n‚Äúthe odds of being pain free were 2.27 times higher with TMS than with the placebo‚Äù"
  },
  {
    "objectID": "slides/04-logistic_slides.html#odds-ratios-7",
    "href": "slides/04-logistic_slides.html#odds-ratios-7",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat if we wanted to calculate this in terms of Not pain-free (with pain-free) as the referent?\n\n\n\n\nTMS\nPlacebo\nTotal\n\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\n\\(\\Large OR = \\frac{61/39}{78/22} = \\frac{1.564}{3.545} = 0.441\\)\nthe odds for still being in pain for the TMS group are 0.441 times the odds of being in pain for the placebo group"
  },
  {
    "objectID": "slides/04-logistic_slides.html#odds-ratios-8",
    "href": "slides/04-logistic_slides.html#odds-ratios-8",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat changed here?\n\n\n\n\nTMS\nPlacebo\nTotal\n\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\n\\(\\Large OR = \\frac{78/22}{61/39} = \\frac{3.545}{1.564} = 2.27\\)\nthe odds for still being in pain for the placebo group are 2.27 times the odds of being in pain for the TMS group"
  },
  {
    "objectID": "slides/04-logistic_slides.html#odds-ratios-9",
    "href": "slides/04-logistic_slides.html#odds-ratios-9",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nIn general, it‚Äôs more natural to interpret odds ratios &gt; 1, you can flip the referent to do so\n\n\n\nTMS\nPlacebo\nTotal\n\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\\(\\Large OR = \\frac{78/22}{61/39} = \\frac{3.545}{1.564} = 2.27\\)\nthe odds for still being in pain for the placebo group are 2.27 times the odds of being in pain for the TMS group"
  },
  {
    "objectID": "slides/04-logistic_slides.html#odds-ratios-10",
    "href": "slides/04-logistic_slides.html#odds-ratios-10",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nLet‚Äôs look at some Titanic data. We are interested in whether the passenger reported being female is related to whether they survived.\n\n\n\nFemale\nMale\nTotal\n\n\n\n\n\nSurvived\n308\n142\n450\n\n\nDied\n154\n709\n863\n\n\nTotal\n462\n851\n1313"
  },
  {
    "objectID": "slides/04-logistic_slides.html#odds-ratios-11",
    "href": "slides/04-logistic_slides.html#odds-ratios-11",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat are the odds of surviving for females versus males?\n\n\n\n\nFemale\nMale\nTotal\n\n\n\n\n\nSurvived\n308\n142\n450\n\n\nDied\n154\n709\n863\n\n\nTotal\n462\n851\n1313\n\n\n\n\\[\\Large OR = \\frac{308/154}{142/709} = \\frac{2}{0.2} = 9.99\\]"
  },
  {
    "objectID": "slides/04-logistic_slides.html#odds-ratios-12",
    "href": "slides/04-logistic_slides.html#odds-ratios-12",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nHow do you interpret this?\n\n\n\n\nFemale\nMale\nTotal\n\n\n\n\n\nSurvived\n308\n142\n450\n\n\nDied\n154\n709\n863\n\n\nTotal\n462\n851\n1313\n\n\n\n\\[\\Large OR = \\frac{308/154}{142/709} = \\frac{2}{0.2} = 9.99\\] the odds of surviving for the female passengers was 9.99 times the odds of surviving for the male passengers"
  },
  {
    "objectID": "slides/04-logistic_slides.html#odds-ratios-13",
    "href": "slides/04-logistic_slides.html#odds-ratios-13",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat if we wanted to fit a model? What would the equation be?\n\n\n\n\nFemale\nMale\nTotal\n\n\n\n\n\nSurvived\n308\n142\n450\n\n\nDied\n154\n709\n863\n\n\nTotal\n462\n851\n1313\n\n\n\n\n\\[\\Large \\log(\\textrm{odds of survival}) = \\beta_0 + \\beta_1 \\textrm{Female}\\]"
  },
  {
    "objectID": "slides/04-logistic_slides.html#odds-ratios-14",
    "href": "slides/04-logistic_slides.html#odds-ratios-14",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\\[\\Large \\log(\\textrm{odds of survival}) = \\beta_0 + \\beta_1 \\textrm{Female}\\]\n\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -1.61    0.0919     -17.5 1.70e-68\n2 Sexfemale       2.30    0.135       17.1 2.91e-65"
  },
  {
    "objectID": "slides/04-logistic_slides.html#odds-ratios-15",
    "href": "slides/04-logistic_slides.html#odds-ratios-15",
    "title": "Chapter 4 Part 1",
    "section": "Odds Ratios",
    "text": "Odds Ratios\n\nHow do you interpret this result?\n\n\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -1.61    0.0919     -17.5 1.70e-68\n2 Sexfemale       2.30    0.135       17.1 2.91e-65"
  },
  {
    "objectID": "slides/04-logistic_slides.html#odds-ratios-16",
    "href": "slides/04-logistic_slides.html#odds-ratios-16",
    "title": "Chapter 4 Part 1",
    "section": "Odds Ratios",
    "text": "Odds Ratios\n\nHow do you interpret this result?\n\n\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.200    0.0919     -17.5 1.70e-68\n2 Sexfemale      9.99     0.135       17.1 2.91e-65\n\n\n[1] 9.99"
  },
  {
    "objectID": "slides/04-logistic_slides.html#odds-ratios-17",
    "href": "slides/04-logistic_slides.html#odds-ratios-17",
    "title": "Chapter 4 Part 1",
    "section": "Odds Ratios",
    "text": "Odds Ratios\n\nHow do you interpret this result?\n\n\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.200    0.0919     -17.5 1.70e-68\n2 Sexfemale      9.99     0.135       17.1 2.91e-65\n\n\n[1] 9.99\n\n\nthe odds of surviving for the female passengers was 9.99 times the odds of surviving for the male passengers"
  },
  {
    "objectID": "slides/04-logistic_slides.html#odds-ratios-18",
    "href": "slides/04-logistic_slides.html#odds-ratios-18",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nWhat if the explanatory variable is continuous?\n\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -19.2       5.63     -3.41 0.000644\n2 GPA             5.45      1.58      3.45 0.000553\n\n\nA one unit increase in GPA yields a 5.45 increase in the log odds of acceptance"
  },
  {
    "objectID": "slides/04-logistic_slides.html#odds-ratios-19",
    "href": "slides/04-logistic_slides.html#odds-ratios-19",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nWhat if the explanatory variable is continuous?\n\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  4.56e-9      5.63     -3.41 0.000644\n2 GPA          2.34e+2      1.58      3.45 0.000553\n\n\nA one unit increase in GPA yields a 234-fold increase in the odds of acceptance\n\nüò± that seems huge! Remember: the interpretation of these coefficients depends on your units (the same as in ordinary linear regression)."
  },
  {
    "objectID": "slides/04-logistic_slides.html#odds-ratios-20",
    "href": "slides/04-logistic_slides.html#odds-ratios-20",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nHow could we get the odds associated with increasing GPA by 0.1?\n\n\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -19.2       5.63     -3.41 0.000644\n2 GPA             5.45      1.58      3.45 0.000553\n\n\n\n\n[1] 234\n\n\n[1] 1.73\n\n\nA one-tenth unit increase in GPA yields a 1.73-fold increase in the odds of acceptance"
  },
  {
    "objectID": "slides/04-logistic_slides.html#odds-ratios-21",
    "href": "slides/04-logistic_slides.html#odds-ratios-21",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nHow could we get the odds associated with increasing GPA by 0.1?\n\n\n\n# A tibble: 2 √ó 5\n  term             estimate std.error statistic  p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) 0.00000000456     5.63      -3.41 0.000644\n2 GPA_10      1.73              0.158      3.45 0.000553\n\n\nA one-tenth unit increase in GPA yields a 1.73-fold increase in the odds of acceptance"
  },
  {
    "objectID": "slides/04-logistic_slides.html#application-exercise",
    "href": "slides/04-logistic_slides.html#application-exercise",
    "title": "Chapter 4 Part 1",
    "section": " Application Exercise",
    "text": "Application Exercise\nUsing the Default data from the ISLR package, fit a logistic regression model predicting whether a customer defaults with whether they are a student and their current balance.\nHere is some code to get you started:\n\n\n\n‚àí+\n05:00\n\n\n\n\n\n\n\nüîó https://sta362-sb8-24.github.io/STA362StatLearning/"
  }
]