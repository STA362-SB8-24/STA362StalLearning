[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Introduction to Statistical Learning",
    "section": "",
    "text": "Instructor\nInstructor Dr.¬†Tyler George\n ¬† Cornell College, West 311\n ¬†  tgeorge@cornellcollege.edu \n\n\n\nClass Meetings\nApril 15th - May 8th\n ¬† 9am-11am & 1pm-2pm\n ¬† West 201\n ¬† Course Calendar\n\n\n\nOffice Hours\n ¬† MWTh 3:05pm-4:05pm and by appt.\n ¬† West 311\n ¬† Optional Appointment\n\n\n\nI am available far beyond these times listed. Please email me and we can set up a time to chat about class material or whatever you prefer! I will generally announce changes to office hours in class but I still suggest checking the Course Calendar to verify availability.\n\nYou Are A Priority\nMy goal this block is to help you learn the material. I want to first and foremost recognize that you are an individual and thus are unique and may learn uniquely. Additionally, your health and wellbeing are priority one. Learning cannot happen effectively if you don‚Äôt meet your other personal needs. That all being said, I have structured the class in a way that I, from experience teaching and learning myself, think will be most beneficial for the majority of students. I promise you that I will do my best to create an inclusive and engaging learning environment. I ask that you keep an open line of communication between us for when you may need help and/or flexibility. You and your learning are why I am here.\n\n\nCourse Description\nThis course will introduce students to relatively new and powerful statistical techniques used to analyze data. The course will begin with a review of linear regression and an introduction to computer-based variable and model selection methods. Other topics will include classification methods, resampling methods for model-building, non-linear models, and tree-based methods. The computer software program R will be used throughout.\n\n\nLearning Objectives\nAt the end of this course I would like you conceptually understand, be able to use apply, and interpret the results of\n\nThe variance/bias trade-off\nMeasuring quality of fits\nLinear model selection and regularization\nClassification including K nearest neighbors\nCross validation\nDimension reduction\nTree based methods\nUnsupervised learning\n\n\n\nPrerequisite\nTo be successful in this class, you should have completed STA 201, STA 202, and DSC 223.\n\n\nOpen Access Books ‚Äì Free!\nAll of materials for this class are free.\nThe main text book is: An Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani ‚Äì it is freely available online.\nThe secondary text is R resource: Tidy Modeling with R by Max Kuhn and Julia Silge ‚Äì it is freely available online.\n\n\nCourse Site and Moodle\nOur course will run from a combination of Moodle and the course website at https://sta362-sb8-24.github.io/STA362StatLearning/.\n\n\nSoftware ‚Äì No need to install\nWe will use a combination of technologies in this course including R, and RStudio (server). Luckily for you I have put lots of effort into setting all of this on a machine we have on campus that we will all access with a web browser! You don‚Äôt need to install any ‚Äì in fact for a while I prefer you don‚Äôt. More on this in class. If you are an off campus student, please let me know right away, as you may need to checkout a laptop (free) from IT to work on homework from home.\nIf you have any technical problems you should contact IT as soon as possible. Submit a Work Order!\n\n\nGroup Work\nIn this class, I would like you to work in groups for a variety of reasons. A large part of this class is communicating analysis ‚Äì not just completing analysis. At the beginning of the block, groups will be formed. You should expect to work with this group every day. When we work in groups in class we will decide on roles, specifically who is controlling the one screen will rotate). Group members will rotate roles between tasks to help make sure everybody is sharing work. You won‚Äôt be working in a group for everything; any quizzes, and exams may be individual.\n\n\nEvaluations and grades\n\nGrade Category Descriptions\n\nHomework:\nHomeworks will be graded for correctness. The goal is the practice the application of the method and then be able to interpret the result.\n\n\nParticipation\nThis will be measured by attending class and working on the work given including labs and class examples.\n\n\nProject\nThis will entail multiple stages of submission with details accessed through ‚ÄúProject‚Äù on the left side of the course website (once available). Some class time will be given for discussing projects with me but not enough to complete the project during class times. I do not anticipate we will start these until week 3.\n\n\nExams\nThere will be a Midterm exam (4/26) and a final exam (morning of 5/8).\n\n\n\n\n\n\n\n\nAssignment\nPoints\n\n\n\n\nHomework\n200\n\n\nParticipation\n100\n\n\nProject\n300\n\n\nExams, two 200pts exams\n400\n\n\nTotal\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\nA\n93-100%\nC\n73-76%\n\n\nA-\n90‚Äì92%\nC-\n70-72%\n\n\nB+\n87‚Äì89%\nD+\n67-69%\n\n\nB\n83-86%\nD\n63-66%\n\n\nB-\n80-82%\nD-\n60-62%\n\n\nC+\n77-79%\nF\n&lt;60%\n\n\n\n\n\n\n\n\n\nUse of AI\nI expect you to generate your own work in this class. When you submit any kind of work (including projects, exams, homeworks), you are asserting that you have generated and written the text, and code, unless you indicate otherwise by the use of quotation marks and proper attribution for the source. Submitting content as your own that has been generated by someone other than you, or was created or assisted by a computer application or tool, including artificial intelligence (AI) tools such as ChatGPT is cheating and constitutes a violation of our Academic Honesty policy. You may use simple word processing tools to update spelling and grammar in your assignments, but unless given permission otherwise, you may not use AI tools to draft your work, even if you edit, revise, or paraphrase it. There may be opportunities for you to use AI tools in this class. Where they exist, I will clearly specify when and in what capacity it is permissible for you to use these tools.\n\n\nDISABILITIES AND ACCOMODATIONS POLICY\nCornell College makes reasonable accommodations for persons with disabilities. Students should notify the Office of Academic Support and Advising and their course instructor of any disability related accommodations within the first three days of the term for which the accommodations are required, due to the fast pace of the block format. For more information on the documentation required to establish the need for accommodations and the process of requesting the accommodations.\n\n\nACADEMIC HONESTY POLICY\nCornell College expects all members of the Cornell community to act with academic integrity. An important aspect of academic integrity is respecting the work of others. A student is expected to explicitly acknowledge ideas, claims, observations, or data of others, unless generally known. When a piece of work is submitted for credit, a student is asserting that the submission is her or his work unless there is a citation of a specific source. If there is no appropriate acknowledgment of sources, whether intended or not, this may constitute a violation of the College‚Äôs requirement for honesty in academic work and may be treated as a case of academic dishonesty. The procedures regarding how the College deals with cases of academic dishonesty appear in The Catalog, under the heading ‚ÄúAcademic Honesty.‚Äù\n\n\nIllness Policy\nIf you are experiencing COVID-19 symptoms, do not attend class. Perform a home test or contact Director of Student Health Services Lynn O‚ÄôBrien at student_health@cornellcollege.edu immediately to arrange a COVID-19 test at the Health Center. If you need to isolate due to COVID-19, or if you become unable to attend class for any other health reason, contact me as soon as possible to determine if you are able to continue in the class. A Withdrawal for Health Reasons may be required.\n\n\nMandatory Reporter Reminder\nIt is my goal that you feel supported and able to share information related to your life experiences during classroom discussions, in your written work, and in any one-on-one meetings with me. You should also know that all Cornell College faculty and staff are mandatory reporters. This means that I will keep information you share with me private to the greatest extent possible. However, I am required to share information regarding sexual assault, abuse, criminal behavior, or about a student who may be a danger to themselves or to others. If you wish to speak to someone confidentially who is not a mandatory reporter, you can schedule an appointment with one of the counselors in the Ebersole Health and Wellbeing Center or contact the College Chaplain, Rev.¬†Melea White, at mwhite@cornelllcollege.edu.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "slides/04-logistic.html#recap",
    "href": "slides/04-logistic.html#recap",
    "title": "Chapter 4 Part 1",
    "section": "Recap",
    "text": "Recap\n\nWe had a linear regression refresher\nLinear regression is a great tool when we have a continuous outcome\nWe are going to learn some fancy ways to do even better in the future\n\nSetup:\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR)"
  },
  {
    "objectID": "slides/04-logistic.html#classification-1",
    "href": "slides/04-logistic.html#classification-1",
    "title": "Chapter 4 Part 1",
    "section": "Classification",
    "text": "Classification\n\nWhat are some examples of classification problems?\n\n\nQualitative response variable in an unordered set, \\(\\mathcal{C}\\)\neye color \\(\\in\\) {blue, brown, green}\nemail \\(\\in\\) {spam, not spam}\nResponse, \\(Y\\) takes on values in \\(\\mathcal{C}\\)\nPredictors are a vector, \\(X\\)\nThe task: build a function \\(C(X)\\) that takes \\(X\\) and predicts \\(Y\\), \\(C(X)\\in\\mathcal{C}\\)\nMany times we are actually more interested in the probabilities that \\(X\\) belongs to each category in \\(\\mathcal{C}\\)"
  },
  {
    "objectID": "slides/04-logistic.html#example-credit-card-default",
    "href": "slides/04-logistic.html#example-credit-card-default",
    "title": "Chapter 4 Part 1",
    "section": "Example: Credit card default",
    "text": "Example: Credit card default\n\n\nCode\nset.seed(1)\nDefault |&gt;\n  sample_frac(size = 0.25) |&gt;\n  ggplot(aes(balance, income, color = default)) +\n  geom_point(pch = 4) +\n  scale_color_manual(values = c(\"cornflower blue\", \"red\")) +\n  theme_classic() +\n  theme(legend.position = \"top\") -&gt; p1\n\np2 &lt;- ggplot(Default, aes(x = default, y = balance, fill = default)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"cornflower blue\", \"red\")) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\np3 &lt;- ggplot(Default, aes(x = default, y = income, fill = default)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"cornflower blue\", \"red\")) +\n  theme_classic() +\n  theme(legend.position = \"none\")\ngrid.arrange(p1, p2, p3, ncol = 3, widths = c(2, 1, 1))"
  },
  {
    "objectID": "slides/04-logistic.html#can-we-use-linear-regression",
    "href": "slides/04-logistic.html#can-we-use-linear-regression",
    "title": "Chapter 4 Part 1",
    "section": "Can we use linear regression?",
    "text": "Can we use linear regression?\nWe can code Default as\n\\[Y = \\begin{cases} 0 & \\textrm{if }\\texttt{No}\\\\ 1&\\textrm{if }\\texttt{Yes}\\end{cases}\\] Can we fit a linear regression of \\(Y\\) on \\(X\\) and classify as Yes if \\(\\hat{Y}&gt; 0.5\\)?\n\nIn this case of a binary outcome, linear regression is okay (it is equivalent to linear discriminant analysis, you can read more about that in your book!)\n\\(E[Y|X=x] = P(Y=1|X=x)\\), so it seems like this is a pretty good idea!\nThe problem: Linear regression can produce probabilities less than 0 or greater than 1 üò±"
  },
  {
    "objectID": "slides/04-logistic.html#can-we-use-linear-regression-1",
    "href": "slides/04-logistic.html#can-we-use-linear-regression-1",
    "title": "Chapter 4 Part 1",
    "section": "Can we use linear regression?",
    "text": "Can we use linear regression?\nWe can code Default as\n\\[Y = \\begin{cases} 0 & \\textrm{if }\\texttt{No}\\\\ 1&\\textrm{if }\\texttt{Yes}\\end{cases}\\] Can we fit a linear regression of \\(Y\\) on \\(X\\) and classify as Yes if \\(\\hat{Y}&gt; 0.5\\)?\n\nWhat may do a better job?\n\n\nLogistic regression!"
  },
  {
    "objectID": "slides/04-logistic.html#linear-versus-logistic-regression",
    "href": "slides/04-logistic.html#linear-versus-logistic-regression",
    "title": "Chapter 4 Part 1",
    "section": "Linear versus logistic regression",
    "text": "Linear versus logistic regression\n\n\nCode\nDefault &lt;- Default |&gt;\n  mutate(\n  p = glm(default ~ balance, data = Default, family = \"binomial\") |&gt;\n  predict(type = \"response\"),\n  p2 = lm(I(default == \"Yes\") ~ balance, data = Default) |&gt; predict(),\n  def = ifelse(default == \"Yes\", 1, 0)\n)\n\n\nDefault |&gt;\n  sample_frac(0.25) |&gt;\nggplot(aes(balance, p2)) +\n  geom_hline(yintercept = c(0, 1), lty = 2, size = 0.2) +\n  geom_line(color = \"cornflower blue\") +\n  geom_point(aes(balance, def), shape = \"|\", color = \"orange\") +\n  theme_classic() +\n  labs(y = \"probability of default\") -&gt; p1\n\nDefault |&gt;\n  sample_frac(0.25) |&gt;\nggplot(aes(balance, p)) +\n  geom_hline(yintercept = c(0, 1), lty = 2, size = 0.2) +\n  geom_line(color = \"cornflower blue\") +\n  geom_point(aes(balance, def), shape = \"|\", color = \"orange\") +\n  theme_classic() +\n  labs(y = \"probability of default\") -&gt; p2\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\nWhich does a better job at predicting the probability of default?\n\n\nThe orange marks represent the response \\(Y\\in\\{0,1\\}\\)"
  },
  {
    "objectID": "slides/04-logistic.html#linear-regression",
    "href": "slides/04-logistic.html#linear-regression",
    "title": "Chapter 4 Part 1",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhat if we have \\(&gt;2\\) possible outcomes? For example, someone comes to the emergency room and we need to classify them according to their symptoms\n\\[\n\\begin{align}\nY = \\begin{cases} 1& \\textrm{if }\\texttt{stroke}\\\\2&\\textrm{if }\\texttt{drug overdose}\\\\3&\\textrm{if }\\texttt{epileptic seizure}\\end{cases}\n\\end{align}\n\\]\n\nWhat could go wrong here?\n\n\nThe coding implies an ordering\nThe coding implies equal spacing (that is the difference between stroke and drug overdose is the same as drug overdose and epileptic seizure)"
  },
  {
    "objectID": "slides/04-logistic.html#linear-regression-1",
    "href": "slides/04-logistic.html#linear-regression-1",
    "title": "Chapter 4 Part 1",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhat if we have \\(&gt;2\\) possible outcomes? For example, someone comes to the emergency room and we need to classify them according to their symptoms\n\\[\n\\begin{align}\nY = \\begin{cases} 1& \\textrm{if }\\texttt{stroke}\\\\2&\\textrm{if }\\texttt{drug overdose}\\\\3&\\textrm{if }\\texttt{epileptic seizure}\\end{cases}\n\\end{align}\n\\]\n\nLinear regression is not appropriate here\nMutliclass logistic regression or discriminant analysis are more appropriate"
  },
  {
    "objectID": "slides/04-logistic.html#logistic-regression",
    "href": "slides/04-logistic.html#logistic-regression",
    "title": "Chapter 4 Part 1",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\\[\np(X) = \\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}\n\\]\n\nNote: \\(p(X)\\) is shorthand for \\(P(Y=1|X)\\)\nNo matter what values \\(\\beta_0\\), \\(\\beta_1\\), or \\(X\\) take \\(p(X)\\) will always be between 0 and 1"
  },
  {
    "objectID": "slides/04-logistic.html#logistic-regression-1",
    "href": "slides/04-logistic.html#logistic-regression-1",
    "title": "Chapter 4 Part 1",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\\[\np(X) = \\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}\n\\]\nWe can rearrange this into the following form:\n\\[\n\\log\\left(\\frac{p(X)}{1-p(X)}\\right) = \\beta_0 + \\beta_1 X\n\\]\n\nWhat is this transformation called?\n\n\nThis is a log odds or logit transformation of \\(p(X)\\)"
  },
  {
    "objectID": "slides/04-logistic.html#linear-versus-logistic-regression-1",
    "href": "slides/04-logistic.html#linear-versus-logistic-regression-1",
    "title": "Chapter 4 Part 1",
    "section": "Linear versus logistic regression",
    "text": "Linear versus logistic regression\n\nLogistic regression ensures that our estimates for \\(p(X)\\) are between 0 and 1 üéâ"
  },
  {
    "objectID": "slides/04-logistic.html#maximum-likelihood",
    "href": "slides/04-logistic.html#maximum-likelihood",
    "title": "Chapter 4 Part 1",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\nRefresher: How did we estimate \\(\\hat\\beta\\) in linear regression?"
  },
  {
    "objectID": "slides/04-logistic.html#maximum-likelihood-1",
    "href": "slides/04-logistic.html#maximum-likelihood-1",
    "title": "Chapter 4 Part 1",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\nRefresher: How did we estimate \\(\\hat\\beta\\) in linear regression?\n\nIn logistic regression, we use maximum likelihood to estimate the parameters\n\\[\\mathcal{l}(\\beta_0,\\beta_1)=\\prod_{i:y_i=1}p(x_i)\\prod_{i:y_i=0}(1-p(x_i))\\]\n\nThis likelihood give the probability of the observed ones and zeros in the data\nWe pick \\(\\beta_0\\) and \\(\\beta_1\\) to maximize the likelihood\nWe‚Äôll let R do the heavy lifting here"
  },
  {
    "objectID": "slides/04-logistic.html#lets-see-it-in-r",
    "href": "slides/04-logistic.html#lets-see-it-in-r",
    "title": "Chapter 4 Part 1",
    "section": "Let‚Äôs see it in R",
    "text": "Let‚Äôs see it in R\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(default ~ balance, \n      data = Default) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term         estimate std.error statistic   p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) -10.7      0.361        -29.5 3.62e-191\n2 balance       0.00550  0.000220      25.0 1.98e-137\n\n\n\nUse the logistic_reg() function in R with the glm engine"
  },
  {
    "objectID": "slides/04-logistic.html#making-predictions",
    "href": "slides/04-logistic.html#making-predictions",
    "title": "Chapter 4 Part 1",
    "section": "Making predictions",
    "text": "Making predictions\n\nWhat is our estimated probability of default for someone with a balance of $1000?\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.6513306\n0.3611574\n-29.49221\n0\n\n\nbalance\n0.0054989\n0.0002204\n24.95309\n0\n\n\n\n\n\n\n\n\n\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}{1+e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}=\\frac{e^{-10.65+0.0055\\times 1000}}{1+e^{-10.65+0.0055\\times 1000}}=0.006\n\\]"
  },
  {
    "objectID": "slides/04-logistic.html#making-predictions-1",
    "href": "slides/04-logistic.html#making-predictions-1",
    "title": "Chapter 4 Part 1",
    "section": "Making predictions",
    "text": "Making predictions\n\nWhat is our estimated probability of default for someone with a balance of $2000?\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.6513306\n0.3611574\n-29.49221\n0\n\n\nbalance\n0.0054989\n0.0002204\n24.95309\n0\n\n\n\n\n\n\n\n\n\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}{1+e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}=\\frac{e^{-10.65+0.0055\\times 2000}}{1+e^{-10.65+0.0055\\times 2000}}=0.586\n\\]"
  },
  {
    "objectID": "slides/04-logistic.html#logistic-regression-example",
    "href": "slides/04-logistic.html#logistic-regression-example",
    "title": "Chapter 4 Part 1",
    "section": "Logistic regression example",
    "text": "Logistic regression example\nLet‚Äôs refit the model to predict the probability of default given the customer is a student\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-3.5041278\n0.0707130\n-49.554219\n0.0000000\n\n\nstudentYes\n0.4048871\n0.1150188\n3.520181\n0.0004313\n\n\n\n\n\n\n\n\n\\[P(\\texttt{default = Yes}|\\texttt{student = Yes}) = \\frac{e^{-3.5041+0.4049\\times1}}{1+e^{-3.5041+0.4049\\times1}}=0.0431\\]\n\n\nHow will this change if student = No?\n\n\n\n\\[P(\\texttt{default = Yes}|\\texttt{student = No}) = \\frac{e^{-3.5041+0.4049\\times0}}{1+e^{-3.5041+0.4049\\times0}}=0.0292\\]"
  },
  {
    "objectID": "slides/04-logistic.html#potential-confounding",
    "href": "slides/04-logistic.html#potential-confounding",
    "title": "Chapter 4 Part 1",
    "section": "Potential Confounding",
    "text": "Potential Confounding\n\n\nWhat is going on here?"
  },
  {
    "objectID": "slides/04-logistic.html#confounding",
    "href": "slides/04-logistic.html#confounding",
    "title": "Chapter 4 Part 1",
    "section": "Confounding",
    "text": "Confounding\n\n\nStudents tend to have higher balances than non-students\nTheir marginal default rate is higher\nFor each level of balance, students default less\nTheir conditional default rate is lower"
  },
  {
    "objectID": "slides/04-logistic.html#a-bit-about-odds",
    "href": "slides/04-logistic.html#a-bit-about-odds",
    "title": "Chapter 4 Part 1",
    "section": "A bit about ‚Äúodds‚Äù",
    "text": "A bit about ‚Äúodds‚Äù\n\nThe ‚Äúodds‚Äù tell you how likely an event is\nüåÇ Let‚Äôs say there is a 60% chance of rain today * What is the probability that it will rain?\n\\(p = 0.6\\)\nWhat is the probability that it won‚Äôt rain?\n\\(1-p = 0.4\\)\nWhat are the odds that it will rain?\n3 to 2, 3:2, \\(\\frac{0.6}{0.4} = 1.5\\)"
  },
  {
    "objectID": "slides/04-logistic.html#transforming-logs",
    "href": "slides/04-logistic.html#transforming-logs",
    "title": "Chapter 4 Part 1",
    "section": "Transforming logs",
    "text": "Transforming logs\n\nHow do you ‚Äúundo‚Äù a \\(\\log\\) base \\(e\\)?\nUse \\(e\\)! For example:\n\\(e^{\\log(10)} = 10\\)\n\\(e^{\\log(1283)} = 1283\\)\n\\(e^{\\log(x)} = x\\)"
  },
  {
    "objectID": "slides/04-logistic.html#transforming-logs-1",
    "href": "slides/04-logistic.html#transforming-logs-1",
    "title": "Chapter 4 Part 1",
    "section": "Transforming logs",
    "text": "Transforming logs\n\nHow would you get the odds from the log(odds)?\n\n\n\nHow do you ‚Äúundo‚Äù a \\(\\log\\) base \\(e\\)?\nUse \\(e\\)! For example:\n\\(e^{\\log(10)} = 10\\)\n\\(e^{\\log(1283)} = 1283\\)\n\\(e^{\\log(x)} = x\\)\n\n\n\n\\(e^{\\log(odds)}\\) = odds"
  },
  {
    "objectID": "slides/04-logistic.html#transforming-odds",
    "href": "slides/04-logistic.html#transforming-odds",
    "title": "Chapter 4 Part 1",
    "section": "Transforming odds",
    "text": "Transforming odds\n\nodds = \\(\\frac{\\pi}{1-\\pi}\\)\nSolving for \\(\\pi\\)\n\\(\\pi = \\frac{\\textrm{odds}}{1+\\textrm{odds}}\\)\nPlugging in \\(e^{\\log(odds)}\\) = odds\n\\(\\pi = \\frac{e^{\\log(odds)}}{1+e^{\\log(odds)}}\\)\nPlugging in \\(\\log(odds) = \\beta_0 + \\beta_1x\\)\n\\(\\pi = \\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}\\)"
  },
  {
    "objectID": "slides/04-logistic.html#the-logistic-model",
    "href": "slides/04-logistic.html#the-logistic-model",
    "title": "Chapter 4 Part 1",
    "section": "The logistic model",
    "text": "The logistic model\n\n‚úåÔ∏è forms\n\n\n\n\n\n\n\n\nForm\nModel\n\n\n\n\nLogit form\n\\(\\log\\left(\\frac{\\pi}{1-\\pi}\\right) = \\beta_0 + \\beta_1x\\)\n\n\nProbability form\n\\(\\Large\\pi = \\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}\\)"
  },
  {
    "objectID": "slides/04-logistic.html#the-logistic-model-1",
    "href": "slides/04-logistic.html#the-logistic-model-1",
    "title": "Chapter 4 Part 1",
    "section": "The logistic model",
    "text": "The logistic model\n\n\n\nprobability\nodds\nlog(odds)\n\n\n\n\n\\(\\pi\\)\n\\(\\frac{\\pi}{1-\\pi}\\)\n\\(\\log\\left(\\frac{\\pi}{1-\\pi}\\right)=l\\)\n\n\n\n‚¨ÖÔ∏è\n\n\n\nlog(odds)\nodds\nprobability\n\n\n\n\n\\(l\\)\n\\(e^l\\)\n\\(\\frac{e^l}{1+e^l} = \\pi\\)"
  },
  {
    "objectID": "slides/04-logistic.html#the-logistic-model-2",
    "href": "slides/04-logistic.html#the-logistic-model-2",
    "title": "Chapter 4 Part 1",
    "section": "The logistic model",
    "text": "The logistic model\n\n‚úåÔ∏è forms\nlog(odds): \\(l = \\beta_0 + \\beta_1x\\)\nP(Outcome = Yes): \\(\\Large\\pi =\\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}\\)"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios",
    "href": "slides/04-logistic.html#odds-ratios",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later. (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-1",
    "href": "slides/04-logistic.html#odds-ratios-1",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat is the explanatory variable?\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-2",
    "href": "slides/04-logistic.html#odds-ratios-2",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat type of variable is this?\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-3",
    "href": "slides/04-logistic.html#odds-ratios-3",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat is the outcome variable?\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-4",
    "href": "slides/04-logistic.html#odds-ratios-4",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat type of variable is this?\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-5",
    "href": "slides/04-logistic.html#odds-ratios-5",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\n\n\nTreatment\nTMS\nPlacebo\nTotal\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\nWe can compare the results using odds\nWhat are the odds of being pain-free for the placebo group?\n\\((22/100)/(78/100) = 22/78 = 0.282\\)\nWhat are the odds of being pain-free for the treatment group?\n\\(39/61 = 0.639\\)\nComparing the odds what can we conclude?\nTMS increases the likelihood of success"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-6",
    "href": "slides/04-logistic.html#odds-ratios-6",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\n\n\nTreatment\nTMS\nPlacebo\nTotal\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\nWe can summarize this relationship with an odds ratio: the ratio of the two odds\n\\(\\Large OR = \\frac{39/61}{22/78} = \\frac{0.639}{0.282} = 2.27\\)\n‚Äúthe odds of being pain free were 2.27 times higher with TMS than with the placebo‚Äù"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-7",
    "href": "slides/04-logistic.html#odds-ratios-7",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat if we wanted to calculate this in terms of Not pain-free (with pain-free) as the referent?\n\n\n\n\nTreatment\nTMS\nPlacebo\nTotal\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\n\\(\\Large OR = \\frac{61/39}{78/22} = \\frac{1.564}{3.545} = 0.441\\)\nthe odds for still being in pain for the TMS group are 0.441 times the odds of being in pain for the placebo group"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-8",
    "href": "slides/04-logistic.html#odds-ratios-8",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat changed here?\n\n\n\n\nTreatment\nTMS\nPlacebo\nTotal\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\n\\(\\Large OR = \\frac{78/22}{61/39} = \\frac{3.545}{1.564} = 2.27\\)\nthe odds for still being in pain for the placebo group are 2.27 times the odds of being in pain for the TMS group"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-9",
    "href": "slides/04-logistic.html#odds-ratios-9",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nIn general, it‚Äôs more natural to interpret odds ratios &gt; 1, you can flip the referent to do so\n\n\n\nTreatment\nTMS\nPlacebo\nTotal\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\\(\\Large OR = \\frac{78/22}{61/39} = \\frac{3.545}{1.564} = 2.27\\)\nthe odds for still being in pain for the placebo group are 2.27 times the odds of being in pain for the TMS group"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-10",
    "href": "slides/04-logistic.html#odds-ratios-10",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nLet‚Äôs look at some Titanic data. We are interested in whether the passenger reported being female is related to whether they survived.\n\n\n\n\nFemale\nMale\nTotal\n\n\n\n\nSurvived\n308\n142\n450\n\n\nDied\n154\n709\n863\n\n\nTotal\n462\n851\n1313"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-11",
    "href": "slides/04-logistic.html#odds-ratios-11",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat are the odds of surviving for females versus males?\n\n\n\n\n\nFemale\nMale\nTotal\n\n\n\n\nSurvived\n308\n142\n450\n\n\nDied\n154\n709\n863\n\n\nTotal\n462\n851\n1313\n\n\n\n\\[\\Large OR = \\frac{308/154}{142/709} = \\frac{2}{0.2} = 9.99\\]"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-12",
    "href": "slides/04-logistic.html#odds-ratios-12",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nHow do you interpret this?\n\n\n\n\n\nFemale\nMale\nTotal\n\n\n\n\nSurvived\n308\n142\n450\n\n\nDied\n154\n709\n863\n\n\nTotal\n462\n851\n1313\n\n\n\n\\[\\Large OR = \\frac{308/154}{142/709} = \\frac{2}{0.2} = 9.99\\] the odds of surviving for the female passengers was 9.99 times the odds of surviving for the male passengers"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-13",
    "href": "slides/04-logistic.html#odds-ratios-13",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat if we wanted to fit a model? What would the equation be?\n\n\n\n\n\nFemale\nMale\nTotal\n\n\n\n\nSurvived\n308\n142\n450\n\n\nDied\n154\n709\n863\n\n\nTotal\n462\n851\n1313\n\n\n\n\n\\[\\Large \\log(\\textrm{odds of survival}) = \\beta_0 + \\beta_1 \\textrm{Female}\\]"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-14",
    "href": "slides/04-logistic.html#odds-ratios-14",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\\[\\Large \\log(\\textrm{odds of survival}) = \\beta_0 + \\beta_1 \\textrm{Female}\\]\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Survived ~ Sex, data = Titanic) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -1.61    0.0919     -17.5 1.70e-68\n2 Sexfemale       2.30    0.135       17.1 2.91e-65"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-15",
    "href": "slides/04-logistic.html#odds-ratios-15",
    "title": "Chapter 4 Part 1",
    "section": "Odds Ratios",
    "text": "Odds Ratios\n\nHow do you interpret this result?\n\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Survived ~ Sex, data = Titanic) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -1.61    0.0919     -17.5 1.70e-68\n2 Sexfemale       2.30    0.135       17.1 2.91e-65"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-16",
    "href": "slides/04-logistic.html#odds-ratios-16",
    "title": "Chapter 4 Part 1",
    "section": "Odds Ratios",
    "text": "Odds Ratios\n\nHow do you interpret this result?\n\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Survived ~ Sex, data = Titanic) |&gt;\n  tidy(exponentiate = TRUE) \n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.200    0.0919     -17.5 1.70e-68\n2 Sexfemale      9.99     0.135       17.1 2.91e-65\n\nexp(2.301176)\n\n[1] 9.99"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-17",
    "href": "slides/04-logistic.html#odds-ratios-17",
    "title": "Chapter 4 Part 1",
    "section": "Odds Ratios",
    "text": "Odds Ratios\n\nHow do you interpret this result?\n\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Survived ~ Sex, data = Titanic) |&gt;\n  tidy(exponentiate = TRUE) \n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.200    0.0919     -17.5 1.70e-68\n2 Sexfemale      9.99     0.135       17.1 2.91e-65\n\nexp(2.301176)\n\n[1] 9.99\n\n\nthe odds of surviving for the female passengers was 9.99 times the odds of surviving for the male passengers"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-18",
    "href": "slides/04-logistic.html#odds-ratios-18",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nWhat if the explanatory variable is continuous?\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Acceptance ~ GPA, data = MedGPA) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -19.2       5.63     -3.41 0.000644\n2 GPA             5.45      1.58      3.45 0.000553\n\n\nA one unit increase in GPA yields a 5.45 increase in the log odds of acceptance"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-19",
    "href": "slides/04-logistic.html#odds-ratios-19",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nWhat if the explanatory variable is continuous?\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Acceptance ~ GPA, data = MedGPA) |&gt;\n  tidy(exponentiate = TRUE) \n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  4.56e-9      5.63     -3.41 0.000644\n2 GPA          2.34e+2      1.58      3.45 0.000553\n\n\nA one unit increase in GPA yields a 234-fold increase in the odds of acceptance\n\nüò± that seems huge! Remember: the interpretation of these coefficients depends on your units (the same as in ordinary linear regression)."
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-20",
    "href": "slides/04-logistic.html#odds-ratios-20",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nHow could we get the odds associated with increasing GPA by 0.1?\n\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Acceptance ~ GPA, data = MedGPA) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -19.2       5.63     -3.41 0.000644\n2 GPA             5.45      1.58      3.45 0.000553\n\n\n\nexp(5.454) ## a one unit increase in GPA\n\n[1] 234\n\nexp(5.454 * 0.1) ## a 0.1 increase in GPA\n\n[1] 1.73\n\n\nA one-tenth unit increase in GPA yields a 1.73-fold increase in the odds of acceptance"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-21",
    "href": "slides/04-logistic.html#odds-ratios-21",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nHow could we get the odds associated with increasing GPA by 0.1?\n\n\nMedGPA &lt;- MedGPA |&gt;\n  mutate(GPA_10 = GPA * 10)\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Acceptance ~ GPA_10, data = MedGPA) |&gt;\n  tidy(exponentiate = TRUE)\n\n# A tibble: 2 √ó 5\n  term             estimate std.error statistic  p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) 0.00000000456     5.63      -3.41 0.000644\n2 GPA_10      1.73              0.158      3.45 0.000553\n\n\nA one-tenth unit increase in GPA yields a 1.73-fold increase in the odds of acceptance"
  },
  {
    "objectID": "slides/04-logistic.html#application-exercise",
    "href": "slides/04-logistic.html#application-exercise",
    "title": "Chapter 4 Part 1",
    "section": " Application Exercise",
    "text": "Application Exercise\nUsing the Default data from the ISLR package. Fit two logistic regression models that predict whether a customer defaults\n\nOne model with student as a predictor. Interpret the coefficient of studentYes.\nAnother model with balance as a predictor. Interpret the coefficient of balance.\n\nHere is some code to get you started:\n\nlibrary(ISLR)\ndata(\"Default\")\n\n\n\n\n\nüîó https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "slides/03-linear-regression.html#application-exercise",
    "href": "slides/03-linear-regression.html#application-exercise",
    "title": "Chapter 3 - Linear Regression",
    "section": " Application Exercise",
    "text": "Application Exercise\n\nCreate a new quarto file for this homework in your exercises R project."
  },
  {
    "objectID": "slides/03-linear-regression.html#lets-look-at-an-example",
    "href": "slides/03-linear-regression.html#lets-look-at-an-example",
    "title": "Chapter 3 - Linear Regression",
    "section": "Let‚Äôs look at an example",
    "text": "Let‚Äôs look at an example\nLet‚Äôs look at a sample of 116 sparrows from Kent Island. We are interested in the relationship between Weight and Wing Length\n\n\nthe standard error of \\(\\hat{\\beta_1}\\) ( \\(SE_{\\hat{\\beta}_1}\\) ) is how much we expect the sample slope to vary from one random sample to another."
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows",
    "href": "slides/03-linear-regression.html#sparrows",
    "title": "Chapter 3 - Linear Regression",
    "section": "Sparrows",
    "text": "Sparrows\n\nHow can we quantify how much we‚Äôd expect the slope to differ from one random sample to another?\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Weight ~ WingLength, data = Sparrows) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1\n2 WingLength     0.467    0.0347     13.5  2.62e-25"
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-1",
    "href": "slides/03-linear-regression.html#sparrows-1",
    "title": "Chapter 3 - Linear Regression",
    "section": "Sparrows",
    "text": "Sparrows\n\nHow do we interpret this?\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Weight ~ WingLength, data = Sparrows) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1\n2 WingLength     0.467    0.0347     13.5  2.62e-25\n\n\n\n‚Äúthe sample slope is more than 13 standard errors above a slope of zero‚Äù"
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-2",
    "href": "slides/03-linear-regression.html#sparrows-2",
    "title": "Chapter 3 - Linear Regression",
    "section": "Sparrows",
    "text": "Sparrows\n\nHow do we know what values of this statistic are worth paying attention to?\n\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Weight ~ WingLength, data = Sparrows) |&gt;\n  tidy(conf.int = TRUE)\n\n# A tibble: 2 √ó 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1   -0.531     3.26 \n2 WingLength     0.467    0.0347     13.5  2.62e-25    0.399     0.536\n\n\n\nconfidence intervals\np-values"
  },
  {
    "objectID": "slides/03-linear-regression.html#application-exercise-1",
    "href": "slides/03-linear-regression.html#application-exercise-1",
    "title": "Chapter 3 - Linear Regression",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nFit a linear model using the mtcars data frame predicting miles per gallon (mpg) from weight and horsepower (wt and hp).\nPull out the coefficients and confidence intervals using the tidy() function demonstrated. How do you interpret these?"
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-3",
    "href": "slides/03-linear-regression.html#sparrows-3",
    "title": "Chapter 3 - Linear Regression",
    "section": "Sparrows",
    "text": "Sparrows\n\nHow are these statistics distributed under the null hypothesis?\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Weight ~ WingLength, data = Sparrows) |&gt;\n  tidy() \n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1\n2 WingLength     0.467    0.0347     13.5  2.62e-25"
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-4",
    "href": "slides/03-linear-regression.html#sparrows-4",
    "title": "Chapter 3 - Linear Regression",
    "section": "Sparrows",
    "text": "Sparrows\n\n\nI‚Äôve generated some data under a null hypothesis where \\(n = 20\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-5",
    "href": "slides/03-linear-regression.html#sparrows-5",
    "title": "Chapter 3 - Linear Regression",
    "section": "Sparrows",
    "text": "Sparrows\n\n\nthis is a t-distribution with n-p-1 degrees of freedom."
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-6",
    "href": "slides/03-linear-regression.html#sparrows-6",
    "title": "Chapter 3 - Linear Regression",
    "section": "Sparrows",
    "text": "Sparrows\nThe distribution of test statistics we would expect given the null hypothesis is true, \\(\\beta_1 = 0\\), is t-distribution with n-2 degrees of freedom."
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-7",
    "href": "slides/03-linear-regression.html#sparrows-7",
    "title": "Chapter 3 - Linear Regression",
    "section": "Sparrows",
    "text": "Sparrows"
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-8",
    "href": "slides/03-linear-regression.html#sparrows-8",
    "title": "Chapter 3 - Linear Regression",
    "section": "Sparrows",
    "text": "Sparrows\n\nHow can we compare this line to the distribution under the null?\n\n\n\np-value"
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-9",
    "href": "slides/03-linear-regression.html#sparrows-9",
    "title": "Chapter 3 - Linear Regression",
    "section": "Sparrows",
    "text": "Sparrows\n\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Weight ~ WingLength, data = Sparrows) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1\n2 WingLength     0.467    0.0347     13.5  2.62e-25"
  },
  {
    "objectID": "slides/03-linear-regression.html#return-to-generated-data-n-20",
    "href": "slides/03-linear-regression.html#return-to-generated-data-n-20",
    "title": "Chapter 3 - Linear Regression",
    "section": "Return to generated data, n = 20",
    "text": "Return to generated data, n = 20\n\n\nLet‚Äôs say we get a statistic of 1.5 in a sample"
  },
  {
    "objectID": "slides/03-linear-regression.html#lets-do-it-in-r",
    "href": "slides/03-linear-regression.html#lets-do-it-in-r",
    "title": "Chapter 3 - Linear Regression",
    "section": "Let‚Äôs do it in R!",
    "text": "Let‚Äôs do it in R!\nThe proportion of area less than 1.5\n\n\npt(1.5, df = 18)\n\n[1] 0.9245248"
  },
  {
    "objectID": "slides/03-linear-regression.html#lets-do-it-in-r-1",
    "href": "slides/03-linear-regression.html#lets-do-it-in-r-1",
    "title": "Chapter 3 - Linear Regression",
    "section": "Let‚Äôs do it in R!",
    "text": "Let‚Äôs do it in R!\nThe proportion of area greater than 1.5\n\n\npt(1.5, df = 18, lower.tail = FALSE)\n\n[1] 0.07547523"
  },
  {
    "objectID": "slides/03-linear-regression.html#lets-do-it-in-r-2",
    "href": "slides/03-linear-regression.html#lets-do-it-in-r-2",
    "title": "Chapter 3 - Linear Regression",
    "section": "Let‚Äôs do it in R!",
    "text": "Let‚Äôs do it in R!\nThe proportion of area greater than 1.5 or less than -1.5.\n\n\n\npt(1.5, df = 18, lower.tail = FALSE) * 2\n\n[1] 0.1509505"
  },
  {
    "objectID": "slides/03-linear-regression.html#hypothesis-test",
    "href": "slides/03-linear-regression.html#hypothesis-test",
    "title": "Chapter 3 - Linear Regression",
    "section": "Hypothesis test",
    "text": "Hypothesis test\n\nnull hypothesis \\(H_0: \\beta_1 = 0\\)\nalternative hypothesis \\(H_A: \\beta_1 \\ne 0\\)\np-value: 0.15\nOften, we have an \\(\\alpha\\)-level cutoff to compare this to, for example 0.05. Since this is greater than 0.05, we fail to reject the null hypothesis"
  },
  {
    "objectID": "slides/03-linear-regression.html#application-exercise-2",
    "href": "slides/03-linear-regression.html#application-exercise-2",
    "title": "Chapter 3 - Linear Regression",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nUsing the linear model you fit previously (mpg from wt and hp) - calculate the p-value for the coefficient for weight\nInterpret this value. What is the null hypothesis? What is the alternative hypothesis? Do you reject the null?"
  },
  {
    "objectID": "slides/03-linear-regression.html#lets-do-it-in-r-3",
    "href": "slides/03-linear-regression.html#lets-do-it-in-r-3",
    "title": "Chapter 3 - Linear Regression",
    "section": "Let‚Äôs do it in R!",
    "text": "Let‚Äôs do it in R!\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Weight ~ WingLength, data = Sparrows) |&gt;\n  tidy(conf.int = TRUE)\n\n# A tibble: 2 √ó 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1   -0.531     3.26 \n2 WingLength     0.467    0.0347     13.5  2.62e-25    0.399     0.536\n\n\n\n\\(t^* = t_{n-p-1} = t_{114} = 1.98\\)\n\\(LB = 0.47 - 1.98\\times 0.0347 = 0.399\\)\n\\(UB = 0.47+1.98 \\times 0.0347 = 0.536\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#linear-regression-questions",
    "href": "slides/03-linear-regression.html#linear-regression-questions",
    "title": "Chapter 3 - Linear Regression",
    "section": "Linear Regression Questions",
    "text": "Linear Regression Questions\n\n‚úîÔ∏è Is there a relationship between a response variable and predictors?\n‚úîÔ∏è How strong is the relationship?\n‚úîÔ∏è What is the uncertainty?\nHow accurately can we predict a future outcome?"
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-10",
    "href": "slides/03-linear-regression.html#sparrows-10",
    "title": "Chapter 3 - Linear Regression",
    "section": "Sparrows",
    "text": "Sparrows\n\nUsing the information here, how could I predict a new sparrow‚Äôs weight if I knew the wing length was 30?\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Weight ~ WingLength, data = Sparrows) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1\n2 WingLength     0.467    0.0347     13.5  2.62e-25\n\n\n\n\\(1.37 + 0.467 \\times 30 = 15.38\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#linear-regression-accuracy",
    "href": "slides/03-linear-regression.html#linear-regression-accuracy",
    "title": "Chapter 3 - Linear Regression",
    "section": "Linear Regression Accuracy",
    "text": "Linear Regression Accuracy\n\nWhat is the residual sum of squares again?\n\n\nNote: In previous classes, this may have been referred to as SSE (sum of squares error), the book uses RSS, so we will stick with that!\n\n\n\\[RSS = \\sum(y_i - \\hat{y}_i)^2\\]"
  },
  {
    "objectID": "slides/03-linear-regression.html#linear-regression-accuracy-1",
    "href": "slides/03-linear-regression.html#linear-regression-accuracy-1",
    "title": "Chapter 3 - Linear Regression",
    "section": "Linear Regression Accuracy",
    "text": "Linear Regression Accuracy\n\n\nThe total sum of squares represents the variability of the outcome, it is equivalent to the variability described by the model plus the remaining residual sum of squares\n\n\\[TSS = \\sum(y_i - \\bar{y})^2\\]"
  },
  {
    "objectID": "slides/03-linear-regression.html#linear-regression-accuracy-2",
    "href": "slides/03-linear-regression.html#linear-regression-accuracy-2",
    "title": "Chapter 3 - Linear Regression",
    "section": "Linear Regression Accuracy",
    "text": "Linear Regression Accuracy\n\nThere are many ways ‚Äúmodel fit‚Äù can be assessed. Two common ones are:\n\nResidual Standard Error (RSE)\n\\(R^2\\) - the fraction of the variance explained\n\n\\(RSE = \\sqrt{\\frac{1}{n-p-1}RSS}\\)\n\\(R^2 = 1 - \\frac{RSS}{TSS}\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#lets-do-it-in-r-4",
    "href": "slides/03-linear-regression.html#lets-do-it-in-r-4",
    "title": "Chapter 3 - Linear Regression",
    "section": "Let‚Äôs do it in R!",
    "text": "Let‚Äôs do it in R!\n\nlm_fit &lt;- linear_reg() |&gt; \n  set_engine(\"lm\") |&gt;\n  fit(Weight ~ WingLength, data = Sparrows)\n\nlm_fit |&gt;\n  predict(new_data = Sparrows) |&gt;\n  bind_cols(Sparrows) |&gt;\n  rsq(truth = Weight, estimate = .pred) \n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.614\n\n\n\n\nIs this testing \\(R^2\\) or training \\(R^2\\)?"
  },
  {
    "objectID": "slides/03-linear-regression.html#application-exercise-3",
    "href": "slides/03-linear-regression.html#application-exercise-3",
    "title": "Chapter 3 - Linear Regression",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nFit a linear model using the mtcars data frame predicting miles per gallon (mpg) from weight and horsepower (wt and hp), using polynomials with 4 degrees of freedom for both.\nEstimate the training \\(R^2\\) using the rsq function.\nInterpret this values."
  },
  {
    "objectID": "slides/03-linear-regression.html#application-exercise-4",
    "href": "slides/03-linear-regression.html#application-exercise-4",
    "title": "Chapter 3 - Linear Regression",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nCreate a cross validation object to do 5 fold cross validation using the mtcars data\nRefit the model on this object (using fit_resamples)\nUse collect_metrics to estimate the test \\(R^2\\) - how does this compare to the training \\(R^2\\) calculated in the previous exercise?"
  },
  {
    "objectID": "slides/03-linear-regression.html#additional-linear-regression-topics",
    "href": "slides/03-linear-regression.html#additional-linear-regression-topics",
    "title": "Chapter 3 - Linear Regression",
    "section": "Additional Linear Regression Topics",
    "text": "Additional Linear Regression Topics\n\nPolynomial terms\nInteractions\nOutliers\nNon-constant variance of error terms\nHigh leverage points\nCollinearity\n\nRefer to Chapter 3 for more details on these topics if you need a refresher.\n\n\n\n\nüîó https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "labs/03-lda_qda.html",
    "href": "labs/03-lda_qda.html",
    "title": "Lab 03",
    "section": "",
    "text": "Go to our RStudio and create a new R project inside your class folder.\n\n\nCreate a .qmd file for your lab, make sure the author is your name, and Render the document."
  },
  {
    "objectID": "labs/03-lda_qda.html#yaml",
    "href": "labs/03-lda_qda.html#yaml",
    "title": "Lab 03",
    "section": "",
    "text": "Create a .qmd file for your lab, make sure the author is your name, and Render the document."
  },
  {
    "objectID": "labs/03-lda_qda.html#conceptual-questions",
    "href": "labs/03-lda_qda.html#conceptual-questions",
    "title": "Lab 03",
    "section": "Conceptual questions",
    "text": "Conceptual questions\n\nSuppose that an individual has a 23% chance of defaulting on their credit card payment. What are the odds that they will default?"
  },
  {
    "objectID": "labs/03-lda_qda.html#logistic-regression",
    "href": "labs/03-lda_qda.html#logistic-regression",
    "title": "Lab 03",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nFor this lab we are using the Smarket data. Examine this data set - how many observations are there? How many columns? What are the variables?\nLet‚Äôs look at the correlation between all of the variables. Add the code below to your .qmd file. What can you learn from this visualization? Which pair of variables have the highest correlation?\n\n\nggpairs(Smarket, \n        lower = list(combo = wrap(ggally_facethist, binwidth = 0.5)), \n        progress = FALSE)\n\n\nInference Fit a logistic regression model to predict Direction using Lag1, Lag2, Lag3, Lag4, Lag5, and Volume. Show a table that contains the coefficients and p-values along with the confidence intervals for each of the 6 predictors. Which predictor has the smallest p-value? Interpret the coefficient, confidence interval, and p-value for this predictor.\nInference Exponentiate the results from Exercise 4. Interpret the odds ratio for the same predictor you selected in Exercise 4.\nPrediction Using 5-fold cross validation, fit the same logistic regression model as Exercise 4. What is the test Accuracy for this model? Interpret this result.\nInference Fit a logistic regression model to predict Direction using only Lag1 and Lag2. Show a table that contains the coefficients and p-values along with the confidence intervals for each of the 2 predictors. Which predictor has the smallest p-value? Interpret the coefficient, confidence interval, and p-value for this predictor.\nInference Exponentiate the results from Exercise 7. Interpret the odds ratio for the same predictor you selected in Exercise 7.\nPrediction Using 5-fold cross validation, fit the same logistic regression model as Exercise 7. What is the test Accuracy for this model? Interpret this result.\nIf you had to choose between the model fit in Exercise 4 and the one fit in Exercise 7, which would you choose? Why?"
  },
  {
    "objectID": "labs/01-cv-tm.html",
    "href": "labs/01-cv-tm.html",
    "title": "Lab 01",
    "section": "",
    "text": "Go to our RStudio and create a new R project inside your class folder."
  },
  {
    "objectID": "labs/01-cv-tm.html#yaml",
    "href": "labs/01-cv-tm.html#yaml",
    "title": "Lab 01",
    "section": "YAML:",
    "text": "YAML:\nOpen the .qmd file in your project, make sure the author is your name, and Render the document."
  },
  {
    "objectID": "labs/01-cv-tm.html#conceptual-questions",
    "href": "labs/01-cv-tm.html#conceptual-questions",
    "title": "Lab 01",
    "section": "Conceptual questions",
    "text": "Conceptual questions\n\nExplain how k-fold Cross Validation is implemented.\nWhat are the advantages / disadvantages of k-fold Cross Validation compared to the Validation Set approach? What are the advantages / disadvantages of k-fold Cross Validation compared to Leave-one-out Cross Validation?"
  },
  {
    "objectID": "labs/01-cv-tm.html#data-exploration",
    "href": "labs/01-cv-tm.html#data-exploration",
    "title": "Lab 01",
    "section": "Data exploration",
    "text": "Data exploration\n\nFor this analysis, we are using the Auto dataset from the ISLR package. How many rows are in this dataset? How many columns? Is there any missing data?\nOur outcome of interest is miles per gallon: mpg. Create a publication-ready figure examining the distribution of this variable.\nOur main predictor of interest is horsepower. Create a publication-ready figure looking at the relationship between miles per gallon and horsepower."
  },
  {
    "objectID": "labs/01-cv-tm.html#k-fold-cross-validation",
    "href": "labs/01-cv-tm.html#k-fold-cross-validation",
    "title": "Lab 01",
    "section": "K-fold cross validation",
    "text": "K-fold cross validation\nWe are trying to decide between three models of varying flexibility:\n\nModel 1: \\(\\texttt{mpg} = \\beta_0 + \\beta_1 \\texttt{horsepower} + \\epsilon\\)\nModel 2: \\(\\texttt{mpg} = \\beta_0 + \\beta_1 \\texttt{horsepower} + \\beta_2 \\texttt{horsepower}^2 + \\epsilon\\)\nModel 3: \\(\\texttt{mpg} = \\beta_0 + \\beta_1 \\texttt{horsepower} + \\beta_2 \\texttt{horsepower}^2 + \\beta_3 \\texttt{horsepower}^3 + \\epsilon\\)\n\n\nUsing the Auto data, split the data into two groups a training data set, saved as Auto_train and a testing data set, saved as Auto_test. Be sure to set a seed to ensure that you get the same result each time you Render your document.\n\n\n\nYou can use the poly() function to fit a model with a polynomial term. For example, to fit the model \\(y = \\beta_0 + \\beta_1 \\texttt{x} + \\beta_2 \\texttt{x}^2 + \\beta_3 \\texttt{x}^3 + \\epsilon\\), you would run fit(lm_spec, y ~ poly(x, 3), data = data)\n\nFit the three models outlined above on the training data. Using the model created on the training data, predict mpg in the test data set for each model. What is the test RMSE for the three models? Which model would you choose?\nFit the same three models, but instead of the validation set approach, perform 5-fold cross validation. Make sure to set a seed so you get the same answer each time you run the analysis. Calculate the overall 5-fold cross validation error for each of the three models. Which model would you chose?\nThe tidymodels package allows you to do this faster! Instead of having a fit 3 (or more!) different models to determine the best flexibility, you can (1) create a recipe to specify how you would like to fit a model and then (2) tune this model to determine the best output. Copy the code below. What do you think the line step_poly(horsepower, degree = tune()) does? Hint: you can run ?step_poly in the Console to learn more about this function.\n\n\nauto_prep &lt;- Auto |&gt;\n  recipe(mpg ~ horsepower) |&gt;\n  step_poly(horsepower, degree = tune())\n\n\nTo tune this model, you will replace fit_resamples with tune_grid. The pseudo code to do this is below - you may need to update some names to match what you have named objects in your document. Add the code to tune your model based on the code below.\n\n\nauto_tune &lt;- tune_grid(lm_spec,\n          auto_prep,\n          resamples = auto_cv)\n\n\nUsing the collect_metrics function, look at the RMSE for auto_tune. Which degree is preferable?\nYou can plot the output from Exercise 11 to make it a bit easier to determine. First, save your output from Exercise 11 as auto_metrics. Then filter this data frame to only include rows where .metric == \"rmse\". Save this filtered data frame as auto_rmse. Edit the code below to plot the degree on the x-axis and mean on the y-axis. Describe what this plot shows.\n\n\nggplot(auto_rmse, aes(x = ----, y = ----)) + \n  geom_line() +\n  geom_pointrange(aes(ymin = mean - std_err, ymax = mean + std_err)) + \n  labs(x = \"Degree\",\n       y = \"Cross validation error\",\n       title = ---)"
  },
  {
    "objectID": "hw/hw-01-intro-review.html",
    "href": "hw/hw-01-intro-review.html",
    "title": "Homework 1",
    "section": "",
    "text": "R is the name of the programming language itself and RStudio is a convenient interface.\nThe main goal of this homework is to re-introduce you to R and RStudio, which we will be using throughout the course both to learn the statistical concepts discussed in the course and to analyze real data and come to informed conclusions.\nAs the homework‚Äôs progress, you are encouraged to explore beyond what the homework dictates; a willingness to experiment will make you a much better programmer. Before we get to that stage, however, you need to build some basic fluency in R. Today we begin with the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands."
  },
  {
    "objectID": "hw/hw-01-intro-review.html#this-one-time",
    "href": "hw/hw-01-intro-review.html#this-one-time",
    "title": "Homework 1",
    "section": "This One Time",
    "text": "This One Time\n\nGo to our RStudio Server at http://turing.cornellcollege.edu:8787/\nClick File tab on the bottom right and then click the work Home.\nCreate a new folder using the little folder icon with the green plus on it. Use STA 362 in the folder name."
  },
  {
    "objectID": "hw/hw-01-intro-review.html#every-homeworklabactivity",
    "href": "hw/hw-01-intro-review.html#every-homeworklabactivity",
    "title": "Homework 1",
    "section": "Every Homework/lab/activity",
    "text": "Every Homework/lab/activity\nEach of your assignments will begin with the following steps.\n\nFinding the instructions on our website: https://sta362-sb8-24.github.io/STA362StatLearning/\nGoing to our RStudio Server at http://turing.cornellcollege.edu:8787/\nCreating a new project. and giving it a sensible name such as homework_1 and having that project in the course folder you created.\nCreate a new quarto document and give it a sensible name such as hw1.\nIn the YAML add the following (add what you don‚Äôt have). The embed-resources component will make your final rendered html self-contained.\n\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\n    embed-resources: true\n---"
  },
  {
    "objectID": "hw/hw-01-intro-review.html#yaml",
    "href": "hw/hw-01-intro-review.html#yaml",
    "title": "Homework 1",
    "section": "YAML",
    "text": "YAML\nIn your Quarto (qmd) file in your project, change the author name to your name, and render the document. Make sure that you also have added the extra YAML clode above."
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "Introduction to Statistical Learning",
    "section": "",
    "text": "Course Description\nThis course will introduce students to relatively new and powerful statistical techniques used to analyze data. The course will begin with a review of linear regression and an introduction to computer-based variable and model selection methods. Other topics will include classification methods, resampling methods for model-building, non-linear models, and tree-based methods. The computer software program R will be used throughout.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "course-instructor.html",
    "href": "course-instructor.html",
    "title": "Instructor",
    "section": "",
    "text": "Dr.¬†Tyler George (he/him) is a Assistant Professor of Statistics at Cornell College. He received his PhD in Statistics and Analytics from Central Michigan University. During his PhD he also studied mathematics and statistics education. His dissertation work involved creating a new lack of fit test for linear regression models. His interests are broadly in statistics, data science and best pedagogy to teach them.\n\n\n\nOffice hours\nLocation\n\n\n\n\nMonday - Thursday 3:05pm - 4:05pm\nWest 311\n\n\nOther Times by Appointment\nWest 311\n\n\n\nOffice hours are for STUDENTS. Please take advantage of them to get help with class, advising, and/or getting to know your professor! If you miss class, check out course calendar to verify there have been no changes.",
    "crumbs": [
      "Course information",
      "Instructor"
    ]
  },
  {
    "objectID": "course-instructor.html#instructor",
    "href": "course-instructor.html#instructor",
    "title": "Instructor",
    "section": "",
    "text": "Dr.¬†Tyler George (he/him) is a Assistant Professor of Statistics at Cornell College. He received his PhD in Statistics and Analytics from Central Michigan University. During his PhD he also studied mathematics and statistics education. His dissertation work involved creating a new lack of fit test for linear regression models. His interests are broadly in statistics, data science and best pedagogy to teach them.\n\n\n\nOffice hours\nLocation\n\n\n\n\nMonday - Thursday 3:05pm - 4:05pm\nWest 311\n\n\nOther Times by Appointment\nWest 311\n\n\n\nOffice hours are for STUDENTS. Please take advantage of them to get help with class, advising, and/or getting to know your professor! If you miss class, check out course calendar to verify there have been no changes.",
    "crumbs": [
      "Course information",
      "Instructor"
    ]
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "To access computing resources the course, Introduction to Data Science, offered by the Cornell College Department of Mathematics and Statistics, go to the RStudio Server while on campus and connected to campus internet.\nYour account will be pre-created before the class begins and will use your Cornell College username. The default password will be shared in class and you will need to change it.",
    "crumbs": [
      "Course information",
      "R/RStudio Access"
    ]
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful links",
    "section": "",
    "text": "RStudio Server\nüîó on Cornell College Cluster\n\n\nCourse GitHub organization\nüîó on GitHub\n\n\nGradebook\nüîó on Moodle",
    "crumbs": [
      "Course Contents",
      "Useful links"
    ]
  },
  {
    "objectID": "course-links.html#course-links",
    "href": "course-links.html#course-links",
    "title": "Useful links",
    "section": "",
    "text": "RStudio Server\nüîó on Cornell College Cluster\n\n\nCourse GitHub organization\nüîó on GitHub\n\n\nGradebook\nüîó on Moodle",
    "crumbs": [
      "Course Contents",
      "Useful links"
    ]
  },
  {
    "objectID": "course-links.html#other-useful-links",
    "href": "course-links.html#other-useful-links",
    "title": "Useful links",
    "section": "Other Useful Links",
    "text": "Other Useful Links\n\nData Wrangling and Viz Interactive Tutorials\nRStudio Cheatsheets\nIntroduction to dplyr\nR Date Examples\nNY Times Cornell College Sign-up\nR for Data Science 2nd Ed\nQuarto Documentation\nData visualization\n\nggplot2 Reference\nggplot2: Elegant Graphics for Data Analysis\nData Visualization: A Practice Introduction\nPatchwork R Package",
    "crumbs": [
      "Course Contents",
      "Useful links"
    ]
  },
  {
    "objectID": "course-links.html#data-links",
    "href": "course-links.html#data-links",
    "title": "Useful links",
    "section": "Data Links",
    "text": "Data Links\n\nTidyTuesday\nR Data Sources for Regression Analysis\nFiveThirtyEight data\nAmazon Registry of Open Data\nOpen data StackExchange\nMicrosoft R Application Window\nData.gov\nUS Census\nNew York City data\nGeorge Mason University Data Link List\nToward Data Science list of Data Sources\nNHS Scotland Open Data\nEdinburgh Open Data\nOpen access to Scotland‚Äôs official statistics\nBikeshare data portal\nUK Gov Data\nKaggle datasets\nOpenIntro datasets\nAwesome public datasets\nYouth Risk Behavior Surveillance System (YRBSS)\nPRISM Data Archive Project\nHarvard Dataverse\nAndrew G. Reiter Poly Scie Datasets\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies\nU.S. Data\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nState of Iowa Open Geospatial Data\nIf you know of others, let me know!",
    "crumbs": [
      "Course Contents",
      "Useful links"
    ]
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you can identify when that is without getting too frustrated and feel comfortable seeking help.\nYou can ask anonymous course questions by adding them at https://docs.google.com/spreadsheets/d/1R33yYyRdFoRMDtSnD-CzLOqIEwYNrC3kepMm4ZN_PWM/edit?usp=sharing.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#lectures-and-labs",
    "href": "course-support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#office-hours",
    "href": "course-support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nYou are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage each and every one of you to take advantage of this resource! Make a pledge to stop by office hours at least once during the first few days of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of your professors office hours here.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#academic-support",
    "href": "course-support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\n\nQuantitative Reasoning Studio (QRS)\nThere are times you may need help outside of class or office hours. Or, maybe you need something explained in a different way. In those instances, I encourage you to visit the Quantitative Reasoning Studio in Cole Library room 322. The Quantitative Reasoning Studio (QRS) offers free tutoring to all students at Cornell College. There will be at least 1 peer tutor that has taken this course and will be able to help you, if you arrive at a time they are working. Feel free to email Jessica Johanningmeier at QRS@cornellcollege.edu to ask when the tutor for this class will be available. They often will have a schedule posted on the wall in the studio.\n\n\nQRS Hours\n\n\n\nDay(s)\nTimes\n\n\n\n\nMonday-Thursday\n8 a.m. - 5 p.m. and 7 p.m. - 10 p.m.\n\n\nFriday\n8 a.m. - 5 p.m.\n\n\nSunday\n3 p.m. - 5 p.m. and 7 p.m. - 10 p.m.\n\n\n\n\n\nDungy Writing Studio\nFor help with your writing, visit the Dungy Writing Studio. You can make online appointments individual or groups to get help with items such as your group project. If you have any questions about the studio, email Dungy Writing Studio Director and Director of Fellowships and Scholarships, Laura Farmer, at lfarmer@cornellcollege.edu.\n\n\nWriting Studio Hours\n\n\n\nDay(s)\nTimes\n\n\n\n\nMonday-Thursday\n8 a.m. - 5 p.m. and 7 p.m. - 10 p.m.\n\n\nFriday\n8 a.m. - 5 p.m.\n\n\nSunday\n1 p.m. - 5 p.m.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#student-success-center",
    "href": "course-support.html#student-success-center",
    "title": "Course support",
    "section": "Student Success Center",
    "text": "Student Success Center\nThe Student Success Center is a resource for all students. Their staff serves as student success coaches for all students and welcome students to visit us to talk about academic concerns, study plans, finding their place at Cornell, or any questions you have and aren‚Äôt sure where to start! You can walk in to chat or contact a staff member directly to set up an appointment! See the website for more information.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#professor-email",
    "href": "course-support.html#professor-email",
    "title": "Course support",
    "section": "Professor Email",
    "text": "Professor Email\nIf you are not available during office hours times or have a questions later in the evening or other times outside of class, email your professor at tgeorge@cornellcollege.edu. If your question involves code - it is very likely you will need to meet with him to get help. Please reach out with any concerns you have during the course!",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#ebersole-health-and-wellbeing-center",
    "href": "course-support.html#ebersole-health-and-wellbeing-center",
    "title": "Course support",
    "section": "Ebersole Health and Wellbeing Center",
    "text": "Ebersole Health and Wellbeing Center\nThe mission of Cornell College Student Health Services complements the mission of the college by promoting the optimal well-being of students. We do this by:\n\nproviding and coordinating quality health care services\nadvocating for students in their pursuit of health and wellness\npreparing students to be their own health advocates and informed consumers of appropriate health care services\nproviding health education to promote the development of healthy lifestyles\n\nThe Student Health Center is located in the Ebersole Building, directly south of the Thomas Commons. Appointments are preferred. You can schedule an appointment online or by phone at 319-895-4292. Walk-ins will be accommodated as time permits. Appointments with the nurse are free.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#technology-support",
    "href": "course-support.html#technology-support",
    "title": "Course support",
    "section": "Technology Support",
    "text": "Technology Support\nIf you have issues with your computer during the block, IT may be able to help. Please submit a ticket.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#course-materials-costs",
    "href": "course-support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.).",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Statistical Learning Schedule",
    "section": "",
    "text": "Note: The timeline of topics and assignments might be updated throughout the semester.\n\n\n\n\n\n\n\n\n\nDay\nDate\nTopic\nNotes\nOutline\nLab\nHomework\nExam\n\n\n\n\n1\n15 April\nCourse Intro, Ch 1 - Stat Learning Examples, Ch 2 - SL, Bias-Variance Tradeoff\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\n16 April\nCh 5 - Cross Validation, Tidy Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\n17 April\nCh 5 - Cross Validation, Tidy Models, Ch 3 - LR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4\n18 April\nCh 4 - Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5\n19 April\nCh 4 - LDA, QDA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6\n22 April\nCh 4 - Naive Bayes, KMeans Classification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7\n23 April\nCh 6 - Subset Selection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8\n24 April\nCh 6 - Shrinkage Methods\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9\n25 April\nTidymodel recipies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10\n26 April\nExam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11\n29 April\nCh 6 - Dimension Reduction, project\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12\n30 April\nCh 8 - Decision Trees, project\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13\n1 May\nCh 8 - Bagging, RF, Boosting, Bayesian Additive, project\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14\n2 May\nCh 8 - Bagging, RF, Boosting, Bayesian Additive, project\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15\n3 May\nCh 12 - PCA, project\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16\n6 May\nCh 12 - Clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n17\n7 May\nCh 12 - Clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n18\n8 May\nExam",
    "crumbs": [
      "Course Contents",
      "Schedule & Assignments"
    ]
  },
  {
    "objectID": "labs/02-logistic.html",
    "href": "labs/02-logistic.html",
    "title": "Lab 02",
    "section": "",
    "text": "Go to our RStudio and create a new R project inside your class folder.\n\n\nCreate a .qmd file for your lab, make sure the author is your name, and Render the document."
  },
  {
    "objectID": "labs/02-logistic.html#yaml",
    "href": "labs/02-logistic.html#yaml",
    "title": "Lab 02",
    "section": "",
    "text": "Create a .qmd file for your lab, make sure the author is your name, and Render the document."
  },
  {
    "objectID": "labs/02-logistic.html#conceptual-questions",
    "href": "labs/02-logistic.html#conceptual-questions",
    "title": "Lab 02",
    "section": "Conceptual questions",
    "text": "Conceptual questions\n\nSuppose that an individual has a 23% chance of defaulting on their credit card payment. What are the odds that they will default?"
  },
  {
    "objectID": "labs/02-logistic.html#logistic-regression",
    "href": "labs/02-logistic.html#logistic-regression",
    "title": "Lab 02",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nFor this lab we are using the Smarket data. Examine this data set - how many observations are there? How many columns? What are the variables?\nLet‚Äôs look at the correlation between all of the variables. Add the code below to your .qmd file. What can you learn from this visualization? Which pair of variables have the highest correlation?\n\n\nggpairs(Smarket, \n        lower = list(combo = wrap(ggally_facethist, binwidth = 0.5)), \n        progress = FALSE)\n\n\nInference Fit a logistic regression model to predict Direction using Lag1, Lag2, Lag3, Lag4, Lag5, and Volume. Show a table that contains the coefficients and p-values along with the confidence intervals for each of the 6 predictors. Which predictor has the smallest p-value? Interpret the coefficient, confidence interval, and p-value for this predictor.\nInference Exponentiate the results from Exercise 4. Interpret the odds ratio for the same predictor you selected in Exercise 4.\nPrediction Using 5-fold cross validation, fit the same logistic regression model as Exercise 4. What is the test Accuracy for this model? Interpret this result.\nInference Fit a logistic regression model to predict Direction using only Lag1 and Lag2. Show a table that contains the coefficients and p-values along with the confidence intervals for each of the 2 predictors. Which predictor has the smallest p-value? Interpret the coefficient, confidence interval, and p-value for this predictor.\nInference Exponentiate the results from Exercise 7. Interpret the odds ratio for the same predictor you selected in Exercise 7.\nPrediction Using 5-fold cross validation, fit the same logistic regression model as Exercise 7. What is the test Accuracy for this model? Interpret this result.\nIf you had to choose between the model fit in Exercise 4 and the one fit in Exercise 7, which would you choose? Why?"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section",
    "href": "slides/01-02-welcome_to_sl.html#section",
    "title": "Chapter 1 and 2",
    "section": "üëã",
    "text": "üëã\nTyler George\n ¬† tgeorge@cornellcollege.edu\n ¬† MWTh 3:05pm-4:05pm and by appt."
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#course-website",
    "href": "slides/01-02-welcome_to_sl.html#course-website",
    "title": "Chapter 1 and 2",
    "section": "Course Website",
    "text": "Course Website\nhttps://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#intros",
    "href": "slides/01-02-welcome_to_sl.html#intros",
    "title": "Chapter 1 and 2",
    "section": "Intros",
    "text": "Intros\n\nName\nMajor\nFun OR boring fact"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#statistical-learning-problems",
    "href": "slides/01-02-welcome_to_sl.html#statistical-learning-problems",
    "title": "Chapter 1 and 2",
    "section": "Statistical Learning Problems",
    "text": "Statistical Learning Problems\n\n\n\nIdentify risk factors for breast cancer\n\n\n\n\n\n\nDr.¬†Tyler George adapted from slides by Hastie & Tibshirani"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#statistical-learning-problems-1",
    "href": "slides/01-02-welcome_to_sl.html#statistical-learning-problems-1",
    "title": "Chapter 1 and 2",
    "section": "Statistical Learning Problems",
    "text": "Statistical Learning Problems\n\n\n\nCustomize an email spam detection system\n\n\n\nData: 4601 labeled emails sent to George who works at HP Labs\nInput features: frequencies of words and punctuation\n\n\n\n\n\n\n‚Äî\ngeorge\nyou\nhp\nfree\n!\nedu\nremove\n\n\n\n\nspam\n0.00\n2.26\n0.02\n0.52\n0.51\n0.01\n0.28\n\n\nemail\n2.27\n1.27\n0.90\n0.07\n0.11\n0.29\n0.01\n\n\n\n\nDr.¬†Tyler George adapted from slides by Hastie & Tibshirani"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#statistical-learning-problems-2",
    "href": "slides/01-02-welcome_to_sl.html#statistical-learning-problems-2",
    "title": "Chapter 1 and 2",
    "section": "Statistical Learning Problems",
    "text": "Statistical Learning Problems\n\n\n\nIdentify numbers in handwritten zip code\n\n\n\n\n\n\n\nDr.¬†Tyler George adapted from slides by Hastie & Tibshirani"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#statistical-learning-problems-3",
    "href": "slides/01-02-welcome_to_sl.html#statistical-learning-problems-3",
    "title": "Chapter 1 and 2",
    "section": "Statistical Learning Problems",
    "text": "Statistical Learning Problems\n\n\nEstablish the relationship between variables in population survey data\n\nIncome survey data for males from the central Atlantic region of US, 2009\n\n\n\n\nDr.¬†Tyler George adapted from slides by Hastie & Tibshirani"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#types-of-statistical-learning",
    "href": "slides/01-02-welcome_to_sl.html#types-of-statistical-learning",
    "title": "Chapter 1 and 2",
    "section": "‚úåÔ∏è types of statistical learning",
    "text": "‚úåÔ∏è types of statistical learning\n\nSupervised Learning\nUnsupervised Learning"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#supervised-learning",
    "href": "slides/01-02-welcome_to_sl.html#supervised-learning",
    "title": "Chapter 1 and 2",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nDr.¬†Tyler George adapted from slides by Hastie & Tibshirani\n\n\noutcome variable: \\(Y\\), (dependent variable, response, target)\npredictors: vector of \\(p\\) predictors, \\(X\\), (inputs, regressors, covariates, features, independent variables)\nIn the regression problem, \\(Y\\) is quantitative (e.g price, blood pressure)\nIn the classification problem, \\(Y\\) takes values in a finite, unordered set (survived/died, digit 0-9, cancer class of tissue sample)\nWe have training data \\((x_1, y_1), \\dots, (x_N, y_N)\\). These are observations (examples, instances) of these measurements"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#supervised-learning-1",
    "href": "slides/01-02-welcome_to_sl.html#supervised-learning-1",
    "title": "Chapter 1 and 2",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nWhat do you think are some objectives here?\n\nObjectives\n\nAccurately predict unseen test cases\nUnderstand which inputs affect the outcome, and how\nAssess the quality of our predictions and inferences\n\n\nDr.¬†Tyler George adapted from slides by Hastie & Tibshirani"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#unsupervised-learning",
    "href": "slides/01-02-welcome_to_sl.html#unsupervised-learning",
    "title": "Chapter 1 and 2",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nDr.¬†Tyler George adapted from slides by Hastie & Tibshirani\n\n\nNo outcome variable, just a set of predictors (features) measured on a set of samples\nobjective is more fuzzy ‚Äì find groups of samples that behave similarly, find features that behave similarly, find linear combinations of features with the most variation\ndifficult to know how well your are doing\ndifferent from supervised learning, but can be useful as a pre-processing step for supervised learning"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#lets-take-a-tour---class-website",
    "href": "slides/01-02-welcome_to_sl.html#lets-take-a-tour---class-website",
    "title": "Chapter 1 and 2",
    "section": "Let‚Äôs take a tour - class website",
    "text": "Let‚Äôs take a tour - class website\n\n\n\n\n\nConcepts introduced:\n\nHow to find slides\nHow to find assignments\nHow to find RStudio\nHow to get help\nHow to find policies"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-and-classification",
    "href": "slides/01-02-welcome_to_sl.html#regression-and-classification",
    "title": "Chapter 1 and 2",
    "section": "Regression and Classification",
    "text": "Regression and Classification\n\nRegression: quantitative response\nClassification: qualitative (categorical) response"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-and-classification-1",
    "href": "slides/01-02-welcome_to_sl.html#regression-and-classification-1",
    "title": "Chapter 1 and 2",
    "section": "Regression and Classification",
    "text": "Regression and Classification\n\nWhat would be an example of a regression problem?\n\n\nRegression: quantitative response\nClassification: qualitative (categorical) response"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-and-classification-2",
    "href": "slides/01-02-welcome_to_sl.html#regression-and-classification-2",
    "title": "Chapter 1 and 2",
    "section": "Regression and Classification",
    "text": "Regression and Classification\n\nWhat would be an example of a classification problem?\n\n\nRegression: quantitative response\nClassification: qualitative (categorical) response"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#auto-data",
    "href": "slides/01-02-welcome_to_sl.html#auto-data",
    "title": "Chapter 1 and 2",
    "section": "Auto data",
    "text": "Auto data\n\n\n\nAbove are mpg vs horsepower, weight, and acceleration, with a blue linear-regression line fit separately to each. Can we predict mpg using these three?\n\nMaybe we can do better using a model:\n\\[\\texttt{mpg} \\approx f(\\texttt{horsepower}, \\texttt{weight}, \\texttt{acceleration})\\]"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#notation",
    "href": "slides/01-02-welcome_to_sl.html#notation",
    "title": "Chapter 1 and 2",
    "section": "Notation",
    "text": "Notation\n\nmpg is the response variable, the outcome variable, we refer to this as \\(Y\\)\nhorsepower is a feature, input, predictor, we refer to this as \\(X_1\\)\nweight is \\(X_2\\)\nacceleration is \\(X_3\\)\nOur input vector is:\n\n\\(X = \\begin{bmatrix} X_1 \\\\X_2 \\\\X_3\\end{bmatrix}\\)\n\nOur model is\n\n\\(Y = f(X) + \\varepsilon\\)\n\n\\(\\varepsilon\\) is our error"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#why-do-we-care-about-fx",
    "href": "slides/01-02-welcome_to_sl.html#why-do-we-care-about-fx",
    "title": "Chapter 1 and 2",
    "section": "Why do we care about \\(f(X)\\)?",
    "text": "Why do we care about \\(f(X)\\)?\n\nWe can use \\(f(X)\\) to make predictions of \\(Y\\) for new values of \\(X = x\\)\nWe can gain a better understanding of which components of \\(X = (X_1, X_2, \\dots, X_p)\\) are important for explaining \\(Y\\)\nDepending on how complex \\(f\\) is, maybe we can understand how each component ( \\(X_j\\) ) of \\(X\\) affects \\(Y\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx",
    "href": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx",
    "title": "Chapter 1 and 2",
    "section": "How do we choose \\(f(X)\\)?",
    "text": "How do we choose \\(f(X)\\)?\nWhat is a good value for \\(f(X)\\) at any selected value of \\(X\\), say \\(X = 100\\)? There can be many \\(Y\\) values at \\(X = 100\\)."
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx-1",
    "href": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx-1",
    "title": "Chapter 1 and 2",
    "section": "How do we choose \\(f(X)\\)?",
    "text": "How do we choose \\(f(X)\\)?\nWhat is a good value for \\(f(X)\\) at any selected value of \\(X\\), say \\(X = 100\\)? There can be many \\(Y\\) values at \\(X = 100\\)."
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx-2",
    "href": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx-2",
    "title": "Chapter 1 and 2",
    "section": "How do we choose \\(f(X)\\)?",
    "text": "How do we choose \\(f(X)\\)?\nWhat is a good value for \\(f(X)\\) at any selected value of \\(X\\), say \\(X = 100\\)? There can be many \\(Y\\) values at \\(X = 100\\).\n\nThere are 17 points here, what value should I choose for f(100). What do you think the blue dot represents?"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx-3",
    "href": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx-3",
    "title": "Chapter 1 and 2",
    "section": "How do we choose \\(f(X)\\)?",
    "text": "How do we choose \\(f(X)\\)?\nA good value is\n\\[f(100) = E(Y|X = 100)\\]\n\n\\(E(Y|X = 100)\\) means expected value (average) of \\(Y\\) given \\(X = 100\\)\n\n\nThis ideal \\(f(x) = E(Y | X = x)\\) is called the regression function"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-function-fx",
    "href": "slides/01-02-welcome_to_sl.html#regression-function-fx",
    "title": "Chapter 1 and 2",
    "section": "Regression function, \\(f(X)\\)",
    "text": "Regression function, \\(f(X)\\)\n\nAlso works or a vector, \\(X\\), for example,\n\n\\[f(x) = f(x_1, x_2, x_3) = E[Y | X_1 = x_1, X_2 = x_2, X_3 = x_3]\\]\n\nThis is the optimal predictor of \\(Y\\) in terms of mean-squared prediction error"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-function-fx-1",
    "href": "slides/01-02-welcome_to_sl.html#regression-function-fx-1",
    "title": "Chapter 1 and 2",
    "section": "Regression function, \\(f(X)\\)",
    "text": "Regression function, \\(f(X)\\)\n\n\\(f(x) = E(Y|X = x)\\) is the function that minimizes \\(E[(Y - g(X))^2 |X = x]\\) over all functions \\(g\\) at all points \\(X = x\\)\n\n\n\\(\\varepsilon = Y - f(x)\\) is the irreducible error\neven if we knew \\(f(x)\\), we would still make errors in prediction, since at each \\(X = x\\) there is typically a distribution of possible \\(Y\\) values"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-function-fx-2",
    "href": "slides/01-02-welcome_to_sl.html#regression-function-fx-2",
    "title": "Chapter 1 and 2",
    "section": "Regression function, \\(f(X)\\)",
    "text": "Regression function, \\(f(X)\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-function-fx-3",
    "href": "slides/01-02-welcome_to_sl.html#regression-function-fx-3",
    "title": "Chapter 1 and 2",
    "section": "Regression function, \\(f(X)\\)",
    "text": "Regression function, \\(f(X)\\)\n\nUsing these points, how would I calculate the regression function?\n\n\n\nTake the average! \\(f(100) = E[\\texttt{mpg}|\\texttt{horsepower} = 100] = 19.6\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-function-fx-4",
    "href": "slides/01-02-welcome_to_sl.html#regression-function-fx-4",
    "title": "Chapter 1 and 2",
    "section": "Regression function, \\(f(X)\\)",
    "text": "Regression function, \\(f(X)\\)\n\nThis point has a \\(Y\\) value of 32.9. What is \\(\\hat\\varepsilon\\)?\n\n\n\\(\\hat\\varepsilon = Y - \\hat{f}(X) = 32.9 - 19.6 = \\color{red}{13.3}\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#the-error",
    "href": "slides/01-02-welcome_to_sl.html#the-error",
    "title": "Chapter 1 and 2",
    "section": "The error",
    "text": "The error\nFor any estimate, \\(\\hat{f}(x)\\), of \\(f(x)\\), we have\n\\[E[(Y - \\hat{f}(x))^2 | X = x] = \\underbrace{[f(x) - \\hat{f}(x)]^2}_{\\textrm{reducible error}} + \\underbrace{Var(\\varepsilon)}_{\\textrm{irreducible error}}\\]\n\nAssume for a moment that both \\(\\hat{f}\\) and X are fixed.\n\\(E(Y ‚àí \\hat{Y})^2\\) represents the average, or expected value, of the squared difference between the predicted and actual value of Y, and Var( \\(\\varepsilon\\) ) represents the variance associated with the error term\nThe focus of this class is on techniques for estimating f with the aim of minimizing the reducible error.\nthe irreducible error will always provide an upper bound on the accuracy of our prediction for Y\nThis bound is almost always unknown in practice"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#estimating-f",
    "href": "slides/01-02-welcome_to_sl.html#estimating-f",
    "title": "Chapter 1 and 2",
    "section": "Estimating \\(f\\)",
    "text": "Estimating \\(f\\)\n\nTypically we have very few (if any!) data points at \\(X=x\\) exactly, so we cannot compute \\(E[Y|X=x]\\)\nFor example, what if we were interested in estimating miles per gallon when horsepower was 104.\n\n\n\n\n\nüí° We can relax the definition and let\n\\[\\hat{f}(x) = E[Y | X\\in \\mathcal{N}(x)]\\]\n\nWhere \\(\\mathcal{N}(x)\\) is some neighborhood of \\(x\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#notation-pause",
    "href": "slides/01-02-welcome_to_sl.html#notation-pause",
    "title": "Chapter 1 and 2",
    "section": "Notation pause!",
    "text": "Notation pause!\n\n\\[\\hat{f}(x) = \\underbrace{E}_{\\textrm{The expectation}}[\\underbrace{Y}_{\\textrm{of Y}} \\underbrace{|}_{\\textrm{given}} \\underbrace{X\\in \\mathcal{N}(x)}_{\\textrm{X is in the neighborhood of x}}]\\]\n\n\nüö® If you need a notation pause at any point during this class, please let me know!"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#estimating-f-1",
    "href": "slides/01-02-welcome_to_sl.html#estimating-f-1",
    "title": "Chapter 1 and 2",
    "section": "Estimating \\(f\\)",
    "text": "Estimating \\(f\\)\nüí° We can relax the definition and let\n\\[\\hat{f}(x) = E[Y | X\\in \\mathcal{N}(x)]\\]\n\nNearest neighbor averaging does pretty well with small \\(p\\) ( \\(p\\leq 4\\) ) and large \\(n\\)\nNearest neighbor is not great when \\(p\\) is large because of the curse of dimensionality (because nearest neighbors tend to be far away in high dimensions)\n\n\n\nWhat do I mean by \\(p\\)? What do I mean by \\(n\\)?"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#parametric-models",
    "href": "slides/01-02-welcome_to_sl.html#parametric-models",
    "title": "Chapter 1 and 2",
    "section": "Parametric models",
    "text": "Parametric models\nA common parametric model is a linear model\n\\[f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p\\]\n\nA linear model has \\(p + 1\\) parameters ( \\(\\beta_0,\\dots,\\beta_p\\) )\nWe estimate these parameters by fitting a model to training data\nAlthough this model is almost never correct it can often be a good interpretable approximation to the unknown true function, \\(f(X)\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-1",
    "href": "slides/01-02-welcome_to_sl.html#section-1",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "The  red  points are simulated values for income from the model:\n\n\\[\\texttt{income} = f(\\texttt{education, senority}) + \\varepsilon\\]\n\n\\(f\\) is the  blue  surface"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-2",
    "href": "slides/01-02-welcome_to_sl.html#section-2",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Linear regression model fit to the simulated data\n\\[\\hat{f}_L(\\texttt{education, senority}) = \\hat{\\beta}_0 + \\hat{\\beta}_1\\texttt{education}+\\hat{\\beta}_2\\texttt{senority}\\]"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-3",
    "href": "slides/01-02-welcome_to_sl.html#section-3",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "More flexible regression model \\(\\hat{f}_S(\\texttt{education, seniority})\\) fit to the simulated data.\nHere we use a technique called a thin-plate spline to fit a flexible surface"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-4",
    "href": "slides/01-02-welcome_to_sl.html#section-4",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "And even MORE flexible üò± model \\(\\hat{f}(\\texttt{education, seniority})\\).\n\nHere we‚Äôve basically drawn the surface to hit every point, minimizing the error, but completely overfitting"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#finding-balance",
    "href": "slides/01-02-welcome_to_sl.html#finding-balance",
    "title": "Chapter 1 and 2",
    "section": "ü§π Finding balance",
    "text": "ü§π Finding balance\n\nPrediction accuracy versus interpretability\nLinear models are easy to interpret, thin-plate splines are not\nGood fit versus overfit or underfit\nHow do we know when the fit is just right?\nParsimony versus black-box\nWe often prefer a simpler model involving fewer variables over a black-box predictor involving them all"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy",
    "href": "slides/01-02-welcome_to_sl.html#accuracy",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\nWe‚Äôve fit a model \\(\\hat{f}(x)\\) to some training data.\nWe can measure accuracy as the average squared prediction error over that train data\n\n\\[MSE_{\\texttt{train}} = \\textrm{Ave}_{train}[y_i-\\hat{f}(x_i)]^2\\]\n\n\nWhat can go wrong here?\n\n\nThis may be biased towards overfit models"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy-1",
    "href": "slides/01-02-welcome_to_sl.html#accuracy-1",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\nI have some train data, plotted above. What \\(\\hat{f}(x)\\) would minimize the \\(MSE_{\\texttt{train}}\\)?\n\n\\[MSE_{\\texttt{train}} = \\textrm{Ave}_{train}[y_i-\\hat{f}(x_i)]^2\\]"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy-2",
    "href": "slides/01-02-welcome_to_sl.html#accuracy-2",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\nI have some train data, plotted above. What \\(\\hat{f}(x)\\) would minimize the \\(MSE_{\\texttt{train}}\\)?\n\n\\[MSE_{train} = \\textrm{Ave}_{i\\in\\texttt{train}}[y_i-\\hat{f}(x_i)]^2\\]"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy-3",
    "href": "slides/01-02-welcome_to_sl.html#accuracy-3",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\nWhat is wrong with this?\n\n\nIt‚Äôs overfit!"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy-4",
    "href": "slides/01-02-welcome_to_sl.html#accuracy-4",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\nIf we get a new sample, that overfit model is probably going to be terrible!"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy-5",
    "href": "slides/01-02-welcome_to_sl.html#accuracy-5",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\nWe‚Äôve fit a model \\(\\hat{f}(x)\\) to some training data.\nInstead of measuring accuracy as the average squared prediction error over that train data, we can compute it using fresh test data.\n\n\\[MSE_{\\texttt{test}} = \\textrm{Ave}_{test}[y_i-\\hat{f}(x_i)]^2\\]"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-6",
    "href": "slides/01-02-welcome_to_sl.html#section-6",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Black curve is the ‚Äútruth‚Äù on the left.  Red  curve on right is \\(MSE_{\\texttt{test}}\\), grey curve is \\(MSE_{\\texttt{train}}\\). Orange, blue and green curves/squares correspond to fis of different flexibility."
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-7",
    "href": "slides/01-02-welcome_to_sl.html#section-7",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Here the truth is smoother, so the smoother fit and linear model do really well"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-8",
    "href": "slides/01-02-welcome_to_sl.html#section-8",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Here the truth is wiggly and the noise is low, so the more flexible fits do the best"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#bias-variance-trade-off",
    "href": "slides/01-02-welcome_to_sl.html#bias-variance-trade-off",
    "title": "Chapter 1 and 2",
    "section": "Bias-variance trade-off",
    "text": "Bias-variance trade-off\n\nWe‚Äôve fit a model, \\(\\hat{f}(x)\\), to some training data\nLet‚Äôs pull a test observation from this population ( \\(x_0, y_0\\) )\nThe true model is \\(Y = f(x) + \\varepsilon\\)\n\\(f(x) = E[Y|X=x]\\)\n\n\n\\[E(y_0 - \\hat{f}(x_0))^2 = \\textrm{Var}(\\hat{f}(x_0)) + [\\textrm{Bias}(\\hat{f}(x_0))]^2 + \\textrm{Var}(\\varepsilon)\\]\n\n\nThe expectation averages over the variability of \\(y_0\\) as well as the variability of the training data. \\(\\textrm{Bias}(\\hat{f}(x_0)) =E[\\hat{f}(x_0)]-f(x_0)\\)\n\nAs flexibility of \\(\\hat{f}\\) \\(\\uparrow\\), its variance \\(\\uparrow\\) and its bias \\(\\downarrow\\)\nchoosing the flexibility based on average test error amounts to a bias-variance trade-off\n\n\n\nThat U-shape we see for the test MSE curves is due to this bias-variance trade-off\nThe expected test MSE for a given \\(x_0\\) can be decomposed into three components: the variance of \\(\\hat{f}(x_o)\\), the squared bias of \\(\\hat{f}(x_o)\\) and t4he variance of the error term \\(\\varepsilon\\)\nHere the notation \\(E[y_0 ‚àí \\hat{f}(x_0)]^2\\) defines the expected test MSE, and refers to the average test MSE that we would obtain if we repeatedly estimated \\(f\\) using a large number of training sets, and tested each at \\(x_0\\)\nThe overall expected test MSE can be computed by averaging \\(E[y_0 ‚àí \\hat{f}(x_0)]^2\\) over all possible values of \\(x_0\\) in the test set.\nSO we want to minimize the expected test error, so to do that we need to pick a statistical learning method to simultenously acheive low bias and low variance.\nSince both of these quantities are non-negative, the expected test MSE can never fall below Var( \\(\\varepsilon\\) )"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#bias-variance-trade-off-1",
    "href": "slides/01-02-welcome_to_sl.html#bias-variance-trade-off-1",
    "title": "Chapter 1 and 2",
    "section": "Bias-variance trade-off",
    "text": "Bias-variance trade-off"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#conceptual-idea",
    "href": "slides/01-02-welcome_to_sl.html#conceptual-idea",
    "title": "Chapter 1 and 2",
    "section": "Conceptual Idea",
    "text": "Conceptual Idea\nWatch StatQuest video: Machine Learning Fundamentals: Bias and Variance"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#notation-1",
    "href": "slides/01-02-welcome_to_sl.html#notation-1",
    "title": "Chapter 1 and 2",
    "section": "Notation",
    "text": "Notation\n\n\\(Y\\) is the response variable. It is qualitative\n\\(\\mathcal{C}(X)\\) is the classifier that assigns a class \\(\\mathcal{C}\\) to some future unlabeled observation, \\(X\\)\nExamples:\nEmail can be classified as \\(\\mathcal{C}=(\\texttt{spam, not spam})\\)\nWritten number is one of \\(\\mathcal{C}=\\{0, 1, 2, \\dots, 9\\}\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#classification-problem",
    "href": "slides/01-02-welcome_to_sl.html#classification-problem",
    "title": "Chapter 1 and 2",
    "section": "Classification Problem",
    "text": "Classification Problem\n\nWhat is the goal?\n\n\nBuild a classifier \\(\\mathcal{C}(X)\\) that assigns a class label from \\(\\mathcal{C}\\) to a future unlabeled observation \\(X\\)\n\nAssess the uncertainty in each classification\n\nUnderstand the roles of the different predictors among \\(X = (X_1, X_2, \\dots, X_p)\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-9",
    "href": "slides/01-02-welcome_to_sl.html#section-9",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Suppose there are \\(K\\) elements in \\(\\mathcal{C}\\), numbered \\(1, 2, \\dots, K\\)\n\\[p_k(x) = P(Y = k|X=x), k = 1, 2, \\dots, K\\] These are conditional class probabilities at \\(x\\)\n\n\nHow do you think we could calculate this?\n\n\n\n\nIn the plot, you could examine the mini-barplot at \\(x = 5\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-10",
    "href": "slides/01-02-welcome_to_sl.html#section-10",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Suppose there are \\(K\\) elements in \\(\\mathcal{C}\\), numbered \\(1, 2, \\dots, K\\)\n\\[p_k(x) = P(Y = k|X=x), k = 1, 2, \\dots, K\\] These are conditional class probabilities at \\(x\\)\n\nThe Bayes optimal classifier at \\(x\\) is\n\n\\[\\mathcal{C}(x) = j \\textrm{ if } p_j(x) = \\textrm{max}\\{p_1(x), p_2(x), \\dots, p_K(x)\\}\\]\n\n\nNotice that probability is a conditional probability\nIt is the probability that Y equals k given the observed preditor vector, \\(x\\)\nLet‚Äôs say we were using a Bayes Classifier for a two class problem, Y is 1 or 2. We would predict that the class is one if \\(P(Y=1|X=x_0)&gt;0.5\\) and 2 otherwise"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-11",
    "href": "slides/01-02-welcome_to_sl.html#section-11",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "What if this was our data and there were no points at exactly \\(x = 5\\)? Then how could we calculate this?\n\n\nNearest neighbor like before!\nThis does break down as the dimensions grow, but the impact of \\(\\mathcal{\\hat{C}}(x)\\) is less than on \\(\\hat{p}_k(x), k = 1,2,\\dots,K\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy-6",
    "href": "slides/01-02-welcome_to_sl.html#accuracy-6",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\nMisclassification error rate\n\n\\[Err_{\\texttt{test}} = \\frac{\\#correct predictions}{total predictions} = \\textrm{Ave}_{test}I[y_i\\neq \\mathcal{\\hat{C}}(x_i)]\\] &gt; * \\(I(\\cdot)\\) is an indicator function and will only be eitehr 0 or 1.\n\nThe Bayes Classifier using the true \\(p_k(x)\\) has the smallest error\nSome of the methods we (may) learn build structured models for \\(\\mathcal{C}(x)\\) (support vector machines, for example)\nSome build structured models for \\(p_k(x)\\) (logistic regression, for example)\n\n\n\nthe test error rate \\(\\textrm{Ave}_{i\\in\\texttt{test}}I[y_i\\neq \\mathcal{\\hat{C}}(x_i)]\\) is minimized on average by very simple classifier that assigns each observation to the most likely class, given its predictor values (that‚Äôs the Bayes classifier)\n\n\n\n\n\n\nüîó https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#recap",
    "href": "slides/04-2-lda-qda.html#recap",
    "title": "Chapter 4 Part 2",
    "section": "Recap",
    "text": "Recap\n\nWe had a logistic regression refresher\nNow‚Ä¶\n\nWhat if our response has more than two levels?\nWhat if logistic regression is a poor fit?"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#setup",
    "href": "slides/04-2-lda-qda.html#setup",
    "title": "Chapter 4 Part 2",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR)\nlibrary(Stat2Data)\n#install.packages(\"discrim\")"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#multinomial-logistic",
    "href": "slides/04-2-lda-qda.html#multinomial-logistic",
    "title": "Chapter 4 Part 2",
    "section": "Multinomial Logistic",
    "text": "Multinomial Logistic\n\nSo far we have discussed logistic regression with two classes.\nIt is easily generalized to more than two classes."
  },
  {
    "objectID": "slides/04-2-lda-qda.html#confounding",
    "href": "slides/04-2-lda-qda.html#confounding",
    "title": "Chapter 4 Part 2",
    "section": "Confounding",
    "text": "Confounding\nRecall our defaults data with variable default, student, and balance\n\n\nWhat is going on here?"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#confounding-1",
    "href": "slides/04-2-lda-qda.html#confounding-1",
    "title": "Chapter 4 Part 2",
    "section": "Confounding",
    "text": "Confounding\n\n\nStudents tend to have higher balances than non-students\nTheir marginal default rate is higher\nFor each level of balance, students default less\nTheir conditional default rate is lower"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#multiple-logistic-regression",
    "href": "slides/04-2-lda-qda.html#multiple-logistic-regression",
    "title": "Chapter 4 Part 2",
    "section": "Multiple logistic regression",
    "text": "Multiple logistic regression\n\\[\\log\\left(\\frac{p(X)}{1-p(X)}\\right)=\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p\\] \\[p(X) = \\frac{e^{\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p}}{1+e^{\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p}}\\]\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.8690452\n0.4922555\n-22.080088\n0.0000000\n\n\nbalance\n0.0057365\n0.0002319\n24.737563\n0.0000000\n\n\nincome\n0.0000030\n0.0000082\n0.369815\n0.7115203\n\n\nstudentYes\n-0.6467758\n0.2362525\n-2.737646\n0.0061881\n\n\n\n\n\n\n\n\n\nWhy is the coefficient for student negative now when it was positive before?"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#logistic-regression-for-more-than-two-classes",
    "href": "slides/04-2-lda-qda.html#logistic-regression-for-more-than-two-classes",
    "title": "Chapter 4 Part 2",
    "section": "Logistic regression for more than two classes",
    "text": "Logistic regression for more than two classes\n\\[P(Y=k|X) = \\frac{e ^{\\beta_{0k}+\\beta_{1k}X_1+\\dots+\\beta_{pk}X_p}}{\\sum_{l=1}^Ke^{\\beta_{0l}+\\beta_{1l}X_1+\\dots+\\beta_{pl}X_p}}\\]\n\nWe generalize this to situations with multiple classes\nHere we have a linear function for each of the \\(K\\) classes\nThis is known as multinomial logistic regression"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#lda-warmup",
    "href": "slides/04-2-lda-qda.html#lda-warmup",
    "title": "Chapter 4 Part 2",
    "section": "LDA Warmup",
    "text": "LDA Warmup\nTo give us a general overview, we are going to watch the StatQuest video on the topic: https://www.youtube.com/watch?v=azXCzI57Yfc"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#discriminant-analysis",
    "href": "slides/04-2-lda-qda.html#discriminant-analysis",
    "title": "Chapter 4 Part 2",
    "section": "Discriminant Analysis",
    "text": "Discriminant Analysis\n\nHere the approach is to model the distribution of X in each of the classes separately, and then use Bayes theorem to flip things around and obtain \\(P(Y|X)\\).\nWhen we use normal (Gaussian) distributions for each class, this leads to linear or quadratic discriminant analysis.\nHowever, this approach is quite general, and other distributions can be used as well. We will focus on normal distributions."
  },
  {
    "objectID": "slides/04-2-lda-qda.html#why-another-approach",
    "href": "slides/04-2-lda-qda.html#why-another-approach",
    "title": "Chapter 4 Part 2",
    "section": "Why Another Approach?",
    "text": "Why Another Approach?\n\nWhen the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.\nIf n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\nLinear discriminant analysis is popular when we have more than two response classes, because it also provides low-dimensional views of the data."
  },
  {
    "objectID": "slides/04-2-lda-qda.html#bayes-theorem-classification",
    "href": "slides/04-2-lda-qda.html#bayes-theorem-classification",
    "title": "Chapter 4 Part 2",
    "section": "Bayes Theorem (classification)",
    "text": "Bayes Theorem (classification)\nThomas Bayes was a famous mathematician whose name represents a big subfield of statistical and probabilistic modeling. Here we focus on a simple result, known as Bayes theorem:\n\\[P(Y=k|X=x) = \\frac{P(X=x|Y=k)\\cdot P(Y=k)}{P(X=x)}\\]"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#bayes-for-discriminant-analysis",
    "href": "slides/04-2-lda-qda.html#bayes-for-discriminant-analysis",
    "title": "Chapter 4 Part 2",
    "section": "Bayes for Discriminant Analysis",
    "text": "Bayes for Discriminant Analysis\n\\[P(Y=k|X=x) = \\frac{\\pi_kf_k(x)}{\\sum_{l=1}^K\\pi_lf_l(x)} \\text{, where}\\]\n\n\\(f_k(x)=P(X=x|Y=k)\\) is the density for \\(X\\) in class \\(k\\). Here we use normal‚Äôs but they could be other distributions (such as \\(\\chi^2\\))\n\\(\\pi_k = P(Y=k)\\) is the marginal or prior probability for class \\(k\\)."
  },
  {
    "objectID": "slides/04-2-lda-qda.html#classify-to-the-highest-density",
    "href": "slides/04-2-lda-qda.html#classify-to-the-highest-density",
    "title": "Chapter 4 Part 2",
    "section": "Classify to the highest density",
    "text": "Classify to the highest density\n\\[\\pi_1=.5, \\pi_2=.5\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe classify a new point according to which density is highest.\nWhen the priors are different, we take them into account as well, and compare \\(\\pi_kf_k(x)\\).\nOn the right, we favor the pink class - the decision boundary has shifted to the left."
  },
  {
    "objectID": "slides/04-2-lda-qda.html#lda-when-p1",
    "href": "slides/04-2-lda-qda.html#lda-when-p1",
    "title": "Chapter 4 Part 2",
    "section": "LDA (when \\(p=1\\))",
    "text": "LDA (when \\(p=1\\))\nThe Gaussian (normal) density has the form\n\\[f_k(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k}e^{-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma_k})^2}\\]\n\n\\(\\mu_k\\) is the mean, \\(\\sigma_k^2\\) the variance (in class \\(k\\))\nFor now, we assume \\(\\sigma_k=\\sigma\\) for all groups (we will need to check this with real data)"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#lda-when-p1-1",
    "href": "slides/04-2-lda-qda.html#lda-when-p1-1",
    "title": "Chapter 4 Part 2",
    "section": "LDA (when \\(p=1\\))",
    "text": "LDA (when \\(p=1\\))\nWe plug this \\(f_k(x)\\) into Bayes formula and after some simplifying we get:\n\\[p_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma_k})^2}}{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma_k})^2}}\\]"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#discriminant-function",
    "href": "slides/04-2-lda-qda.html#discriminant-function",
    "title": "Chapter 4 Part 2",
    "section": "Discriminant Function",
    "text": "Discriminant Function\nTo classify at the value X = x, we need to see which of the \\(p_k(x)\\) is largest. Taking logs, and discarding terms that do not depend on \\(k\\), we see that this is equivalent to assigning x to the class with the largest discriminant score:\n\\[\\delta_k(x) = x\\cdot \\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+log(\\pi_k)\\]\n\nImportantly, \\(\\delta_k(x)\\) is a linear function of \\(x\\).\nIf there are \\(K=2\\) classes and \\(\\pi_1=\\pi_2=.5\\), then the decision boundry is at\n\n\\[x=\\frac{\\mu_1+\\mu_2}{2}\\]"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#maximizing-delta_kx",
    "href": "slides/04-2-lda-qda.html#maximizing-delta_kx",
    "title": "Chapter 4 Part 2",
    "section": "Maximizing \\(\\delta_k(x)\\)",
    "text": "Maximizing \\(\\delta_k(x)\\)\n\nIn order to maximize this, we need estimates for all the parameters\n\n\nWhat should we estimate \\(\\hat{\\pi_k}\\), \\(\\mu_k\\), and \\(\\sigma^2\\) with?"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#maximizing-delta_kx-1",
    "href": "slides/04-2-lda-qda.html#maximizing-delta_kx-1",
    "title": "Chapter 4 Part 2",
    "section": "Maximizing \\(\\delta_k(x)\\)",
    "text": "Maximizing \\(\\delta_k(x)\\)\n\\[\\hat{\\pi}_k = \\frac{n_k}{n}\\]\n\\[\\hat{\\mu}_k = \\frac{1}{n_k}\\sum_{i:y_k=k}x_i\\]\n\\[\\hat{\\sigma}^2 = \\frac{1}{n-K}\\sum_{k=1}^K\\sum_{i:y_i=k}(x_i-\\hat{\\mu}_k)^2 = \\sum_{k=1}^K\\frac{n_k-1}{n-K}\\cdot \\hat{\\sigma}_k^2\\]\nWhere \\[\\hat{\\sigma}_k^2 = \\frac{1}{n_k-1}\\sum_{i:y_i=k}(x_i-\\hat{\\mu}_k)^2\\]"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#lda-in-r-example-p1",
    "href": "slides/04-2-lda-qda.html#lda-in-r-example-p1",
    "title": "Chapter 4 Part 2",
    "section": "LDA In R Example (\\(p=1\\))",
    "text": "LDA In R Example (\\(p=1\\))\n\ndata(\"BlueJays\") # Bring data into environment\nlibrary(Stat2Data)\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(discrim)\n\n\nCan we determine the sex of a blue jay by measuring the distance from the tip of the bill to the back of the head (Head)?"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#lda-bluejays-eda",
    "href": "slides/04-2-lda-qda.html#lda-bluejays-eda",
    "title": "Chapter 4 Part 2",
    "section": "LDA Bluejays EDA",
    "text": "LDA Bluejays EDA\n\ntt_split &lt;- initial_split(BlueJays,prop=.7)\n\nBJ_train &lt;- training(tt_split)\n\nBJ_train |&gt; ggplot(aes(x = Head,y=KnownSex)) +\n  geom_boxplot() +\n  theme_bw()"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#lda-bluejays-eda-1",
    "href": "slides/04-2-lda-qda.html#lda-bluejays-eda-1",
    "title": "Chapter 4 Part 2",
    "section": "LDA Bluejays EDA",
    "text": "LDA Bluejays EDA\n\nWhat assumptions should we check?\n\n\nNormality of our predictor\nConstant variance between groups."
  },
  {
    "objectID": "slides/04-2-lda-qda.html#lda-bluejays-eda---normality",
    "href": "slides/04-2-lda-qda.html#lda-bluejays-eda---normality",
    "title": "Chapter 4 Part 2",
    "section": "LDA Bluejays EDA - Normality",
    "text": "LDA Bluejays EDA - Normality\n\nlibrary(patchwork)\n\n(BJ_train |&gt; ggplot(aes(sample = Head)) + geom_qq())+\n  (BJ_train |&gt; ggplot(aes(x = Head)) + geom_histogram())"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#lda-bluejays-eda---variance",
    "href": "slides/04-2-lda-qda.html#lda-bluejays-eda---variance",
    "title": "Chapter 4 Part 2",
    "section": "LDA Bluejays EDA - Variance",
    "text": "LDA Bluejays EDA - Variance\n\nBJ_train |&gt; group_by(KnownSex)|&gt;\n  summarize(Head_sd = sd(Head))\n\n# A tibble: 2 √ó 2\n  KnownSex Head_sd\n  &lt;fct&gt;      &lt;dbl&gt;\n1 F          1.21 \n2 M          0.999\n\n\n\nVery similar standard deviations (and thus variances)"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#fit-lda",
    "href": "slides/04-2-lda-qda.html#fit-lda",
    "title": "Chapter 4 Part 2",
    "section": "Fit LDA",
    "text": "Fit LDA\n\nlda_spec &lt;- discrim_linear() |&gt;\n  set_mode(\"classification\")|&gt;\n  set_engine(\"MASS\")\n\nlda_fit&lt;-  lda_spec |&gt; \n  fit(KnownSex ~ Head,\n      data = BJ_train)"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#check-the-fit",
    "href": "slides/04-2-lda-qda.html#check-the-fit",
    "title": "Chapter 4 Part 2",
    "section": "Check The Fit",
    "text": "Check The Fit\n\nBJTest &lt;- testing(tt_split)\n\nlda_fit |&gt; \n  augment(new_data = BJTest) %&gt;%\n  conf_mat(truth = KnownSex, estimate = .pred_class) \n\n          Truth\nPrediction  F  M\n         F 14  2\n         M  9 12\n\n\n\nlda_fit|&gt; \n  augment(new_data = BJTest) |&gt; \n  accuracy(truth = KnownSex,estimate=.pred_class)\n\n# A tibble: 1 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.703"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#compare-to-logistic",
    "href": "slides/04-2-lda-qda.html#compare-to-logistic",
    "title": "Chapter 4 Part 2",
    "section": "Compare to Logistic",
    "text": "Compare to Logistic\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(KnownSex ~ Head,\n      data = BJ_train) |&gt;\n  augment(new_data = BJTest)|&gt;\n    accuracy(truth = KnownSex,estimate=.pred_class)\n\n# A tibble: 1 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.703"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#application-exercise",
    "href": "slides/04-2-lda-qda.html#application-exercise",
    "title": "Chapter 4 Part 2",
    "section": " Application Exercise",
    "text": "Application Exercise\nWe are going to use the penguins data from the palmerpeguins package.\n\nConduct basic EDA to see if penguin bill length is different by sex and if LDA is an appropriate model choice.\nUse LDA to predict penguin sex based on their bill length using training and testing data. Get the accuracy on the testing set.\nFit a logistic regression model and get it‚Äôs accuracy to see which did better."
  },
  {
    "objectID": "slides/04-2-lda-qda.html#lda-with-p1",
    "href": "slides/04-2-lda-qda.html#lda-with-p1",
    "title": "Chapter 4 Part 2",
    "section": "LDA with \\(p>1\\)",
    "text": "LDA with \\(p&gt;1\\)\n\nWhen we have 2 or more predictors, the distribution becomes multivariate.\nIf the covariance between predictors is 0 within each class of the response, LDA is still appropriate."
  },
  {
    "objectID": "slides/04-2-lda-qda.html#lda-with-p1-1",
    "href": "slides/04-2-lda-qda.html#lda-with-p1-1",
    "title": "Chapter 4 Part 2",
    "section": "LDA with \\(p>1\\)",
    "text": "LDA with \\(p&gt;1\\)\n\nFor example, with 2 normal predictors, their distributions in 3d would like like this:\n\n\n\nLuckily, the discriminate function remains linear\n\n\\[\\delta_k(x) = c_{k0} + c_{k1}x_1+...c_{kp}x_p\\]"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#example-p2k3",
    "href": "slides/04-2-lda-qda.html#example-p2k3",
    "title": "Chapter 4 Part 2",
    "section": "Example: \\(p=2,K=3\\)",
    "text": "Example: \\(p=2,K=3\\)\n\nThere is no limit on the number of levels of the categorical response for LDA\nSuppose \\(\\pi_1=\\pi_2\\pi_3=1/3\\)\n\n - The dashed lines are known as the Bayes decision boundaries"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#probabilities",
    "href": "slides/04-2-lda-qda.html#probabilities",
    "title": "Chapter 4 Part 2",
    "section": "Probabilities",
    "text": "Probabilities\n\nOnce the estimates for the \\(\\hat{\\delta}_k(x)\\) have been found, we can plug them in and get:\n\\[\\hat{P}(Y=k|X=x)=\\frac{e^{\\hat{\\delta}_k(x)}}{\\sum_{l=1}^Ke^{\\hat{\\delta}^l(x)}}\\]\nClassifying to the largest \\(\\hat{\\delta}_k(x)\\) amounts to classifying to the class for which \\(\\hat{P}(Y = k|X = x)\\) is largest.\nWhen \\(K = 2\\), we classify to class 2 if \\(\\hat{P}(Y = 2|X = x)\\geq 0.5\\), else to class 1."
  },
  {
    "objectID": "slides/04-2-lda-qda.html#example---credit-card-fraud",
    "href": "slides/04-2-lda-qda.html#example---credit-card-fraud",
    "title": "Chapter 4 Part 2",
    "section": "Example - Credit Card Fraud",
    "text": "Example - Credit Card Fraud\n\n\n          Truth\nPrediction   No  Yes\n       No  2889   83\n       Yes    2   26\n\n\n# A tibble: 1 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.972\n\n\n\nAccuracy of 97.7% on a testing set!\nWhat about the different types of errors?"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#errors",
    "href": "slides/04-2-lda-qda.html#errors",
    "title": "Chapter 4 Part 2",
    "section": "Errors",
    "text": "Errors\n\nOf the true yes, we made errors at the rate of 83/(83+26), 76%\nOf the true no, we made errors at the rate of 2/(2889+2), .069%"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#types-of-errors",
    "href": "slides/04-2-lda-qda.html#types-of-errors",
    "title": "Chapter 4 Part 2",
    "section": "Types of Errors",
    "text": "Types of Errors\n\nRecall:\n\nFalse positive rate: The fraction of negative examples that are classified as positive.\nFalse negative rate: The fraction of positive examples that are classified as negative.\n\nRemember the model gave probabilities, the final yes or no is decided by\n\\[\\hat{P}(Default=Yes|Balance,Student) \\geq .5\\]\n\nWe can adjust our error rates by changing that threshold"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#sensitivity-and-specificity",
    "href": "slides/04-2-lda-qda.html#sensitivity-and-specificity",
    "title": "Chapter 4 Part 2",
    "section": "Sensitivity and Specificity",
    "text": "Sensitivity and Specificity\n\n\n          Truth\nPrediction   No  Yes\n       No  2889   83\n       Yes    2   26\n\n\n\nThe sensitivity is the true positive rate.\n\nThe rate at which we correctly predict a person will default.\n26/(83+26) = .24 (24%)\n\nThe specificity is the true negative rate.\n\nThe rate at which we correctly predict a person will not default.\n2889/(2889+2) =.999 (99.9%)"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#varying-the-threshold",
    "href": "slides/04-2-lda-qda.html#varying-the-threshold",
    "title": "Chapter 4 Part 2",
    "section": "Varying the threshold",
    "text": "Varying the threshold\n\nIn order to determine the best threshold, we want to maximize the specificty and sensitivity.\nOR - maximize the sensitivty and minimize 1-specificity.\nIn order to to do this we use an \\(ROC\\) curve which stands for receiver operating characteristic curve."
  },
  {
    "objectID": "slides/04-2-lda-qda.html#roc-curve",
    "href": "slides/04-2-lda-qda.html#roc-curve",
    "title": "Chapter 4 Part 2",
    "section": "ROC Curve",
    "text": "ROC Curve\nWe want to maximize the area under this curve, called ROC AUC\n\nlda_roc&lt;- lda_fit_2 |&gt;\n  augment(new_data = testing(def_splits)) |&gt;\n  roc_curve(truth = default,.pred_No) \n\nhead(lda_roc)\n\n# A tibble: 6 √ó 3\n  .threshold specificity sensitivity\n       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1  -Inf          0                 1\n2     0.0819     0                 1\n3     0.129      0.00917           1\n4     0.151      0.0183            1\n5     0.154      0.0275            1\n6     0.201      0.0367            1"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#roc-curve-1",
    "href": "slides/04-2-lda-qda.html#roc-curve-1",
    "title": "Chapter 4 Part 2",
    "section": "ROC Curve",
    "text": "ROC Curve\n\nautoplot(lda_roc)"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#roc-auc",
    "href": "slides/04-2-lda-qda.html#roc-auc",
    "title": "Chapter 4 Part 2",
    "section": "ROC AUC",
    "text": "ROC AUC\n\nlda_fit_2 |&gt;\n  augment(new_data = testing(def_splits)) |&gt;\n  roc_auc(truth = default,.pred_No) \n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.957\n\n\nIf we want to tune our model better, we can optimize the ROC AUC by changing the threshold.\n\nI did a train/test approach here. What should I do different if I want to tune for threshold?"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#qda",
    "href": "slides/04-2-lda-qda.html#qda",
    "title": "Chapter 4 Part 2",
    "section": "QDA",
    "text": "QDA\n\nQDA arises when \\(p&gt;1\\) and the is a covariance structure between the predictors within the same level of the response.\nThis introduces a squared term into the maximization problem of the \\(\\delta_k(x)\\), thus the name."
  },
  {
    "objectID": "slides/04-2-lda-qda.html#qda-in-r",
    "href": "slides/04-2-lda-qda.html#qda-in-r",
    "title": "Chapter 4 Part 2",
    "section": "QDA In R",
    "text": "QDA In R\n\nqda_fit&lt;-discrim_quad() |&gt;\n  set_mode(\"classification\")|&gt;\n  set_engine(\"MASS\")|&gt; \n  fit(default ~ balance + student,\n      data = training(def_splits))\n\nqda_fit |&gt;\n  augment(new_data = testing(def_splits)) %&gt;%\n  conf_mat(truth = default, estimate = .pred_class) \n\n          Truth\nPrediction   No  Yes\n       No  2888   80\n       Yes    3   29\n\nqda_fit |&gt;\n  augment(new_data = testing(def_splits)) %&gt;%\n  roc_auc(truth = default,.pred_No) \n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.957"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#tuning-by-threhold",
    "href": "slides/04-2-lda-qda.html#tuning-by-threhold",
    "title": "Chapter 4 Part 2",
    "section": "Tuning By Threhold",
    "text": "Tuning By Threhold\nhttps://www.tidymodels.org/start/case-study/\n\n\n\n\nüîó https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#setup",
    "href": "slides/05-cv-tidymodels.html#setup",
    "title": "Chapter 5 and tidymodels",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(ISLR)\nlibrary(tidymodels)\nlibrary(gridExtra)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#cross-validation",
    "href": "slides/05-cv-tidymodels.html#cross-validation",
    "title": "Chapter 5 and tidymodels",
    "section": "Cross validation",
    "text": "Cross validation\nüí° Big idea\n\n\nWe have determined that it is sensible to use a test set to calculate metrics like prediction error"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#cross-validation-1",
    "href": "slides/05-cv-tidymodels.html#cross-validation-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Cross validation",
    "text": "Cross validation\nüí° Big idea\n\n\nWe have determined that it is sensible to use a test set to calculate metrics like prediction error\n\n\n\nWhy?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#cross-validation-2",
    "href": "slides/05-cv-tidymodels.html#cross-validation-2",
    "title": "Chapter 5 and tidymodels",
    "section": "Cross validation",
    "text": "Cross validation\nüí° Big idea\n\n\nWe have determined that it is sensible to use a test set to calculate metrics like prediction error\n\n\n\nHow could we do this?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#cross-validation-3",
    "href": "slides/05-cv-tidymodels.html#cross-validation-3",
    "title": "Chapter 5 and tidymodels",
    "section": "Cross validation",
    "text": "Cross validation\nüí° Big idea\n\n\nWe have determined that it is sensible to use a test set to calculate metrics like prediction error\nWhat if we don‚Äôt have a separate data set to test our model on?\nüéâ We can use resampling methods to estimate the test-set prediction error"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#training-error-versus-test-error",
    "href": "slides/05-cv-tidymodels.html#training-error-versus-test-error",
    "title": "Chapter 5 and tidymodels",
    "section": "Training error versus test error",
    "text": "Training error versus test error\n\nWhat is the difference? Which is typically larger?\n\n\nThe training error is calculated by using the same observations used to fit the statistical learning model\nThe test error is calculated by using a statistical learning method to predict the response of new observations\nThe training error rate typically underestimates the true prediction error rate"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#estimating-prediction-error",
    "href": "slides/05-cv-tidymodels.html#estimating-prediction-error",
    "title": "Chapter 5 and tidymodels",
    "section": "Estimating prediction error",
    "text": "Estimating prediction error\n\nBest case scenario: We have a large data set to test our model on\nThis is not always the case!\n\n\nüí° Let‚Äôs instead find a way to estimate the test error by holding out a subset of the training observations from the model fitting process, and then applying the statistical learning method to those held out observations"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set",
    "text": "Approach #1: Validation set\n\nRandomly divide the available set up samples into two parts: a training set and a validation set\nFit the model on the training set, calculate the prediction error on the validation set\n\n\n\nIf we have a quantitative predictor what metric would we use to calculate this test error?\n\n\nOften we use Mean Squared Error (MSE)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set-1",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set",
    "text": "Approach #1: Validation set\n\n\nRandomly divide the available set up samples into two parts: a training set and a validation set\nFit the model on the training set, calculate the prediction error on the validation set\n\n\n\nIf we have a qualitative predictor what metric would we use to calculate this test error?\n\n\nOften we use misclassification rate"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set-2",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set-2",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set",
    "text": "Approach #1: Validation set\n\n\n\\[\\Large\\color{orange}{MSE_{\\texttt{test-split}} = \\textrm{Ave}_{i\\in\\texttt{test-split}}[y_i-\\hat{f}(x_i)]^2}\\]\n\n\n\\[\\Large\\color{orange}{Err_{\\texttt{test-split}} = \\textrm{Ave}_{i\\in\\texttt{test-split}}I[y_i\\neq \\mathcal{\\hat{C}}(x_i)]}\\]"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set-3",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set-3",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set",
    "text": "Approach #1: Validation set\nAuto example:\n\n\nWe have 392 observations.\nTrying to predict mpg from horsepower.\nWe can split the data in half and use 196 to fit the model and 196 to test"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set-4",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set-4",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set",
    "text": "Approach #1: Validation set\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split}}}\\)\n\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split}}}\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split}}}\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split}}}\\)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set-5",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set-5",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set",
    "text": "Approach #1: Validation set\nAuto example:\n\n\nWe have 392 observations.\nTrying to predict mpg from horsepower.\nWe can split the data in half and use 196 to fit the model and 196 to test - what if we did this many times?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set-drawbacks",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set-drawbacks",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set (Drawbacks)",
    "text": "Approach #1: Validation set (Drawbacks)\n\nthe validation estimate of the test error can be highly variable, depending on which observations are included in the training set and which observations are included in the validation set\nIn the validation approach, only a subset of the observations (those that are included in the training set rather than in the validation set) are used to fit the model\nTherefore, the validation set error may tend to overestimate the test error for the model fit on the entire data set"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-2-k-fold-cross-validation",
    "href": "slides/05-cv-tidymodels.html#approach-2-k-fold-cross-validation",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #2: K-fold cross validation",
    "text": "Approach #2: K-fold cross validation\nüí° The idea is to do the following:\n\nRandomly divide the data into \\(K\\) equal-sized parts\nLeave out part \\(k\\), fit the model to the other \\(K - 1\\) parts (combined)\nObtain predictions for the left-out \\(k\\)th part\nDo this for each part \\(k = 1, 2,\\dots K\\), and then combine the result"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#k-fold-cross-validation",
    "href": "slides/05-cv-tidymodels.html#k-fold-cross-validation",
    "title": "Chapter 5 and tidymodels",
    "section": "K-fold cross validation",
    "text": "K-fold cross validation\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split-1}}}\\)\n\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split-2}}}\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split-3}}}\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split-4}}}\\)\nTake the mean of the \\(k\\) MSE values"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise",
    "href": "slides/05-cv-tidymodels.html#application-exercise",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\nCreate a new R project, then a new quarto file with cv in its name in that project. Answer the questions in that file.\nIf we use 10 folds:\n\n\nWhat percentage of the training data is used in each analysis for each fold?\nWhat percentage of the training data is used in the assessment for each fold?\n\n\n\ncountdown::countdown(2)\n\n\n‚àí+\n02:00"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#estimating-prediction-error-quantitative-outcome",
    "href": "slides/05-cv-tidymodels.html#estimating-prediction-error-quantitative-outcome",
    "title": "Chapter 5 and tidymodels",
    "section": "Estimating prediction error (quantitative outcome)",
    "text": "Estimating prediction error (quantitative outcome)\n\nSplit the data into K parts, where \\(C_1, C_2, \\dots, C_k\\) indicate the indices of observations in part \\(k\\)\n\\(CV_{(K)} = \\sum_{k=1}^K\\frac{n_k}{n}MSE_k\\)\n\\(MSE_k = \\sum_{i \\in C_k} (y_i - \\hat{y}_i)^2/n_k\\)\n\\(n_k\\) is the number of observations in group \\(k\\)\n\\(\\hat{y}_i\\) is the fit for observation \\(i\\) obtained from the data with the part \\(k\\) removed\nIf we set \\(K = n\\), we‚Äôd have \\(n-fold\\) cross validation which is the same as leave-one-out cross validation (LOOCV)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#estimating-prediction-error-quantitative-outcome-1",
    "href": "slides/05-cv-tidymodels.html#estimating-prediction-error-quantitative-outcome-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Estimating prediction error (quantitative outcome)",
    "text": "Estimating prediction error (quantitative outcome)\n\n\nSplit the data into K parts, where \\(C_1, C_2, \\dots, C_k\\) indicate the indices of observations in part \\(k\\)\n\\(CV_{(K)} = \\sum_{k=1}^K\\frac{n_k}{n}MSE_k\\)\n\\(MSE_k = \\sum_{i \\in C_k} (y_i - \\hat{y}_i)^2/n_k\\)\n\\(n_k\\) is the number of observations in group \\(k\\)\n\\(\\hat{y}_i\\) is the fit for observation \\(i\\) obtained from the data with the part \\(k\\) removed\nIf we set \\(K = n\\), we‚Äôd have \\(n-fold\\) cross validation which is the same as leave-one-out cross validation (LOOCV)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#leave-one-out-cross-validation",
    "href": "slides/05-cv-tidymodels.html#leave-one-out-cross-validation",
    "title": "Chapter 5 and tidymodels",
    "section": "Leave-one-out cross validation",
    "text": "Leave-one-out cross validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\dots\\)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#special-case",
    "href": "slides/05-cv-tidymodels.html#special-case",
    "title": "Chapter 5 and tidymodels",
    "section": "Special Case!",
    "text": "Special Case!\n\nWith linear regression, you can actually calculate the LOOCV error without having to iterate!\n\\(CV_{(n)} = \\frac{1}{n}\\sum_{i=1}^n\\left(\\frac{y_i-\\hat{y}_i}{1-h_i}\\right)^2\\)\n\\(\\hat{y}_i\\) is the \\(i\\)th fitted value from the linear model\n\\(h_i\\) is the diagonal of the ‚Äúhat‚Äù matrix (remember that! üéì)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#picking-k",
    "href": "slides/05-cv-tidymodels.html#picking-k",
    "title": "Chapter 5 and tidymodels",
    "section": "Picking \\(K\\)",
    "text": "Picking \\(K\\)\n\n\\(K\\) can vary from 2 (splitting the data in half each time) to \\(n\\) (LOOCV)\nLOOCV is sometimes useful but usually the estimates from each fold are very correlated, so their average can have a high variance\nA better choice tends to be \\(K=5\\) or \\(K=10\\)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#bias-variance-trade-off",
    "href": "slides/05-cv-tidymodels.html#bias-variance-trade-off",
    "title": "Chapter 5 and tidymodels",
    "section": "Bias variance trade-off",
    "text": "Bias variance trade-off\n\nSince each training set is only \\((K - 1)/K\\) as big as the original training set, the estimates of prediction error will typically be biased upward\nThis bias is minimized when \\(K = n\\) (LOOCV), but this estimate has a high variance\n\\(K =5\\) or \\(K=10\\) provides a nice compromise for the bias-variance trade-off"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-2-k-fold-cross-validation-1",
    "href": "slides/05-cv-tidymodels.html#approach-2-k-fold-cross-validation-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #2: K-fold Cross Validation",
    "text": "Approach #2: K-fold Cross Validation\nAuto example:\n\n\nWe have 392 observations.\nTrying to predict mpg from horsepower"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#estimating-prediction-error-qualitative-outcome",
    "href": "slides/05-cv-tidymodels.html#estimating-prediction-error-qualitative-outcome",
    "title": "Chapter 5 and tidymodels",
    "section": "Estimating prediction error (qualitative outcome)",
    "text": "Estimating prediction error (qualitative outcome)\n\nThe premise is the same as cross valiation for quantitative outcomes\nSplit the data into K parts, where \\(C_1, C_2, \\dots, C_k\\) indicate the indices of observations in part \\(k\\)\n\\(CV_K = \\sum_{k=1}^K\\frac{n_k}{n}Err_k\\)\n\\(Err_k = \\sum_{i\\in C_k}I(y_i\\neq\\hat{y}_i)/n_k\\) (misclassification rate)\n\\(n_k\\) is the number of observations in group \\(k\\)\n\\(\\hat{y}_i\\) is the fit for observation \\(i\\) obtained from the data with the part \\(k\\) removed"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#estimating-prediction-error-qualitative-outcome-1",
    "href": "slides/05-cv-tidymodels.html#estimating-prediction-error-qualitative-outcome-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Estimating prediction error (qualitative outcome)",
    "text": "Estimating prediction error (qualitative outcome)\n\n\nThe premise is the same as cross valiation for quantitative outcomes\nSplit the data into K parts, where \\(C_1, C_2, \\dots, C_k\\) indicate the indices of observations in part \\(k\\)\n\\(CV_K = \\sum_{k=1}^K\\frac{n_k}{n}Err_k\\)\n\\(Err_k = \\sum_{i\\in C_k}I(y_i\\neq\\hat{y}_i)/n_k\\) (misclassification rate)\n\\(n_k\\) is the number of observations in group \\(k\\)\n\\(\\hat{y}_i\\) is the fit for observation \\(i\\) obtained from the data with the part \\(k\\) removed"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise-1",
    "href": "slides/05-cv-tidymodels.html#application-exercise-1",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nCreate a new quarto file in your project and add tidymodels in the name.\nLoad the packages by running the top chunk of R code\n\n\n\nlibrary(tidymodels)\nlibrary(broom)\nlibrary(ISLR)\nlibrary(countdown)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#tidymodels-1",
    "href": "slides/05-cv-tidymodels.html#tidymodels-1",
    "title": "Chapter 5 and tidymodels",
    "section": "tidymodels",
    "text": "tidymodels\n\n\n\n\ntidymodels.org\n\ntidymodels is an opinionated collection of R packages designed for modeling and statistical analysis.\nAll packages share an underlying philosophy and a common grammar."
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#step-1-specify-the-model",
    "href": "slides/05-cv-tidymodels.html#step-1-specify-the-model",
    "title": "Chapter 5 and tidymodels",
    "section": "Step 1: Specify the model",
    "text": "Step 1: Specify the model\n\nPick the model\nSet the engine"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#specify-the-model",
    "href": "slides/05-cv-tidymodels.html#specify-the-model",
    "title": "Chapter 5 and tidymodels",
    "section": "Specify the model",
    "text": "Specify the model\n\nlinear_reg() |&gt;\n  set_engine(\"lm\")"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#specify-the-model-1",
    "href": "slides/05-cv-tidymodels.html#specify-the-model-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Specify the model",
    "text": "Specify the model\n\nlinear_reg() |&gt;\n  set_engine(\"glmnet\")"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#specify-the-model-2",
    "href": "slides/05-cv-tidymodels.html#specify-the-model-2",
    "title": "Chapter 5 and tidymodels",
    "section": "Specify the model",
    "text": "Specify the model\n\nlinear_reg() |&gt;\n  set_engine(\"spark\")"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#specify-the-model-3",
    "href": "slides/05-cv-tidymodels.html#specify-the-model-3",
    "title": "Chapter 5 and tidymodels",
    "section": "Specify the model",
    "text": "Specify the model\n\ndecision_tree() |&gt;\n  set_engine(\"rpart\")"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#specify-the-model-4",
    "href": "slides/05-cv-tidymodels.html#specify-the-model-4",
    "title": "Chapter 5 and tidymodels",
    "section": "Specify the model",
    "text": "Specify the model\n\n\nAll available models:\n\ntidymodels.org"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise-2",
    "href": "slides/05-cv-tidymodels.html#application-exercise-2",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nWrite a pipe that creates a model that uses lm() to fit a linear regression using tidymodels. Save it as lm_spec and look at the object. What does it return?\n\n\nHint: you‚Äôll need https://www.tidymodels.org\n\n\n\n‚àí+\n05:00"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#answer",
    "href": "slides/05-cv-tidymodels.html#answer",
    "title": "Chapter 5 and tidymodels",
    "section": "Answer",
    "text": "Answer\n\nlm_spec &lt;- \n  linear_reg() |&gt; # Pick linear regression\n  set_engine(engine = \"lm\") # set engine\nlm_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#fit-the-data",
    "href": "slides/05-cv-tidymodels.html#fit-the-data",
    "title": "Chapter 5 and tidymodels",
    "section": "Fit the data",
    "text": "Fit the data\n\n\nYou can train your model using the fit() function\n\n\nfit(lm_spec,\n    mpg ~ horsepower,\n    data = Auto)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = mpg ~ horsepower, data = data)\n\nCoefficients:\n(Intercept)   horsepower  \n    39.9359      -0.1578"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise-3",
    "href": "slides/05-cv-tidymodels.html#application-exercise-3",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nFit the model:\n\n\nlibrary(ISLR)\nlm_fit &lt;- fit(lm_spec,\n              mpg ~ horsepower,\n              data = Auto)\nlm_fit\n\nDoes this give the same results as\n\nlm(mpg ~ horsepower, data = Auto)\n\n\n\n\n‚àí+\n03:00"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#answer-1",
    "href": "slides/05-cv-tidymodels.html#answer-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Answer",
    "text": "Answer\n\nlm_fit &lt;- fit(lm_spec,\n              mpg ~ horsepower,\n              data = Auto)\nlm_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = mpg ~ horsepower, data = data)\n\nCoefficients:\n(Intercept)   horsepower  \n    39.9359      -0.1578  \n\nlm(mpg ~ horsepower, data = Auto)\n\n\nCall:\nlm(formula = mpg ~ horsepower, data = Auto)\n\nCoefficients:\n(Intercept)   horsepower  \n    39.9359      -0.1578"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#get-predictions",
    "href": "slides/05-cv-tidymodels.html#get-predictions",
    "title": "Chapter 5 and tidymodels",
    "section": "Get predictions",
    "text": "Get predictions\n\nlm_fit |&gt;\n  predict(new_data = Auto)\n\n\nUses the predict() function\n‚ÄºÔ∏è new_data has an underscore\nüòÑ This automagically creates a data frame"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#get-predictions-1",
    "href": "slides/05-cv-tidymodels.html#get-predictions-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Get predictions",
    "text": "Get predictions\n\nlm_fit |&gt;\n  predict(new_data = Auto) |&gt;\n  bind_cols(Auto)\n\n# A tibble: 392 √ó 10\n   .pred   mpg cylinders displacement horsepower weight acceleration  year origin name    \n * &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;   \n 1 19.4     18         8          307        130   3504         12      70      1 chevrol‚Ä¶\n 2 13.9     15         8          350        165   3693         11.5    70      1 buick s‚Ä¶\n 3 16.3     18         8          318        150   3436         11      70      1 plymout‚Ä¶\n 4 16.3     16         8          304        150   3433         12      70      1 amc reb‚Ä¶\n 5 17.8     17         8          302        140   3449         10.5    70      1 ford to‚Ä¶\n 6  8.68    15         8          429        198   4341         10      70      1 ford ga‚Ä¶\n 7  5.21    14         8          454        220   4354          9      70      1 chevrol‚Ä¶\n 8  6.00    14         8          440        215   4312          8.5    70      1 plymout‚Ä¶\n 9  4.42    14         8          455        225   4425         10      70      1 pontiac‚Ä¶\n10  9.95    15         8          390        190   3850          8.5    70      1 amc amb‚Ä¶\n# ‚Ñπ 382 more rows\n\n\n\n\nWhat does bind_cols do?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#get-predictions-2",
    "href": "slides/05-cv-tidymodels.html#get-predictions-2",
    "title": "Chapter 5 and tidymodels",
    "section": "Get predictions",
    "text": "Get predictions\n\nlm_fit |&gt;\n  predict(new_data = Auto) |&gt;\n  bind_cols(Auto)\n\n# A tibble: 392 √ó 10\n   .pred   mpg cylinders displacement horsepower weight acceleration  year origin name    \n * &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;   \n 1 19.4     18         8          307        130   3504         12      70      1 chevrol‚Ä¶\n 2 13.9     15         8          350        165   3693         11.5    70      1 buick s‚Ä¶\n 3 16.3     18         8          318        150   3436         11      70      1 plymout‚Ä¶\n 4 16.3     16         8          304        150   3433         12      70      1 amc reb‚Ä¶\n 5 17.8     17         8          302        140   3449         10.5    70      1 ford to‚Ä¶\n 6  8.68    15         8          429        198   4341         10      70      1 ford ga‚Ä¶\n 7  5.21    14         8          454        220   4354          9      70      1 chevrol‚Ä¶\n 8  6.00    14         8          440        215   4312          8.5    70      1 plymout‚Ä¶\n 9  4.42    14         8          455        225   4425         10      70      1 pontiac‚Ä¶\n10  9.95    15         8          390        190   3850          8.5    70      1 amc amb‚Ä¶\n# ‚Ñπ 382 more rows\n\n\n\nWhich column has the predicted values?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise-4",
    "href": "slides/05-cv-tidymodels.html#application-exercise-4",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\n\n‚àí+\n03:00\n\n\n\n\n\nEdit the code below to add the original data to the predicted data.\n\n\n\nmpg_pred &lt;- lm_fit |&gt; \n  predict(new_data = Auto) |&gt; \n  ---"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#get-predictions-3",
    "href": "slides/05-cv-tidymodels.html#get-predictions-3",
    "title": "Chapter 5 and tidymodels",
    "section": "Get predictions",
    "text": "Get predictions\n\nmpg_pred &lt;- lm_fit |&gt;\n  predict(new_data = Auto) |&gt;\n  bind_cols(Auto)\n\nmpg_pred\n\n# A tibble: 392 √ó 10\n   .pred   mpg cylinders displacement horsepower weight acceleration  year origin name    \n * &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;   \n 1 19.4     18         8          307        130   3504         12      70      1 chevrol‚Ä¶\n 2 13.9     15         8          350        165   3693         11.5    70      1 buick s‚Ä¶\n 3 16.3     18         8          318        150   3436         11      70      1 plymout‚Ä¶\n 4 16.3     16         8          304        150   3433         12      70      1 amc reb‚Ä¶\n 5 17.8     17         8          302        140   3449         10.5    70      1 ford to‚Ä¶\n 6  8.68    15         8          429        198   4341         10      70      1 ford ga‚Ä¶\n 7  5.21    14         8          454        220   4354          9      70      1 chevrol‚Ä¶\n 8  6.00    14         8          440        215   4312          8.5    70      1 plymout‚Ä¶\n 9  4.42    14         8          455        225   4425         10      70      1 pontiac‚Ä¶\n10  9.95    15         8          390        190   3850          8.5    70      1 amc amb‚Ä¶\n# ‚Ñπ 382 more rows"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#calculate-the-error",
    "href": "slides/05-cv-tidymodels.html#calculate-the-error",
    "title": "Chapter 5 and tidymodels",
    "section": "Calculate the error",
    "text": "Calculate the error\n\n\nRoot mean square error\n\n\n\nmpg_pred |&gt;\n  rmse(truth = mpg, estimate = .pred)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        4.89\n\n\n\n\nWhat is this estimate? (training error? testing error?)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#validation-set-approach",
    "href": "slides/05-cv-tidymodels.html#validation-set-approach",
    "title": "Chapter 5 and tidymodels",
    "section": "Validation set approach",
    "text": "Validation set approach\n\nAuto_split &lt;- initial_split(Auto, prop = 0.5)\nAuto_split\n\n&lt;Training/Testing/Total&gt;\n&lt;196/196/392&gt;\n\n\n\n\nHow many observations are in the training set?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#validation-set-approach-1",
    "href": "slides/05-cv-tidymodels.html#validation-set-approach-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Validation set approach",
    "text": "Validation set approach\n\nAuto_split &lt;- initial_split(Auto, prop = 0.5)\nAuto_split\n\n&lt;Training/Testing/Total&gt;\n&lt;196/196/392&gt;\n\n\n\nHow many observations are in the test set?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#validation-set-approach-2",
    "href": "slides/05-cv-tidymodels.html#validation-set-approach-2",
    "title": "Chapter 5 and tidymodels",
    "section": "Validation set approach",
    "text": "Validation set approach\n\nAuto_split &lt;- initial_split(Auto, prop = 0.5)\nAuto_split\n\n&lt;Training/Testing/Total&gt;\n&lt;196/196/392&gt;\n\n\n\nHow many observations are there in total?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#validation-set-approach-3",
    "href": "slides/05-cv-tidymodels.html#validation-set-approach-3",
    "title": "Chapter 5 and tidymodels",
    "section": "Validation set approach",
    "text": "Validation set approach\n\nAuto_split &lt;- initial_split(Auto, prop = 0.5)\nAuto_split\n\n&lt;Training/Testing/Total&gt;\n&lt;196/196/392&gt;\n\n\n\n\nExtract the training and testing data\n\n\n\ntraining(Auto_split)\ntesting(Auto_split)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#validation-set-approach-4",
    "href": "slides/05-cv-tidymodels.html#validation-set-approach-4",
    "title": "Chapter 5 and tidymodels",
    "section": "Validation set approach",
    "text": "Validation set approach\n\nAuto_train &lt;- training(Auto_split)\n\n\nAuto_train\n\n\n\n# A tibble: 196 √ó 9\n     mpg cylinders displacement horsepower weight acceleration  year origin name          \n   &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;         \n 1  37.7         4           89         62   2050         17.3    81      3 toyota tercel \n 2  27           4           97         60   1834         19      71      2 volkswagen mo‚Ä¶\n 3  22           6          232        112   2835         14.7    82      1 ford granada l\n 4  16           6          250        100   3781         17      74      1 chevrolet che‚Ä¶\n 5  25           4           90         71   2223         16.5    75      2 volkswagen da‚Ä¶\n 6  18           6          232        100   2945         16      73      1 amc hornet    \n 7  38.1         4           89         60   1968         18.8    80      3 toyota coroll‚Ä¶\n 8  23           4           97         54   2254         23.5    72      2 volkswagen ty‚Ä¶\n 9  15           8          302        130   4295         14.9    77      1 mercury couga‚Ä¶\n10  34           4          108         70   2245         16.9    82      3 toyota corolla\n# ‚Ñπ 186 more rows"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise-5",
    "href": "slides/05-cv-tidymodels.html#application-exercise-5",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nCopy the code below, fill in the blanks to fit a model on the training data then calculate the test RMSE.\n\n\nset.seed(100)\nAuto_split  &lt;- ________\nAuto_train  &lt;- ________\nAuto_test   &lt;- ________\nlm_fit      &lt;- fit(lm_spec, \n                   mpg ~ horsepower, \n                   data = ________)\nmpg_pred  &lt;- ________ |&gt; \n  predict(new_data = ________) |&gt; \n  bind_cols(________)\nrmse(________, truth = ________, estimate = ________)\n\n\n\n\n‚àí+\n06:00"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#a-faster-way",
    "href": "slides/05-cv-tidymodels.html#a-faster-way",
    "title": "Chapter 5 and tidymodels",
    "section": "A faster way!",
    "text": "A faster way!\n\nYou can use last_fit() and specify the split\nThis will automatically train the data on the train data from the split\nInstead of specifying which metric to calculate (with rmse as before) you can just use collect_metrics() and it will automatically calculate the metrics on the test data from the split"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#a-faster-way-1",
    "href": "slides/05-cv-tidymodels.html#a-faster-way-1",
    "title": "Chapter 5 and tidymodels",
    "section": "A faster way!",
    "text": "A faster way!\n\nset.seed(100)\n\nAuto_split &lt;- initial_split(Auto, prop = 0.5)\nlm_fit &lt;- last_fit(lm_spec,\n                   mpg ~ horsepower,\n                   split = Auto_split) \n\nlm_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 √ó 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       4.96  Preprocessor1_Model1\n2 rsq     standard       0.613 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#a-faster-way-2",
    "href": "slides/05-cv-tidymodels.html#a-faster-way-2",
    "title": "Chapter 5 and tidymodels",
    "section": "A faster way!",
    "text": "A faster way!\n\nset.seed(100)\n\nAuto_split &lt;- initial_split(Auto, prop = 0.5)\nlm_fit &lt;- last_fit(lm_spec,\n                   mpg ~ horsepower,\n                   split = Auto_split) \n\nlm_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 √ó 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       4.96  Preprocessor1_Model1\n2 rsq     standard       0.613 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#what-about-cross-validation",
    "href": "slides/05-cv-tidymodels.html#what-about-cross-validation",
    "title": "Chapter 5 and tidymodels",
    "section": "What about cross validation?",
    "text": "What about cross validation?\n\nAuto_cv &lt;- vfold_cv(Auto, v = 5)\nAuto_cv\n\n#  5-fold cross-validation \n# A tibble: 5 √ó 2\n  splits           id   \n  &lt;list&gt;           &lt;chr&gt;\n1 &lt;split [313/79]&gt; Fold1\n2 &lt;split [313/79]&gt; Fold2\n3 &lt;split [314/78]&gt; Fold3\n4 &lt;split [314/78]&gt; Fold4\n5 &lt;split [314/78]&gt; Fold5"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#what-about-cross-validation-1",
    "href": "slides/05-cv-tidymodels.html#what-about-cross-validation-1",
    "title": "Chapter 5 and tidymodels",
    "section": "What about cross validation?",
    "text": "What about cross validation?\n\nInstead of fit we will use fit_resamples\n\n\n\nfit_resamples(lm_spec, \n              mpg ~ horsepower,\n              resamples = Auto_cv)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#what-about-cross-validation-2",
    "href": "slides/05-cv-tidymodels.html#what-about-cross-validation-2",
    "title": "Chapter 5 and tidymodels",
    "section": "What about cross validation?",
    "text": "What about cross validation?\n\nHow do we get the metrics out? With collect_metrics() again!\n\n\n\nresults &lt;- fit_resamples(lm_spec,\n                         mpg ~ horsepower,\n                         resamples = Auto_cv)\n\nresults |&gt;\n  collect_metrics()\n\n# A tibble: 2 √ó 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4.88      5  0.385  Preprocessor1_Model1\n2 rsq     standard   0.616     5  0.0220 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise-6",
    "href": "slides/05-cv-tidymodels.html#application-exercise-6",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\n\n\n‚àí+\n05:00\n\n\n\n\nEdit the code below to get the 5-fold cross validation error rate for the following model:\n\n\\(mpg = \\beta_0 + \\beta_1 horsepower + \\beta_2 horsepower^2+ \\epsilon\\)\n\nAuto_cv &lt;- vfold_cv(Auto, v = 5)\n\nresults &lt;- fit_resamples(lm_spec,\n                         ----,\n                         resamples = ---)\n\nresults |&gt;\n  collect_metrics()\n\n\nWhat do you think rsq is?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#answer-2",
    "href": "slides/05-cv-tidymodels.html#answer-2",
    "title": "Chapter 5 and tidymodels",
    "section": "Answer",
    "text": "Answer\n\nAuto_cv &lt;- vfold_cv(Auto, v = 5)\n\nresults &lt;- fit_resamples(lm_spec,\n                         mpg ~ horsepower + I(horsepower^2),\n                         resamples = Auto_cv)\n\nresults |&gt;\n  collect_metrics()\n\n# A tibble: 2 √ó 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4.38      5  0.110  Preprocessor1_Model1\n2 rsq     standard   0.688     5  0.0177 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise-7",
    "href": "slides/05-cv-tidymodels.html#application-exercise-7",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nFit 3 models on the data using 5 fold cross validation:\n\n\\(mpg = \\beta_0 + \\beta_1 horsepower + \\epsilon\\)\n\\(mpg = \\beta_0 + \\beta_1 horsepower + \\beta_2 horsepower^2+ \\epsilon\\)\n\\(mpg = \\beta_0 + \\beta_1 horsepower + \\beta_2 horsepower^2+ \\beta_3 horsepower^3 +\\epsilon\\)\n\nCollect the metrics from each model, saving the results as results_1, results_2, results_3\nWhich model is ‚Äúbest‚Äù?\n\n\n\n\n\n‚àí+\n08:00\n\n\n\n\n\n\n\nüîó https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "slides/04-3-km-nb.html#knn-smaller",
    "href": "slides/04-3-km-nb.html#knn-smaller",
    "title": "Chapter 4 Part 3",
    "section": "KNN {smaller}",
    "text": "KNN {smaller}\n\nGiven positive integer \\(K\\), and a test observation \\(x_0\\), KNN identifies the \\(K\\) points in the training data that are closest to \\(x_0\\), represented by \\(\\mathcal{N}_0\\).\nKNN then estimates the conditional probabilities for each class \\(j\\) as the fraction of the points in \\(\\mathcal{N}_0\\) whose response values equal \\(j\\):\n\\[P(Y=j|X=x_0)=\\frac{1}{K}\\sum_{i\\in \\mathcal{N}}I(y_i=j)\\]\nLastly, KNN classifies \\(x_0\\) into the class with the largest probability"
  },
  {
    "objectID": "slides/04-3-km-nb.html#knn-smaller-1",
    "href": "slides/04-3-km-nb.html#knn-smaller-1",
    "title": "Chapter 4 Part 3",
    "section": "KNN {smaller}",
    "text": "KNN {smaller}\n\nGiven positive integer \\(K\\), and a test observation \\(x_0\\), KNN identifies the \\(K\\) points in the training data that are closest to \\(x_0\\), represented by \\(\\mathcal{N}_0\\).\nKNN then estimates the conditional probabilities for each class \\(j\\) as the fraction of the points in \\(\\mathcal{N}_0\\) whose response values equal \\(j\\):\n\n\\[P(Y=j|X=x_0)=\\frac{1}{K}\\sum_{i\\in \\mathcal{N}}I(y_i=j)\\]\n\nLastly, KNN classifies \\(x_0\\) into the class with the largest probability"
  },
  {
    "objectID": "slides/04-3-km-nb.html#knn-example-smaller",
    "href": "slides/04-3-km-nb.html#knn-example-smaller",
    "title": "Chapter 4 Part 3",
    "section": "KNN Example {smaller}",
    "text": "KNN Example {smaller}\nThe KNN approach, using \\(K = 3\\), is illustrated in a situation with six blue observations and six orange observations.\n\n\nLeft: A test observation, \\(x_0\\), at which a predicted class label is desired is shown as a black cross.\n\nThe three closest points to the test observation are identified, and it is predicted that the test observation belongs to the most commonly-occurring class, in this case blue.\n\nRight: The KNN decision boundary for this example is shown in black. The blue grid indicates the region in which a test observation will be assigned to the blue class, and the orange grid indicates the regio in which it will be assigned to the orange class."
  },
  {
    "objectID": "slides/04-3-km-nb.html#knn-example-knn-vs-bayes",
    "href": "slides/04-3-km-nb.html#knn-example-knn-vs-bayes",
    "title": "Chapter 4 Part 3",
    "section": "KNN Example KNN vs Bayes",
    "text": "KNN Example KNN vs Bayes\n\n\n\nThe black curve indicates the KNN decision boundary, using \\(K = 10\\). The Bayes decision boundary is shown as a purple dashed line.\nThe KNN and Bayes decision boundaries are very similar."
  },
  {
    "objectID": "slides/04-3-km-nb.html#knn-low-and-high-flexibility",
    "href": "slides/04-3-km-nb.html#knn-low-and-high-flexibility",
    "title": "Chapter 4 Part 3",
    "section": "KNN Low and High Flexibility",
    "text": "KNN Low and High Flexibility\n\nComparison of KNN decision boundaries (solid black curves) obtained using \\(K = 1\\) and \\(K = 100\\).\n\nThe \\(K = 1\\) decision boundary is overly flexible and the \\(K = 100\\) boundary is not sufficiently flexible.\n\nThe Bayes decision boundary is shown as a purple dashed line."
  },
  {
    "objectID": "slides/04-3-km-nb.html#statquest",
    "href": "slides/04-3-km-nb.html#statquest",
    "title": "Chapter 4 Part 3",
    "section": "Statquest",
    "text": "Statquest\nhttps://www.youtube.com/watch?v=HVXime0nQeI&t=53s"
  },
  {
    "objectID": "slides/04-3-km-nb.html#naive-bayes-1",
    "href": "slides/04-3-km-nb.html#naive-bayes-1",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nIn LDA and QDA we assumed the predictors were Normal\n\nThis allow us to find the density functions \\(f_k(x)\\)‚Äôs by optimizing a linear or quadratic function \\(\\delta_k(x)\\).\nWith LDA, we assume the predictors have the same covariance structure between classes\nWith QDA, \\(X_j\\)s can have any covariance structure\nWith both, the predictors must be quantitative"
  },
  {
    "objectID": "slides/04-3-km-nb.html#naive-bayes-new-assumptions",
    "href": "slides/04-3-km-nb.html#naive-bayes-new-assumptions",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes New Assumptions",
    "text": "Naive Bayes New Assumptions\n\nThese two independence assumptions allows us to write, for \\(k=1,2,...,K\\),\n\\[f_k(x) = f_{k1}(x)f_{k2}(x)\\cdot \\cdot \\cdot f_{kp}\\]\n\nwhere \\(f_{kj}\\) is the density function for the \\(j\\)th predictor among observations in the \\(k\\)th class."
  },
  {
    "objectID": "slides/04-3-km-nb.html#naive-bayes-probability-function",
    "href": "slides/04-3-km-nb.html#naive-bayes-probability-function",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes Probability Function",
    "text": "Naive Bayes Probability Function\nMaking these assumption, we now have:\n\\[P(Y=k|X=x) = \\frac{\\pi_k\\cdot f_{k1}(x_1)\\cdot \\cdot \\cdot f_{kp}(x_p)}{\\sum_{l=1}^K\\pi_l\\cdot f_{l1}(x_1)\\cdot \\cdot \\cdot f_{lp}(x_p)}\\] for \\(k=1,2,...,K\\)"
  },
  {
    "objectID": "slides/04-3-km-nb.html#naive-bayes---why",
    "href": "slides/04-3-km-nb.html#naive-bayes---why",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes - Why?",
    "text": "Naive Bayes - Why?\n\nEach \\(f\\) is one dimensional!\nIf \\(X_j\\) is quantitative can still assume each \\(X_j|Y=k\\) is univariate normal\nIf \\(X_j\\) is quantitative, then another option is to use a non-parametric estimate for \\(f_{kj}\\)\n\nMake a histogram for the observations of the \\(j\\)th predictor within each class.\nThen we can estimate \\(f_{kj}(x_j)\\) as the fraction of the training observations in the \\(k\\)th class that belong to the same histogram bin as \\(x_j\\) .\nWe can use a kernel density estimator, which is kernel density estimator (essentially a smoothed version of a histogram)"
  },
  {
    "objectID": "slides/04-3-km-nb.html#naive-bayes---why-1",
    "href": "slides/04-3-km-nb.html#naive-bayes---why-1",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes - Why?",
    "text": "Naive Bayes - Why?\n\nIf \\(X_j\\) is qualitative, then we can simply count the proportion of training observations for the \\(j\\)th predictor corresponding to each class.\n\nSuppose that \\(X_j \\in \\{1, 2, 3\\}\\), and we have 100 observations in the \\(k\\)th class.\nSuppose that the \\(j\\)th predictor takes on values of 1,2, and 3 in 32, 55, and 13 of those observations, respectively. Then we can estimate \\(f_{kj}\\) as\n\n\\[\\hat{f}_{kj}(x_j) = \\begin{cases}\n.32 \\quad \\text{if }x_j = 1\\\\\n.55 \\quad \\text{if }x_j = 2\\\\\n.13 \\quad \\text{if }x_j = 3\\\\\n\\end{cases}\\]"
  },
  {
    "objectID": "slides/04-3-km-nb.html#naive-bayes---why-2",
    "href": "slides/04-3-km-nb.html#naive-bayes---why-2",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes - Why?",
    "text": "Naive Bayes - Why?\nhttps://www.youtube.com/watch?v=O2L2Uv9pdDA\n\n\n\n\nüîó https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "slides/04-3-km-nb.html#naive-bayes---statsquest",
    "href": "slides/04-3-km-nb.html#naive-bayes---statsquest",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes - Statsquest",
    "text": "Naive Bayes - Statsquest\nhttps://www.youtube.com/watch?v=O2L2Uv9pdDA"
  },
  {
    "objectID": "slides/04-3-km-nb.html#setup",
    "href": "slides/04-3-km-nb.html#setup",
    "title": "Chapter 4 Part 3",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR)\nlibrary(Stat2Data)\nlibrary(discrim)\n#install.packages(\"kknn\")\n#install.packages(\"klaR\")"
  },
  {
    "objectID": "slides/04-3-km-nb.html#kmeans-classification-tidymodels",
    "href": "slides/04-3-km-nb.html#kmeans-classification-tidymodels",
    "title": "Chapter 4 Part 3",
    "section": "KMeans Classification Tidymodels",
    "text": "KMeans Classification Tidymodels\n\nset.seed(14546)\ndef_splits &lt;- initial_split(Default,.7)\n\ndefault_training &lt;- training(def_splits)\n\nknn_fit_1 &lt;- nearest_neighbor() |&gt;\n  set_mode(\"classification\")|&gt;\n  set_engine(\"kknn\")|&gt; \n    fit(default ~ balance + student,\n      data = default_training)"
  },
  {
    "objectID": "slides/04-3-km-nb.html#kmeans-classification-tidymodels-1",
    "href": "slides/04-3-km-nb.html#kmeans-classification-tidymodels-1",
    "title": "Chapter 4 Part 3",
    "section": "KMeans Classification Tidymodels",
    "text": "KMeans Classification Tidymodels\n\ndefault_testing &lt;- testing(def_splits)\n\nknn_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  conf_mat(truth = default, estimate = .pred_class) \n\n          Truth\nPrediction   No  Yes\n       No  2865   69\n       Yes   26   40\n\nknn_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  accuracy(truth = default,estimate=.pred_class)\n\n# A tibble: 1 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.968"
  },
  {
    "objectID": "slides/04-3-km-nb.html#kmeans-classification-tidymodels-2",
    "href": "slides/04-3-km-nb.html#kmeans-classification-tidymodels-2",
    "title": "Chapter 4 Part 3",
    "section": "KMeans Classification Tidymodels",
    "text": "KMeans Classification Tidymodels\n\nknn_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  roc_auc(truth = default,.pred_No)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.844\n\nknn_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  roc_curve(truth = default,.pred_No) |&gt;\n  autoplot()"
  },
  {
    "objectID": "slides/04-3-km-nb.html#section",
    "href": "slides/04-3-km-nb.html#section",
    "title": "Chapter 4 Part 3",
    "section": "",
    "text": "üîó https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "slides/04-3-km-nb.html#kmeans-roc-curve",
    "href": "slides/04-3-km-nb.html#kmeans-roc-curve",
    "title": "Chapter 4 Part 3",
    "section": "KMeans ROC Curve",
    "text": "KMeans ROC Curve\n\nknn_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  roc_curve(truth = default,.pred_No) |&gt;\n  autoplot()"
  },
  {
    "objectID": "slides/04-3-km-nb.html#kmeans-roc-auc",
    "href": "slides/04-3-km-nb.html#kmeans-roc-auc",
    "title": "Chapter 4 Part 3",
    "section": "KMeans ROC AUC",
    "text": "KMeans ROC AUC\n\nknn_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  roc_auc(truth = default,.pred_No)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.844"
  },
  {
    "objectID": "slides/04-3-km-nb.html#naive-bayes-2",
    "href": "slides/04-3-km-nb.html#naive-bayes-2",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nWith Naive Bayes we drop the normality assumption but introduce a must stronger assumption\n\nWithin a class \\(k\\),\nThe predictors are independent"
  },
  {
    "objectID": "slides/04-2-lda-qda.html",
    "href": "slides/04-2-lda-qda.html",
    "title": "Chapter 4 Part 2",
    "section": "",
    "text": "We had a logistic regression refresher\nNow‚Ä¶\n\nWhat if our response has more than two levels?\nWhat if logistic regression is a poor fit?"
  },
  {
    "objectID": "slides/04-3-km-nb.html#naive-bayes-tidymodels",
    "href": "slides/04-3-km-nb.html#naive-bayes-tidymodels",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes Tidymodels",
    "text": "Naive Bayes Tidymodels\n\nnb_fit_1 &lt;- naive_Bayes() |&gt;\n  set_mode(\"classification\")|&gt;\n  set_engine(\"klaR\")|&gt; \n    fit(default ~ balance + student,\n      data = default_training)"
  },
  {
    "objectID": "slides/04-3-km-nb.html#naive-bayes-confusion-matrix",
    "href": "slides/04-3-km-nb.html#naive-bayes-confusion-matrix",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes Confusion Matrix",
    "text": "Naive Bayes Confusion Matrix\n\nnb_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  conf_mat(truth = default, estimate = .pred_class) \n\n          Truth\nPrediction   No  Yes\n       No  2880   76\n       Yes   11   33"
  },
  {
    "objectID": "slides/04-3-km-nb.html#naive-bayes-accuracy",
    "href": "slides/04-3-km-nb.html#naive-bayes-accuracy",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes Accuracy",
    "text": "Naive Bayes Accuracy\n\nnb_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  accuracy(truth = default,estimate=.pred_class)\n\n# A tibble: 1 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.971"
  },
  {
    "objectID": "slides/04-3-km-nb.html#naive-bayes-roc-curve",
    "href": "slides/04-3-km-nb.html#naive-bayes-roc-curve",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes ROC Curve",
    "text": "Naive Bayes ROC Curve\n\nnb_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  roc_curve(truth = default,.pred_No) |&gt;\n  autoplot()"
  },
  {
    "objectID": "slides/04-3-km-nb.html#naive-bayes-roc-auc",
    "href": "slides/04-3-km-nb.html#naive-bayes-roc-auc",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes ROC AUC",
    "text": "Naive Bayes ROC AUC\n\nnb_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  roc_auc(truth = default,.pred_No)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.952"
  },
  {
    "objectID": "slides/04-3-km-nb.html#knn",
    "href": "slides/04-3-km-nb.html#knn",
    "title": "Chapter 4 Part 3",
    "section": "KNN",
    "text": "KNN\n\nIn theory we would always like to predict qualitative responses using the Bayes classifier.\nFor real data, we do not know the conditional distribution of Y given X, and so computing the Bayes classifier is impossible.\nMany approaches attempt to estimate the conditional distribution of Y given X, and then classify a given observation to the class with highest estimated probability. One such method is the K-nearest neighbors (KNN) classifier."
  },
  {
    "objectID": "slides/04-3-km-nb.html#knn-1",
    "href": "slides/04-3-km-nb.html#knn-1",
    "title": "Chapter 4 Part 3",
    "section": "KNN",
    "text": "KNN\n\nGiven positive integer \\(K\\), and a test observation \\(x_0\\), KNN identifies the \\(K\\) points in the training data that are closest to \\(x_0\\), represented by \\(\\mathcal{N}_0\\).\nKNN then estimates the conditional probabilities for each class \\(j\\) as the fraction of the points in \\(\\mathcal{N}_0\\) whose response values equal \\(j\\):\n\\[P(Y=j|X=x_0)=\\frac{1}{K}\\sum_{i\\in \\mathcal{N}}I(y_i=j)\\]\nLastly, KNN classifies \\(x_0\\) into the class with the largest probability"
  },
  {
    "objectID": "slides/04-3-km-nb.html#knn-example",
    "href": "slides/04-3-km-nb.html#knn-example",
    "title": "Chapter 4 Part 3",
    "section": "KNN Example",
    "text": "KNN Example\nThe KNN approach, using \\(K = 3\\), is illustrated in a situation with six blue observations and six orange observations.\n\n\n\n\nA test observation, \\(x_0\\), at which a predicted class label is desired is shown as a black cross.\nThe three closest points to the test observation are identified, and it is predicted that the test observation belongs to the most commonly-occurring class, in this case blue."
  },
  {
    "objectID": "slides/04-3-km-nb.html#knn-example-1",
    "href": "slides/04-3-km-nb.html#knn-example-1",
    "title": "Chapter 4 Part 3",
    "section": "KNN Example",
    "text": "KNN Example\nThe KNN approach, using \\(K = 3\\), is illustrated in a situation with six blue observations and six orange observations.\n\n\n\n\nThe KNN decision boundary for this example is shown in black. The blue grid indicates the region in which a test observation will be assigned to the blue class, and the orange grid indicates the region in which it will be assigned to the orange class."
  },
  {
    "objectID": "slides/04-3-km-nb.html#comparing-lda-qda-and-normal-naive-bayes",
    "href": "slides/04-3-km-nb.html#comparing-lda-qda-and-normal-naive-bayes",
    "title": "Chapter 4 Part 3",
    "section": "Comparing LDA, QDA, and Normal Naive Bayes",
    "text": "Comparing LDA, QDA, and Normal Naive Bayes\nhttps://towardsdatascience.com/differences-of-lda-qda-and-gaussian-naive-bayes-classifiers-eaa4d1e999f6\nNaive Bayes - Given Y, the predictors X are conditionally independent.\nLDA - LDA assumes that the covariance matrix across classes is the same.\nQDA - QDA does not assume constant covariance matrix across classes.\n\n\n\n\nüîó https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "slides/04-3-km-nb.html#naive-bayes-statquest",
    "href": "slides/04-3-km-nb.html#naive-bayes-statquest",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes Statquest",
    "text": "Naive Bayes Statquest\nhttps://www.youtube.com/watch?v=O2L2Uv9pdDA&t=1s"
  },
  {
    "objectID": "slides/04-3-km-nb.html",
    "href": "slides/04-3-km-nb.html",
    "title": "Chapter 4 Part 3",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR)\nlibrary(Stat2Data)\nlibrary(discrim)\n#install.packages(\"kknn\")\n#install.packages(\"klaR\")"
  },
  {
    "objectID": "labs/03-classification.html",
    "href": "labs/03-classification.html",
    "title": "Lab 03",
    "section": "",
    "text": "Go to our RStudio and create a new R project inside your class folder.\n\n\nCreate a .qmd file for your lab, make sure the author is your name, and Render the document."
  },
  {
    "objectID": "labs/03-classification.html#yaml",
    "href": "labs/03-classification.html#yaml",
    "title": "Lab 03",
    "section": "",
    "text": "Create a .qmd file for your lab, make sure the author is your name, and Render the document."
  },
  {
    "objectID": "labs/03-classification.html#the-stock-market-data",
    "href": "labs/03-classification.html#the-stock-market-data",
    "title": "Lab 03",
    "section": "The Stock Market Data",
    "text": "The Stock Market Data\nWe will be examining the Smarket data set for this lab. It contains a number of numeric variables plus a variable called Direction which has the two labels \"Up\" and \"Down\". Before we do on to modeling, let us take a look at the correlation between the variables.\nTo look at the correlation, we will use the corrr package. The correlate() function will calculate the correlation matrix between all the variables that it is being fed. We will therefore remove Direction as it is not numeric. Then we pass that to rplot() to quickly visualize the correlation matrix. I have also changed the colours argument to better see what is going on.\n\nlibrary(corrr)\ncor_Smarket &lt;- Smarket %&gt;%\n  select(-Direction) %&gt;%\n  correlate()\n\nrplot(cor_Smarket, colours = c(\"indianred2\", \"black\", \"skyblue1\"))\n\n\n\n\n\n\n\n\nAnd we see that these variables are more or less uncorrelated with each other. The other pair is Year and Volume that is a little correlated.\nIf you want to create heatmap styled correlation chart you can also create it manually.\n\nlibrary(paletteer)\ncor_Smarket %&gt;%\n  stretch() %&gt;%\n  ggplot(aes(x, y, fill = r)) +\n  geom_tile() +\n  geom_text(aes(label = as.character(fashion(r)))) +\n  scale_fill_paletteer_c(\"scico::roma\", limits = c(-1, 1), direction = -1)\n\n\n\n\n\n\n\n\nIf we plot Year against Volume we see that there is an upwards trend in Volume with time.\n\nggplot(Smarket, aes(Year, Volume)) +\n  geom_jitter(height = 0)"
  },
  {
    "objectID": "labs/03-classification.html#exercises",
    "href": "labs/03-classification.html#exercises",
    "title": "Lab 03",
    "section": "Exercises",
    "text": "Exercises\n\nData Split\n\nSplit the data into training and testing sets with 70% in the training set.\n\n\n\nAnswer the following in questions 2 through 6:\n\nFit the model using Lag1-5 and volume to predict direction.\nWhat are the model assumptions?\nCheck all of the assumptions.\nAssess your model on the testing set with accuracy, the ROC curve, and the ROC AUC and give the confusion matrix.\n\n\nLogistic Regression HINT: For linearity in logistic, you will need to create bins for each predictor, say 5 bins, then check the empirical probabilities for each bin.\nLDA\nQDA\nNaive Bayes\nK Nearest Neighbors\n\n\n\nComparing multiple models\nWe have fit a lot of different models in this lab. And we were able to calculate the performance metrics one by one, but it is not ideal if we want to compare the different models. Below is an example of how you can more conveniently calculate performance metrics for multiple models at the same time.\nStart of by creating a named list of the fitted models you want to evaluate. I have made sure only to include models that were fitted on the same parameters to make it easier to compare them.\n\nmodels &lt;- list(\"logistic regression\" = lr_fit3,\n               \"LDA\" = lda_fit,\n               \"QDA\" = qda_fit,\n               \"KNN\" = knn_fit)\n\nNext use imap_dfr() from the purrr package to apply augment() to each of the models using the testing data set. .id = \"model\" creates a column named \"model\" that is added to the resulting tibble using the names of models.\n\npreds &lt;- imap_dfr(models, augment, \n                  new_data = Smarket_test, .id = \"model\")\n\npreds %&gt;%\n  dplyr::select(model, Direction, .pred_class, .pred_Down, .pred_Up)\n\nWe have seen how to use accuracy() a lot of times by now, but it is not the only metric to use for classification, and yardstick provides many more. You can combine multiple different metrics together with metric_set()\n\nmulti_metric &lt;- metric_set(accuracy, sensitivity, specificity)\n\nand then the resulting function can be applied to calculate multiple metrics at the same time. All of the yardstick works with grouped tibbles so by calling group_by(model) we can calculate the metrics for each of the models in one go.\n\npreds %&gt;%\n  group_by(model) %&gt;%\n  multi_metric(truth = Direction, estimate = .pred_class)\n\nThe same technique can be used to create ROC curves.\n\npreds %&gt;%\n  group_by(model) %&gt;%\n  roc_curve(Direction, .pred_Down) %&gt;%\n  autoplot()\n\n\nUse the above code to fit all models as once and them compare them according to accuracy, ROC, and ROC AUC."
  },
  {
    "objectID": "slides/04-3-km-nb_0.html",
    "href": "slides/04-3-km-nb_0.html",
    "title": "Chapter 4 Part 3",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR)\nlibrary(Stat2Data)\nlibrary(discrim)\n#install.packages(\"kknn\")\n#install.packages(\"klaR\")"
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#setup",
    "href": "slides/04-3-km-nb_0.html#setup",
    "title": "Chapter 4 Part 3",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR)\nlibrary(Stat2Data)\nlibrary(discrim)\n#install.packages(\"kknn\")\n#install.packages(\"klaR\")"
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#knn",
    "href": "slides/04-3-km-nb_0.html#knn",
    "title": "Chapter 4 Part 3",
    "section": "KNN",
    "text": "KNN\n\nIn theory we would always like to predict qualitative responses using the Bayes classifier.\nFor real data, we do not know the conditional distribution of Y given X, and so computing the Bayes classifier is impossible.\nMany approaches attempt to estimate the conditional distribution of Y given X, and then classify a given observation to the class with highest estimated probability. One such method is the K-nearest neighbors (KNN) classifier."
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#knn-1",
    "href": "slides/04-3-km-nb_0.html#knn-1",
    "title": "Chapter 4 Part 3",
    "section": "KNN",
    "text": "KNN\n\nGiven positive integer \\(K\\), and a test observation \\(x_0\\), KNN identifies the \\(K\\) points in the training data that are closest to \\(x_0\\), represented by \\(\\mathcal{N}_0\\).\nKNN then estimates the conditional probabilities for each class \\(j\\) as the fraction of the points in \\(\\mathcal{N}_0\\) whose response values equal \\(j\\):\n\\[P(Y=j|X=x_0)=\\frac{1}{K}\\sum_{i\\in \\mathcal{N}}I(y_i=j)\\]\nLastly, KNN classifies \\(x_0\\) into the class with the largest probability"
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#knn-example",
    "href": "slides/04-3-km-nb_0.html#knn-example",
    "title": "Chapter 4 Part 3",
    "section": "KNN Example",
    "text": "KNN Example\nThe KNN approach, using \\(K = 3\\), is illustrated in a situation with six blue observations and six orange observations.\n\n\n\n\nA test observation, \\(x_0\\), at which a predicted class label is desired is shown as a black cross.\nThe three closest points to the test observation are identified, and it is predicted that the test observation belongs to the most commonly-occurring class, in this case blue."
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#knn-example-1",
    "href": "slides/04-3-km-nb_0.html#knn-example-1",
    "title": "Chapter 4 Part 3",
    "section": "KNN Example",
    "text": "KNN Example\nThe KNN approach, using \\(K = 3\\), is illustrated in a situation with six blue observations and six orange observations.\n\n\n\n\nThe KNN decision boundary for this example is shown in black. The blue grid indicates the region in which a test observation will be assigned to the blue class, and the orange grid indicates the region in which it will be assigned to the orange class."
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#knn-example-knn-vs-bayes",
    "href": "slides/04-3-km-nb_0.html#knn-example-knn-vs-bayes",
    "title": "Chapter 4 Part 3",
    "section": "KNN Example KNN vs Bayes",
    "text": "KNN Example KNN vs Bayes\n\n\n\nThe black curve indicates the KNN decision boundary, using \\(K = 10\\). The Bayes decision boundary is shown as a purple dashed line.\nThe KNN and Bayes decision boundaries are very similar."
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#knn-low-and-high-flexibility",
    "href": "slides/04-3-km-nb_0.html#knn-low-and-high-flexibility",
    "title": "Chapter 4 Part 3",
    "section": "KNN Low and High Flexibility",
    "text": "KNN Low and High Flexibility\n\nComparison of KNN decision boundaries (solid black curves) obtained using \\(K = 1\\) and \\(K = 100\\).\n\nThe \\(K = 1\\) decision boundary is overly flexible and the \\(K = 100\\) boundary is not sufficiently flexible.\n\nThe Bayes decision boundary is shown as a purple dashed line."
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#statquest",
    "href": "slides/04-3-km-nb_0.html#statquest",
    "title": "Chapter 4 Part 3",
    "section": "Statquest",
    "text": "Statquest\nhttps://www.youtube.com/watch?v=HVXime0nQeI&t=53s"
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#kmeans-classification-tidymodels",
    "href": "slides/04-3-km-nb_0.html#kmeans-classification-tidymodels",
    "title": "Chapter 4 Part 3",
    "section": "KMeans Classification Tidymodels",
    "text": "KMeans Classification Tidymodels\n\nset.seed(14546)\ndef_splits &lt;- initial_split(Default,.7)\n\ndefault_training &lt;- training(def_splits)\n\nknn_fit_1 &lt;- nearest_neighbor() |&gt;\n  set_mode(\"classification\")|&gt;\n  set_engine(\"kknn\")|&gt; \n    fit(default ~ balance + student,\n      data = default_training)"
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#kmeans-classification-tidymodels-1",
    "href": "slides/04-3-km-nb_0.html#kmeans-classification-tidymodels-1",
    "title": "Chapter 4 Part 3",
    "section": "KMeans Classification Tidymodels",
    "text": "KMeans Classification Tidymodels\n\ndefault_testing &lt;- testing(def_splits)\n\nknn_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  conf_mat(truth = default, estimate = .pred_class) \n\n          Truth\nPrediction   No  Yes\n       No  2865   69\n       Yes   26   40\n\nknn_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  accuracy(truth = default,estimate=.pred_class)\n\n# A tibble: 1 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.968"
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#kmeans-roc-curve",
    "href": "slides/04-3-km-nb_0.html#kmeans-roc-curve",
    "title": "Chapter 4 Part 3",
    "section": "KMeans ROC Curve",
    "text": "KMeans ROC Curve\n\nknn_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  roc_curve(truth = default,.pred_No) |&gt;\n  autoplot()"
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#kmeans-roc-auc",
    "href": "slides/04-3-km-nb_0.html#kmeans-roc-auc",
    "title": "Chapter 4 Part 3",
    "section": "KMeans ROC AUC",
    "text": "KMeans ROC AUC\n\nknn_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  roc_auc(truth = default,.pred_No)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.844"
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#naive-bayes-1",
    "href": "slides/04-3-km-nb_0.html#naive-bayes-1",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nIn LDA and QDA we assumed the predictors were Normal\n\nThis allow us to find the density functions \\(f_k(x)\\)‚Äôs by optimizing a linear or quadratic function \\(\\delta_k(x)\\).\nWith LDA, we assume the predictors have the same covariance structure between classes\nWith QDA, \\(X_j\\)s can have any covariance structure\nWith both, the predictors must be quantitative"
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#naive-bayes-2",
    "href": "slides/04-3-km-nb_0.html#naive-bayes-2",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nWith Naive Bayes we drop the normality assumption but introduce a must stronger assumption\n\nWithin a class \\(k\\),\nThe predictors are independent"
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#naive-bayes-new-assumptions",
    "href": "slides/04-3-km-nb_0.html#naive-bayes-new-assumptions",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes New Assumptions",
    "text": "Naive Bayes New Assumptions\n\nThese two independence assumptions allows us to write, for \\(k=1,2,...,K\\),\n\\[f_k(x) = f_{k1}(x)f_{k2}(x)\\cdot \\cdot \\cdot f_{kp}\\]\n\nwhere \\(f_{kj}\\) is the density function for the \\(j\\)th predictor among observations in the \\(k\\)th class."
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#naive-bayes-probability-function",
    "href": "slides/04-3-km-nb_0.html#naive-bayes-probability-function",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes Probability Function",
    "text": "Naive Bayes Probability Function\nMaking these assumption, we now have:\n\\[P(Y=k|X=x) = \\frac{\\pi_k\\cdot f_{k1}(x_1)\\cdot \\cdot \\cdot f_{kp}(x_p)}{\\sum_{l=1}^K\\pi_l\\cdot f_{l1}(x_1)\\cdot \\cdot \\cdot f_{lp}(x_p)}\\] for \\(k=1,2,...,K\\)"
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#naive-bayes---why",
    "href": "slides/04-3-km-nb_0.html#naive-bayes---why",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes - Why?",
    "text": "Naive Bayes - Why?\n\nEach \\(f\\) is one dimensional!\nIf \\(X_j\\) is quantitative can still assume each \\(X_j|Y=k\\) is univariate normal\nIf \\(X_j\\) is quantitative, then another option is to use a non-parametric estimate for \\(f_{kj}\\)\n\nMake a histogram for the observations of the \\(j\\)th predictor within each class.\nThen we can estimate \\(f_{kj}(x_j)\\) as the fraction of the training observations in the \\(k\\)th class that belong to the same histogram bin as \\(x_j\\) .\nWe can use a kernel density estimator, which is kernel density estimator (essentially a smoothed version of a histogram)"
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#naive-bayes---why-1",
    "href": "slides/04-3-km-nb_0.html#naive-bayes---why-1",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes - Why?",
    "text": "Naive Bayes - Why?\n\nIf \\(X_j\\) is qualitative, then we can simply count the proportion of training observations for the \\(j\\)th predictor corresponding to each class.\n\nSuppose that \\(X_j \\in \\{1, 2, 3\\}\\), and we have 100 observations in the \\(k\\)th class.\nSuppose that the \\(j\\)th predictor takes on values of 1,2, and 3 in 32, 55, and 13 of those observations, respectively. Then we can estimate \\(f_{kj}\\) as\n\n\\[\\hat{f}_{kj}(x_j) = \\begin{cases}\n.32 \\quad \\text{if }x_j = 1\\\\\n.55 \\quad \\text{if }x_j = 2\\\\\n.13 \\quad \\text{if }x_j = 3\\\\\n\\end{cases}\\]"
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#naive-bayes---statsquest",
    "href": "slides/04-3-km-nb_0.html#naive-bayes---statsquest",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes - Statsquest",
    "text": "Naive Bayes - Statsquest\nhttps://www.youtube.com/watch?v=O2L2Uv9pdDA"
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#naive-bayes-tidymodels",
    "href": "slides/04-3-km-nb_0.html#naive-bayes-tidymodels",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes Tidymodels",
    "text": "Naive Bayes Tidymodels\n\nnb_fit_1 &lt;- naive_Bayes() |&gt;\n  set_mode(\"classification\")|&gt;\n  set_engine(\"klaR\")|&gt; \n    fit(default ~ balance + student,\n      data = default_training)"
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#naive-bayes-confusion-matrix",
    "href": "slides/04-3-km-nb_0.html#naive-bayes-confusion-matrix",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes Confusion Matrix",
    "text": "Naive Bayes Confusion Matrix\n\nnb_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  conf_mat(truth = default, estimate = .pred_class) \n\n          Truth\nPrediction   No  Yes\n       No  2880   76\n       Yes   11   33"
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#naive-bayes-accuracy",
    "href": "slides/04-3-km-nb_0.html#naive-bayes-accuracy",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes Accuracy",
    "text": "Naive Bayes Accuracy\n\nnb_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  accuracy(truth = default,estimate=.pred_class)\n\n# A tibble: 1 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.971"
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#naive-bayes-roc-curve",
    "href": "slides/04-3-km-nb_0.html#naive-bayes-roc-curve",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes ROC Curve",
    "text": "Naive Bayes ROC Curve\n\nnb_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  roc_curve(truth = default,.pred_No) |&gt;\n  autoplot()"
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#naive-bayes-roc-auc",
    "href": "slides/04-3-km-nb_0.html#naive-bayes-roc-auc",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes ROC AUC",
    "text": "Naive Bayes ROC AUC\n\nnb_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  roc_auc(truth = default,.pred_No)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.952"
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#naive-bayes-statquest",
    "href": "slides/04-3-km-nb_0.html#naive-bayes-statquest",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes Statquest",
    "text": "Naive Bayes Statquest\nhttps://www.youtube.com/watch?v=O2L2Uv9pdDA&t=1s"
  },
  {
    "objectID": "slides/04-3-km-nb_0.html#comparing-lda-qda-and-normal-naive-bayes",
    "href": "slides/04-3-km-nb_0.html#comparing-lda-qda-and-normal-naive-bayes",
    "title": "Chapter 4 Part 3",
    "section": "Comparing LDA, QDA, and Normal Naive Bayes",
    "text": "Comparing LDA, QDA, and Normal Naive Bayes\nhttps://towardsdatascience.com/differences-of-lda-qda-and-gaussian-naive-bayes-classifiers-eaa4d1e999f6\nNaive Bayes - Given Y, the predictors X are conditionally independent.\nLDA - LDA assumes that the covariance matrix across classes is the same.\nQDA - QDA does not assume constant covariance matrix across classes."
  },
  {
    "objectID": "slides/04-3-km-nb_o.html",
    "href": "slides/04-3-km-nb_o.html",
    "title": "Chapter 4 Part 3",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR)\nlibrary(Stat2Data)\nlibrary(discrim)\n#install.packages(\"kknn\")\n#install.packages(\"klaR\")"
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#setup",
    "href": "slides/04-3-km-nb_o.html#setup",
    "title": "Chapter 4 Part 3",
    "section": "",
    "text": "library(tidyverse)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR)\nlibrary(Stat2Data)\nlibrary(discrim)\n#install.packages(\"kknn\")\n#install.packages(\"klaR\")"
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#knn",
    "href": "slides/04-3-km-nb_o.html#knn",
    "title": "Chapter 4 Part 3",
    "section": "KNN",
    "text": "KNN\n\nIn theory we would always like to predict qualitative responses using the Bayes classifier.\nFor real data, we do not know the conditional distribution of Y given X, and so computing the Bayes classifier is impossible.\nMany approaches attempt to estimate the conditional distribution of Y given X, and then classify a given observation to the class with highest estimated probability. One such method is the K-nearest neighbors (KNN) classifier."
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#knn-1",
    "href": "slides/04-3-km-nb_o.html#knn-1",
    "title": "Chapter 4 Part 3",
    "section": "KNN",
    "text": "KNN\n\nGiven positive integer \\(K\\), and a test observation \\(x_0\\), KNN identifies the \\(K\\) points in the training data that are closest to \\(x_0\\), represented by \\(\\mathcal{N}_0\\).\nKNN then estimates the conditional probabilities for each class \\(j\\) as the fraction of the points in \\(\\mathcal{N}_0\\) whose response values equal \\(j\\):\n\\[P(Y=j|X=x_0)=\\frac{1}{K}\\sum_{i\\in \\mathcal{N}}I(y_i=j)\\]\nLastly, KNN classifies \\(x_0\\) into the class with the largest probability"
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#knn-example",
    "href": "slides/04-3-km-nb_o.html#knn-example",
    "title": "Chapter 4 Part 3",
    "section": "KNN Example",
    "text": "KNN Example\nThe KNN approach, using \\(K = 3\\), is illustrated in a situation with six blue observations and six orange observations.\n\n\n\n\nA test observation, \\(x_0\\), at which a predicted class label is desired is shown as a black cross.\nThe three closest points to the test observation are identified, and it is predicted that the test observation belongs to the most commonly-occurring class, in this case blue."
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#knn-example-1",
    "href": "slides/04-3-km-nb_o.html#knn-example-1",
    "title": "Chapter 4 Part 3",
    "section": "KNN Example",
    "text": "KNN Example\nThe KNN approach, using \\(K = 3\\), is illustrated in a situation with six blue observations and six orange observations.\n\n\n\n\nThe KNN decision boundary for this example is shown in black. The blue grid indicates the region in which a test observation will be assigned to the blue class, and the orange grid indicates the region in which it will be assigned to the orange class."
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#knn-example-knn-vs-bayes",
    "href": "slides/04-3-km-nb_o.html#knn-example-knn-vs-bayes",
    "title": "Chapter 4 Part 3",
    "section": "KNN Example KNN vs Bayes",
    "text": "KNN Example KNN vs Bayes\n\n\n\nThe black curve indicates the KNN decision boundary, using \\(K = 10\\). The Bayes decision boundary is shown as a purple dashed line.\nThe KNN and Bayes decision boundaries are very similar."
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#knn-low-and-high-flexibility",
    "href": "slides/04-3-km-nb_o.html#knn-low-and-high-flexibility",
    "title": "Chapter 4 Part 3",
    "section": "KNN Low and High Flexibility",
    "text": "KNN Low and High Flexibility\n\nComparison of KNN decision boundaries (solid black curves) obtained using \\(K = 1\\) and \\(K = 100\\).\n\nThe \\(K = 1\\) decision boundary is overly flexible and the \\(K = 100\\) boundary is not sufficiently flexible.\n\nThe Bayes decision boundary is shown as a purple dashed line."
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#statquest",
    "href": "slides/04-3-km-nb_o.html#statquest",
    "title": "Chapter 4 Part 3",
    "section": "Statquest",
    "text": "Statquest\nhttps://www.youtube.com/watch?v=HVXime0nQeI&t=53s"
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#kmeans-classification-tidymodels",
    "href": "slides/04-3-km-nb_o.html#kmeans-classification-tidymodels",
    "title": "Chapter 4 Part 3",
    "section": "KMeans Classification Tidymodels",
    "text": "KMeans Classification Tidymodels\n\nset.seed(14546)\ndef_splits &lt;- initial_split(Default,.7)\n\ndefault_training &lt;- training(def_splits)\n\nknn_fit_1 &lt;- nearest_neighbor() |&gt;\n  set_mode(\"classification\")|&gt;\n  set_engine(\"kknn\")|&gt; \n    fit(default ~ balance + student,\n      data = default_training)\n\nWarning: package 'kknn' was built under R version 4.3.3"
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#kmeans-classification-tidymodels-1",
    "href": "slides/04-3-km-nb_o.html#kmeans-classification-tidymodels-1",
    "title": "Chapter 4 Part 3",
    "section": "KMeans Classification Tidymodels",
    "text": "KMeans Classification Tidymodels\n\ndefault_testing &lt;- testing(def_splits)\n\nknn_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  conf_mat(truth = default, estimate = .pred_class) \n\n          Truth\nPrediction   No  Yes\n       No  2865   69\n       Yes   26   40\n\nknn_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  accuracy(truth = default,estimate=.pred_class)\n\n# A tibble: 1 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.968"
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#kmeans-roc-curve",
    "href": "slides/04-3-km-nb_o.html#kmeans-roc-curve",
    "title": "Chapter 4 Part 3",
    "section": "KMeans ROC Curve",
    "text": "KMeans ROC Curve\n\nknn_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  roc_curve(truth = default,.pred_No) |&gt;\n  autoplot()"
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#kmeans-roc-auc",
    "href": "slides/04-3-km-nb_o.html#kmeans-roc-auc",
    "title": "Chapter 4 Part 3",
    "section": "KMeans ROC AUC",
    "text": "KMeans ROC AUC\n\nknn_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  roc_auc(truth = default,.pred_No)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.844"
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#naive-bayes-1",
    "href": "slides/04-3-km-nb_o.html#naive-bayes-1",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nIn LDA and QDA we assumed the predictors were Normal\n\nThis allow us to find the density functions \\(f_k(x)\\)‚Äôs by optimizing a linear or quadratic function \\(\\delta_k(x)\\).\nWith LDA, we assume the predictors have the same covariance structure between classes\nWith QDA, \\(X_j\\)s can have any covariance structure\nWith both, the predictors must be quantitative"
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#naive-bayes-2",
    "href": "slides/04-3-km-nb_o.html#naive-bayes-2",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nWith Naive Bayes we drop the normality assumption but introduce a must stronger assumption\n\nWithin a class \\(k\\),\nThe predictors are independent"
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#naive-bayes-new-assumptions",
    "href": "slides/04-3-km-nb_o.html#naive-bayes-new-assumptions",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes New Assumptions",
    "text": "Naive Bayes New Assumptions\n\nThese two independence assumptions allows us to write, for \\(k=1,2,...,K\\),\n\\[f_k(x) = f_{k1}(x)f_{k2}(x)\\cdot \\cdot \\cdot f_{kp}\\]\n\nwhere \\(f_{kj}\\) is the density function for the \\(j\\)th predictor among observations in the \\(k\\)th class."
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#naive-bayes-probability-function",
    "href": "slides/04-3-km-nb_o.html#naive-bayes-probability-function",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes Probability Function",
    "text": "Naive Bayes Probability Function\nMaking these assumption, we now have:\n\\[P(Y=k|X=x) = \\frac{\\pi_k\\cdot f_{k1}(x_1)\\cdot \\cdot \\cdot f_{kp}(x_p)}{\\sum_{l=1}^K\\pi_l\\cdot f_{l1}(x_1)\\cdot \\cdot \\cdot f_{lp}(x_p)}\\] for \\(k=1,2,...,K\\)"
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#naive-bayes---why",
    "href": "slides/04-3-km-nb_o.html#naive-bayes---why",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes - Why?",
    "text": "Naive Bayes - Why?\n\nEach \\(f\\) is one dimensional!\nIf \\(X_j\\) is quantitative can still assume each \\(X_j|Y=k\\) is univariate normal\nIf \\(X_j\\) is quantitative, then another option is to use a non-parametric estimate for \\(f_{kj}\\)\n\nMake a histogram for the observations of the \\(j\\)th predictor within each class.\nThen we can estimate \\(f_{kj}(x_j)\\) as the fraction of the training observations in the \\(k\\)th class that belong to the same histogram bin as \\(x_j\\) .\nWe can use a kernel density estimator, which is kernel density estimator (essentially a smoothed version of a histogram)"
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#naive-bayes---why-1",
    "href": "slides/04-3-km-nb_o.html#naive-bayes---why-1",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes - Why?",
    "text": "Naive Bayes - Why?\n\nIf \\(X_j\\) is qualitative, then we can simply count the proportion of training observations for the \\(j\\)th predictor corresponding to each class.\n\nSuppose that \\(X_j \\in \\{1, 2, 3\\}\\), and we have 100 observations in the \\(k\\)th class.\nSuppose that the \\(j\\)th predictor takes on values of 1,2, and 3 in 32, 55, and 13 of those observations, respectively. Then we can estimate \\(f_{kj}\\) as\n\n\\[\\hat{f}_{kj}(x_j) = \\begin{cases}\n.32 \\quad \\text{if }x_j = 1\\\\\n.55 \\quad \\text{if }x_j = 2\\\\\n.13 \\quad \\text{if }x_j = 3\\\\\n\\end{cases}\\]"
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#naive-bayes---statsquest",
    "href": "slides/04-3-km-nb_o.html#naive-bayes---statsquest",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes - Statsquest",
    "text": "Naive Bayes - Statsquest\nhttps://www.youtube.com/watch?v=O2L2Uv9pdDA"
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#naive-bayes-tidymodels",
    "href": "slides/04-3-km-nb_o.html#naive-bayes-tidymodels",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes Tidymodels",
    "text": "Naive Bayes Tidymodels\n\nnb_fit_1 &lt;- naive_Bayes() |&gt;\n  set_mode(\"classification\")|&gt;\n  set_engine(\"klaR\")|&gt; \n    fit(default ~ balance + student,\n      data = default_training)"
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#naive-bayes-confusion-matrix",
    "href": "slides/04-3-km-nb_o.html#naive-bayes-confusion-matrix",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes Confusion Matrix",
    "text": "Naive Bayes Confusion Matrix\n\nnb_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  conf_mat(truth = default, estimate = .pred_class) \n\n          Truth\nPrediction   No  Yes\n       No  2880   76\n       Yes   11   33"
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#naive-bayes-accuracy",
    "href": "slides/04-3-km-nb_o.html#naive-bayes-accuracy",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes Accuracy",
    "text": "Naive Bayes Accuracy\n\nnb_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  accuracy(truth = default,estimate=.pred_class)\n\n# A tibble: 1 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.971"
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#naive-bayes-roc-curve",
    "href": "slides/04-3-km-nb_o.html#naive-bayes-roc-curve",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes ROC Curve",
    "text": "Naive Bayes ROC Curve\n\nnb_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  roc_curve(truth = default,.pred_No) |&gt;\n  autoplot()"
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#naive-bayes-roc-auc",
    "href": "slides/04-3-km-nb_o.html#naive-bayes-roc-auc",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes ROC AUC",
    "text": "Naive Bayes ROC AUC\n\nnb_fit_1 |&gt;\n  augment(new_data = default_testing) |&gt;\n  roc_auc(truth = default,.pred_No)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.952"
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#naive-bayes-statquest",
    "href": "slides/04-3-km-nb_o.html#naive-bayes-statquest",
    "title": "Chapter 4 Part 3",
    "section": "Naive Bayes Statquest",
    "text": "Naive Bayes Statquest\nhttps://www.youtube.com/watch?v=O2L2Uv9pdDA&t=1s"
  },
  {
    "objectID": "slides/04-3-km-nb_o.html#comparing-lda-qda-and-normal-naive-bayes",
    "href": "slides/04-3-km-nb_o.html#comparing-lda-qda-and-normal-naive-bayes",
    "title": "Chapter 4 Part 3",
    "section": "Comparing LDA, QDA, and Normal Naive Bayes",
    "text": "Comparing LDA, QDA, and Normal Naive Bayes\nhttps://towardsdatascience.com/differences-of-lda-qda-and-gaussian-naive-bayes-classifiers-eaa4d1e999f6\nNaive Bayes - Given Y, the predictors X are conditionally independent.\nLDA - LDA assumes that the covariance matrix across classes is the same.\nQDA - QDA does not assume constant covariance matrix across classes."
  },
  {
    "objectID": "slides/06-subset.html#setup",
    "href": "slides/06-subset.html#setup",
    "title": "Chapter 6 Part 1",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR2)\nlibrary(leaps)"
  },
  {
    "objectID": "slides/06-subset.html#recall",
    "href": "slides/06-subset.html#recall",
    "title": "Chapter 6 Part 1",
    "section": "Recall",
    "text": "Recall\n\nLinear model \\[Y = \\beta_0 + \\beta_1X_1+...\\beta_pX_p+\\epsilon \\text{ where }\\epsilon \\sim N(0,\\sigma_{\\epsilon})\\]\nDespite its simplicity, the linear model has distinct advantages in terms of its interpretability and often shows good predictive performance.\n\n\n\nHow can we improve it?"
  },
  {
    "objectID": "slides/06-subset.html#why",
    "href": "slides/06-subset.html#why",
    "title": "Chapter 6 Part 1",
    "section": "Why",
    "text": "Why\n\nPrediction accuracy when \\(p&gt;n\\), to control variance\nRemoving irrelevant features, we can get a simpler model to interpret\nAutomating this procedure, by doing feature selection"
  },
  {
    "objectID": "slides/06-subset.html#three-classes-of-models",
    "href": "slides/06-subset.html#three-classes-of-models",
    "title": "Chapter 6 Part 1",
    "section": "Three classes of models",
    "text": "Three classes of models\n\nSubset Selection (today)\nShrinkage\nDimension Reduction"
  },
  {
    "objectID": "slides/06-subset.html#subset-selection",
    "href": "slides/06-subset.html#subset-selection",
    "title": "Chapter 6 Part 1",
    "section": "Subset Selection",
    "text": "Subset Selection\n\nIdentify a subset of the \\(p\\) predictors that we believe are related to the response\nGenerally going to use least squares, with more or less predictors and compare"
  },
  {
    "objectID": "slides/06-subset.html#best-subset-selection",
    "href": "slides/06-subset.html#best-subset-selection",
    "title": "Chapter 6 Part 1",
    "section": "Best Subset Selection",
    "text": "Best Subset Selection\n\nStart with a null, constant model. Uses mean to predict for all values of response. Call it \\(\\mathcal{M}_0\\)\nFor each \\(k=1,2,...,p\\):\n\n\nFit all \\(\\binom{p}{k}\\) models that contain exactly \\(k\\) predictors\nPick the best model among them, best according to smallest RSS or largest \\(R^2\\). Call it \\(\\mathcal{M}_k\\)\n\n\nSelect a single best model among \\(\\mathcal{M}_0, \\mathcal{M}_1,...,\\mathcal{M}_p\\) using cross-validation prediction error, \\(C_p\\), AIC, BIC, or adjusted \\(R^2\\)"
  },
  {
    "objectID": "slides/06-subset.html#example---credit-data",
    "href": "slides/06-subset.html#example---credit-data",
    "title": "Chapter 6 Part 1",
    "section": "Example - Credit Data",
    "text": "Example - Credit Data\n\n\n\n\n\nRSS and \\(R^2\\) are displayed.\nRed frontier tracks the best model for a given number of predictors, according to RSS and \\(R^2\\).\nData set contains only ten predictors, the x-axis ranges from 1 to 11, since one of the variables is categorical and takes on three values, leading to the creation of two dummy variables"
  },
  {
    "objectID": "slides/06-subset.html#extensions",
    "href": "slides/06-subset.html#extensions",
    "title": "Chapter 6 Part 1",
    "section": "Extensions",
    "text": "Extensions\n\nThese approaches can be applied to other types of regression such as logistic\nIn other models, you would use deviance instead of RSS (lower is still better)"
  },
  {
    "objectID": "slides/06-subset.html#stepwise-selection",
    "href": "slides/06-subset.html#stepwise-selection",
    "title": "Chapter 6 Part 1",
    "section": "Stepwise Selection",
    "text": "Stepwise Selection\n\nWhy is Best Subsets not ideal?\n\n\nBest subsets does not work well when \\(p\\) is large as it increases the chance we \\(overfit\\) on training data.\nThere is large search space for models!\nHigh variance of coefficient estimates\nstepwise methods, are far more restrictive and more attractable"
  },
  {
    "objectID": "slides/06-subset.html#forward-stepwise-selection",
    "href": "slides/06-subset.html#forward-stepwise-selection",
    "title": "Chapter 6 Part 1",
    "section": "Forward Stepwise Selection",
    "text": "Forward Stepwise Selection\n\nBegins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model.\nAt each step the variable that gives the greatest additional improvement to the fit is added to the model."
  },
  {
    "objectID": "slides/06-subset.html#forward-stepwise-selection-1",
    "href": "slides/06-subset.html#forward-stepwise-selection-1",
    "title": "Chapter 6 Part 1",
    "section": "Forward Stepwise Selection",
    "text": "Forward Stepwise Selection\n\nStart with a null, constant model. Uses mean to predict for all values of response. Call it \\(\\mathcal{M}_0\\)\nFor each \\(k=0,1,2,...,p-1\\):\n\n\nFit all \\(p-k\\) models that augment the predictors in \\(\\mathcal{M}_k\\) with one additional predictor.\nChose the best model among the \\(p-k\\) models and call it \\(\\mathcal{M}_{k+1}\\), best according to smallest RSS or largest \\(R^2\\).\n\n\nSelect a single best model among \\(\\mathcal{M}_0, \\mathcal{M}_1,...,\\mathcal{M}_p\\) using cross-validation prediction error, \\(C_p\\), AIC, BIC, or adjusted \\(R^2\\)"
  },
  {
    "objectID": "slides/06-subset.html#forward-stepwise-selection-2",
    "href": "slides/06-subset.html#forward-stepwise-selection-2",
    "title": "Chapter 6 Part 1",
    "section": "Forward Stepwise Selection",
    "text": "Forward Stepwise Selection\n\nComputational advantage over best subset\nNot guaranteed to find the best model of all \\(2^p\\) subsets of predictors.\n\n\n\nWhy not?"
  },
  {
    "objectID": "slides/06-subset.html#forward-stepwise-credit-example",
    "href": "slides/06-subset.html#forward-stepwise-credit-example",
    "title": "Chapter 6 Part 1",
    "section": "Forward Stepwise Credit Example",
    "text": "Forward Stepwise Credit Example\n\n\n\n\n\n\n\n\nNum Variables\nBest Subset\nForward Stepwise\n\n\n\n\nOne\nrating\nrating\n\n\nTwo\nrating, income\nrating, income\n\n\nThree\nrating, income, student\nrating, income, student\n\n\nFour\nstudent, limit\nstudent, limit\n\n\n\nThe first four selected models for best subset selection and forward stepwise selection on the Credit data set.\n\nThe first three models are identical but the fourth models differ."
  },
  {
    "objectID": "slides/06-subset.html#backward-stepwise-selection",
    "href": "slides/06-subset.html#backward-stepwise-selection",
    "title": "Chapter 6 Part 1",
    "section": "Backward Stepwise Selection",
    "text": "Backward Stepwise Selection\n\nLike forward stepwise selection, backward stepwise selection provides an efficient alternative to best subset selection.\nUnlike forward stepwise selection, it begins with the full least squares model containing all p predictors, and then iteratively removes the least useful predictor, one-at-a-time."
  },
  {
    "objectID": "slides/06-subset.html#backward-stepwise-selection-1",
    "href": "slides/06-subset.html#backward-stepwise-selection-1",
    "title": "Chapter 6 Part 1",
    "section": "Backward Stepwise Selection",
    "text": "Backward Stepwise Selection\n\nLet \\(\\mathcal{M}_p\\) denote the full model, which contains \\(p\\) predictors\nFor each \\(k=p,p-1,...,1\\):\n\n\nFit all \\(k\\) models that contain all but one of the predictors in \\(\\mathcal{M}_k\\), for a total of \\(k-1\\) predictors\nChose the best model among the \\(k\\) models and call it \\(\\mathcal{M}_{k-1}\\), best according to smallest RSS or largest \\(R^2\\).\n\n\nSelect a single best model among \\(\\mathcal{M}_0, \\mathcal{M}_1,...,\\mathcal{M}_p\\) using cross-validation prediction error, \\(C_p\\), AIC, BIC, or adjusted \\(R^2\\)"
  },
  {
    "objectID": "slides/06-subset.html#backward-stepwise-selection-2",
    "href": "slides/06-subset.html#backward-stepwise-selection-2",
    "title": "Chapter 6 Part 1",
    "section": "Backward Stepwise Selection",
    "text": "Backward Stepwise Selection\n\nLike forward stepwise selection, the backward selection approach searches through only \\(1+ p(p + 1)=2\\) models, and so can be applied in settings where \\(p\\) is too large to apply best subset selection\nLike forward stepwise selection, backward stepwise selection is not guaranteed to yield the best model containing a subset of the \\(p\\) predictors.\nBackward selection requires that the number of samples \\(n\\) is larger than the number of variables \\(p\\) (so that the full model can be fit). In contrast, forward stepwise can be used even when \\(n &lt; p\\), and so is the only viable subset method when \\(p\\) is very large."
  },
  {
    "objectID": "slides/06-subset.html#choosing-an-optimal-model",
    "href": "slides/06-subset.html#choosing-an-optimal-model",
    "title": "Chapter 6 Part 1",
    "section": "Choosing an Optimal Model",
    "text": "Choosing an Optimal Model\n\nThe model containing all of the predictors will always have the smallest RSS and the largest \\(R^2\\), since these quantities are related to the training error.\nWe wish to choose a model with low test error, not a model with low training error. Recall that training error is usually a poor estimate of test error.\nThus RSS and \\(R^2\\) are not suitable for selecting the best model among a collection of models with different numbers of predictors."
  },
  {
    "objectID": "slides/06-subset.html#estimating-test-error",
    "href": "slides/06-subset.html#estimating-test-error",
    "title": "Chapter 6 Part 1",
    "section": "Estimating Test error",
    "text": "Estimating Test error\n\nWe can indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting.\nWe can directly estimate the test error, using either a validation set approach or a cross-validation approach, as discussed in previous lectures."
  },
  {
    "objectID": "slides/06-subset.html#c_p-aic-bic-adjusted-r2",
    "href": "slides/06-subset.html#c_p-aic-bic-adjusted-r2",
    "title": "Chapter 6 Part 1",
    "section": "\\(C_p\\), AIC, BIC, Adjusted \\(R^2\\)",
    "text": "\\(C_p\\), AIC, BIC, Adjusted \\(R^2\\)\n\nThese adjust the training error for the model size, and can be used to select among a set of models with different numbers of variables."
  },
  {
    "objectID": "slides/06-subset.html#c_p-aic-bic-adjusted-r2-1",
    "href": "slides/06-subset.html#c_p-aic-bic-adjusted-r2-1",
    "title": "Chapter 6 Part 1",
    "section": "\\(C_p\\), AIC, BIC, Adjusted \\(R^2\\)",
    "text": "\\(C_p\\), AIC, BIC, Adjusted \\(R^2\\)\nThis displays \\(C_p\\), BIC, and adjusted \\(R^2\\) for the best model of each size produced by best subset selection on the Credit data set."
  },
  {
    "objectID": "slides/06-subset.html#mallows-c_p",
    "href": "slides/06-subset.html#mallows-c_p",
    "title": "Chapter 6 Part 1",
    "section": "Mallow‚Äôs \\(C_p\\)",
    "text": "Mallow‚Äôs \\(C_p\\)\n\\[C_p = \\frac{1}{n}(RSS + 2d\\hat{\\sigma}^2)\\]\n\nwhere \\(d\\) is the total number of parameters used and \\(\\hat{\\sigma}^2\\) is an estimate of the variance of the error \\(\\epsilon\\) associated with each response measurement."
  },
  {
    "objectID": "slides/06-subset.html#akaike-information-criterion-aic",
    "href": "slides/06-subset.html#akaike-information-criterion-aic",
    "title": "Chapter 6 Part 1",
    "section": "Akaike information criterion (AIC)",
    "text": "Akaike information criterion (AIC)\n\nDefined for large class of models fit by maximum likelihood:\n\\[AIC = -2log(L)+2\\cdot d\\] where \\(L\\) is the maximized value of the likelihood function for the estimated model.\nIn the case linear models with normal errors, maximum likelihood and least squares are the same thing, and \\(C_p\\) and AIC are equal."
  },
  {
    "objectID": "slides/06-subset.html#bayesian-information-criterion-bic",
    "href": "slides/06-subset.html#bayesian-information-criterion-bic",
    "title": "Chapter 6 Part 1",
    "section": "Bayesian information criterion (BIC)",
    "text": "Bayesian information criterion (BIC)\n\n\\[BIC = \\frac{1}{n}(RSS + log(n)d\\hat{\\sigma}^2)\\]\nLike \\(C_p\\), the BIC will tend to take on a small value for a model with low test error, and so generally we select the model that has the lowest BIC value\nNote that BIC replaces the \\(2d\\hat{\\sigma}^2\\) used by \\(C_p\\) with a \\(log(n)d\\hat{\\sigma}^2\\) term, where \\(n\\) is the number of observations\nSince \\(log(n)&gt;2\\) for any \\(n&gt;7\\), the BIC statistics places a heavier penalty on models with many variance and hence results in the selection of smaller models than \\(C_p\\)"
  },
  {
    "objectID": "slides/06-subset.html#adjusted-r2",
    "href": "slides/06-subset.html#adjusted-r2",
    "title": "Chapter 6 Part 1",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\nFor least squares model with \\(d\\) variables, the adjusted \\(R^2\\) is\n\\[\\text{Adjusted } R^2 = 1-\\frac{RSS/(n-d-1)}{TSS/(n-1)}\\]\nUnlike \\(C_p\\), AIC, and BIC, for which a small value indicates a model with a low test error, a large value of adjusted \\(R^2\\) indicates a model with a small test error.\nMaximizing the adjusted \\(R^2\\) is equivalent to minimizing \\(\\frac{RSS}{n-d-1}\\). While RSS always decreases as the number of variables in the model increases, \\(\\frac{RSS}{n-d-1}\\) may increase or decrease, due to the presence of \\(d\\) in the denominator.\nUnlike the \\(R^2\\) statistic, the adjusted \\(R^2\\) statistic pays a price for the inclusion of unnecessary variables in the model."
  },
  {
    "objectID": "slides/06-subset.html#validation-and-cv-sets",
    "href": "slides/06-subset.html#validation-and-cv-sets",
    "title": "Chapter 6 Part 1",
    "section": "Validation and CV Sets",
    "text": "Validation and CV Sets\n\nEach of the procedures returns a sequence of models \\(\\mathcal{M}_k\\) indexed by model size \\(k = 0, 1, 2,,,\\). We want to select \\(\\hat{k}\\). Once selected, we will return model \\(\\mathcal{M}_{\\hat{k}}\\)\nWe compute the validation set error or the cross-validation error for each model \\(\\mathcal{M}_k\\) under consideration, and then select the \\(k\\) for which the resulting estimated test error is smallest.\nThis procedure has an advantage relative to AIC, BIC, \\(C_p\\), and adjusted R2, in that it provides a direct estimate of the test error, and doesn‚Äôt require an estimate of the error variance \\(\\sigma^2\\).\nIt can also be used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom (e.g.¬†the number of predictors in the model) or hard to estimate the error variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "slides/06-subset.html#compare-credit-example",
    "href": "slides/06-subset.html#compare-credit-example",
    "title": "Chapter 6 Part 1",
    "section": "Compare Credit Example",
    "text": "Compare Credit Example\n\nFigureIntepretation\n\n\n\n\n\n\n\n\nThe validation errors were calculated by randomly selecting three-quarters of the observations as the training set, and the remainder as the validation set.\nThe cross-validation errors were computed using \\(k = 10 folds\\). In this case, the validation and cross-validation methods both result in a six-variable model.\nHowever, all three approaches suggest that the four-, five-, and six-variable models are roughly equivalent in terms of their test errors.\nIn this setting, we can select a model using the one-standard-error rule. We first calculate the standard error of the estimated test RSS for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve."
  },
  {
    "objectID": "slides/06-subset.html#today-example-credit",
    "href": "slides/06-subset.html#today-example-credit",
    "title": "Chapter 6 Part 1",
    "section": "Today example: Credit",
    "text": "Today example: Credit\n\ndata(\"Credit\")\nset.seed(343543)\nc_split &lt;- initial_split(Credit,.75)\nc_train &lt;- training(c_split)\nc_test &lt;- testing(c_split)"
  },
  {
    "objectID": "slides/06-subset.html#best-subsets---tidymodels",
    "href": "slides/06-subset.html#best-subsets---tidymodels",
    "title": "Chapter 6 Part 1",
    "section": "Best Subsets - tidymodels",
    "text": "Best Subsets - tidymodels\n\nPredict balance based on Income, Limit, Rating, Cards, Age, Education, Own, Married, and Region\nNot in tidymodels\n(*) indicates the given variable is in the corresponding model\n\n\nbest_ss_model &lt;- regsubsets(Balance ~.,data = Credit)\n\nsummary(best_ss_model)\n\nSubset selection object\nCall: regsubsets.formula(Balance ~ ., data = Credit)\n11 Variables  (and intercept)\n            Forced in Forced out\nIncome          FALSE      FALSE\nLimit           FALSE      FALSE\nRating          FALSE      FALSE\nCards           FALSE      FALSE\nAge             FALSE      FALSE\nEducation       FALSE      FALSE\nOwnYes          FALSE      FALSE\nStudentYes      FALSE      FALSE\nMarriedYes      FALSE      FALSE\nRegionSouth     FALSE      FALSE\nRegionWest      FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: exhaustive\n         Income Limit Rating Cards Age Education OwnYes StudentYes MarriedYes RegionSouth\n1  ( 1 ) \" \"    \" \"   \"*\"    \" \"   \" \" \" \"       \" \"    \" \"        \" \"        \" \"        \n2  ( 1 ) \"*\"    \" \"   \"*\"    \" \"   \" \" \" \"       \" \"    \" \"        \" \"        \" \"        \n3  ( 1 ) \"*\"    \" \"   \"*\"    \" \"   \" \" \" \"       \" \"    \"*\"        \" \"        \" \"        \n4  ( 1 ) \"*\"    \"*\"   \" \"    \"*\"   \" \" \" \"       \" \"    \"*\"        \" \"        \" \"        \n5  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \" \" \" \"       \" \"    \"*\"        \" \"        \" \"        \n6  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \" \"    \"*\"        \" \"        \" \"        \n7  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \"*\"    \"*\"        \" \"        \" \"        \n8  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \"*\"    \"*\"        \" \"        \" \"        \n         RegionWest\n1  ( 1 ) \" \"       \n2  ( 1 ) \" \"       \n3  ( 1 ) \" \"       \n4  ( 1 ) \" \"       \n5  ( 1 ) \" \"       \n6  ( 1 ) \" \"       \n7  ( 1 ) \" \"       \n8  ( 1 ) \"*\""
  },
  {
    "objectID": "slides/06-subset.html#application-exercise",
    "href": "slides/06-subset.html#application-exercise",
    "title": "Chapter 6 Part 1",
    "section": " Application Exercise",
    "text": "Application Exercise\nTry best subset regression on the penguins data in the palmerpenguins package to find the best model to predict weight."
  },
  {
    "objectID": "slides/06-subset.html#best-subsets---not-tidymodels",
    "href": "slides/06-subset.html#best-subsets---not-tidymodels",
    "title": "Chapter 6 Part 1",
    "section": "Best Subsets - not tidymodels",
    "text": "Best Subsets - not tidymodels\n\nPredict balance based on Income, Limit, Rating, Cards, Age, Education, Own, Married, and Region\nNot in tidymodels\n(*) indicates the given variable is in the corresponding model\n\n\n\nCode\nbest_ss_model &lt;- regsubsets(Balance ~.,data = c_train )\n\nsummary(best_ss_model)\n\n\nSubset selection object\nCall: regsubsets.formula(Balance ~ ., data = c_train)\n11 Variables  (and intercept)\n            Forced in Forced out\nIncome          FALSE      FALSE\nLimit           FALSE      FALSE\nRating          FALSE      FALSE\nCards           FALSE      FALSE\nAge             FALSE      FALSE\nEducation       FALSE      FALSE\nOwnYes          FALSE      FALSE\nStudentYes      FALSE      FALSE\nMarriedYes      FALSE      FALSE\nRegionSouth     FALSE      FALSE\nRegionWest      FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: exhaustive\n         Income Limit Rating Cards Age Education OwnYes StudentYes MarriedYes RegionSouth\n1  ( 1 ) \" \"    \" \"   \"*\"    \" \"   \" \" \" \"       \" \"    \" \"        \" \"        \" \"        \n2  ( 1 ) \"*\"    \" \"   \"*\"    \" \"   \" \" \" \"       \" \"    \" \"        \" \"        \" \"        \n3  ( 1 ) \"*\"    \"*\"   \" \"    \" \"   \" \" \" \"       \" \"    \"*\"        \" \"        \" \"        \n4  ( 1 ) \"*\"    \"*\"   \" \"    \"*\"   \" \" \" \"       \" \"    \"*\"        \" \"        \" \"        \n5  ( 1 ) \"*\"    \"*\"   \" \"    \"*\"   \"*\" \" \"       \" \"    \"*\"        \" \"        \" \"        \n6  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \" \"    \"*\"        \" \"        \" \"        \n7  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \" \"    \"*\"        \"*\"        \" \"        \n8  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \"*\"       \" \"    \"*\"        \"*\"        \" \"        \n         RegionWest\n1  ( 1 ) \" \"       \n2  ( 1 ) \" \"       \n3  ( 1 ) \" \"       \n4  ( 1 ) \" \"       \n5  ( 1 ) \" \"       \n6  ( 1 ) \" \"       \n7  ( 1 ) \" \"       \n8  ( 1 ) \" \""
  },
  {
    "objectID": "slides/06-subset.html#best-subsets---not-tidymodels-1",
    "href": "slides/06-subset.html#best-subsets---not-tidymodels-1",
    "title": "Chapter 6 Part 1",
    "section": "Best Subsets - not tidymodels",
    "text": "Best Subsets - not tidymodels\n\nSave the summary and see whats included\n\n\nreg_summary &lt;- summary(best_ss_model)\nnames(reg_summary) # See what metrics are included \n\n[1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"   \n\n\n\nWe can access each of these with $\nThis gives you the value for each of the best models with different number of predictors\n\n\nreg_summary$rsq\n\n[1] 0.7458484 0.8751179 0.9498788 0.9535800 0.9541606 0.9546879 0.9548167 0.9548880"
  },
  {
    "objectID": "slides/06-subset.html#best-subsets---function",
    "href": "slides/06-subset.html#best-subsets---function",
    "title": "Chapter 6 Part 1",
    "section": "Best Subsets - function",
    "text": "Best Subsets - function\n\nLet‚Äôs make the plots we saw before to help us choose.\n\n\n\nCode\ncreate_metrics_table &lt;- function(X){\n  K &lt;- length(X$rsq)\n  metrics_df &lt;- data.frame(num_pred= 1:K, # K different models\n                           Rsq = X$rsq,\n                           rss = X$rss,\n                           adjr2 = X$adjr2,\n                           cp = X$cp,\n                           bic = X$bic) |&gt;\n    pivot_longer(cols=Rsq:bic,\n                 names_to = \"metric\",values_to = \"metric_val\")\n    # This pivot puts the metric values in 1 column \n        # and creates another column for the name of \n        # the metric\n   return(metrics_df)\n}"
  },
  {
    "objectID": "slides/06-subset.html#best-subsets---graph",
    "href": "slides/06-subset.html#best-subsets---graph",
    "title": "Chapter 6 Part 1",
    "section": "Best Subsets - Graph",
    "text": "Best Subsets - Graph\n\n\nCode\nmetrics_df &lt;- create_metrics_table(reg_summary)\n\nmetrics_df |&gt; \n  ggplot(aes(y=metric_val,x=num_pred))+\n    geom_line() + geom_point()+\n    facet_wrap(~metric,scales = \"free_y\")"
  },
  {
    "objectID": "slides/06-subset.html#best-subsets---best-values",
    "href": "slides/06-subset.html#best-subsets---best-values",
    "title": "Chapter 6 Part 1",
    "section": "Best Subsets - Best Values",
    "text": "Best Subsets - Best Values\n\nWe can use the which function to tell us the vector element that has a max or min\n\n\n\nCode\nwhich.max(reg_summary$rsq)\n\n\n[1] 8"
  },
  {
    "objectID": "slides/06-subset.html#best-subsets---looking-at-model",
    "href": "slides/06-subset.html#best-subsets---looking-at-model",
    "title": "Chapter 6 Part 1",
    "section": "Best Subsets - Looking at model",
    "text": "Best Subsets - Looking at model\n\nWe can use the coef function to look at the coefficients of a model\n\n\n\nCode\ncoef(best_ss_model,8)\n\n\n (Intercept)       Income        Limit       Rating        Cards          Age \n-442.9663782   -7.6554801    0.2039346    0.8930414   17.5046106   -0.7496664 \n   Education   StudentYes   MarriedYes \n  -1.2622327  425.4323902  -12.4991226 \n\n\n\nOnce we choose the best model, we can refit it with tidymodels!"
  },
  {
    "objectID": "slides/06-subset.html#forward-stepwise-in-r",
    "href": "slides/06-subset.html#forward-stepwise-in-r",
    "title": "Chapter 6 Part 1",
    "section": "Forward Stepwise in R",
    "text": "Forward Stepwise in R\n\nfsw_model &lt;- regsubsets(Balance ~.,data = c_train,method=\"forward\")\n\nsummary(fsw_model)\n\nSubset selection object\nCall: regsubsets.formula(Balance ~ ., data = c_train, method = \"forward\")\n11 Variables  (and intercept)\n            Forced in Forced out\nIncome          FALSE      FALSE\nLimit           FALSE      FALSE\nRating          FALSE      FALSE\nCards           FALSE      FALSE\nAge             FALSE      FALSE\nEducation       FALSE      FALSE\nOwnYes          FALSE      FALSE\nStudentYes      FALSE      FALSE\nMarriedYes      FALSE      FALSE\nRegionSouth     FALSE      FALSE\nRegionWest      FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: forward\n         Income Limit Rating Cards Age Education OwnYes StudentYes MarriedYes RegionSouth\n1  ( 1 ) \" \"    \" \"   \"*\"    \" \"   \" \" \" \"       \" \"    \" \"        \" \"        \" \"        \n2  ( 1 ) \"*\"    \" \"   \"*\"    \" \"   \" \" \" \"       \" \"    \" \"        \" \"        \" \"        \n3  ( 1 ) \"*\"    \" \"   \"*\"    \" \"   \" \" \" \"       \" \"    \"*\"        \" \"        \" \"        \n4  ( 1 ) \"*\"    \"*\"   \"*\"    \" \"   \" \" \" \"       \" \"    \"*\"        \" \"        \" \"        \n5  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \" \" \" \"       \" \"    \"*\"        \" \"        \" \"        \n6  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \" \"    \"*\"        \" \"        \" \"        \n7  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \" \"    \"*\"        \"*\"        \" \"        \n8  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \"*\"       \" \"    \"*\"        \"*\"        \" \"        \n         RegionWest\n1  ( 1 ) \" \"       \n2  ( 1 ) \" \"       \n3  ( 1 ) \" \"       \n4  ( 1 ) \" \"       \n5  ( 1 ) \" \"       \n6  ( 1 ) \" \"       \n7  ( 1 ) \" \"       \n8  ( 1 ) \" \""
  },
  {
    "objectID": "slides/06-subset.html#backward-stepwise-in-r",
    "href": "slides/06-subset.html#backward-stepwise-in-r",
    "title": "Chapter 6 Part 1",
    "section": "Backward Stepwise in R",
    "text": "Backward Stepwise in R\n\nbsw_model &lt;- regsubsets(Balance ~.,data = c_train,method=\"backward\")\n\nsummary(bsw_model)\n\nSubset selection object\nCall: regsubsets.formula(Balance ~ ., data = c_train, method = \"backward\")\n11 Variables  (and intercept)\n            Forced in Forced out\nIncome          FALSE      FALSE\nLimit           FALSE      FALSE\nRating          FALSE      FALSE\nCards           FALSE      FALSE\nAge             FALSE      FALSE\nEducation       FALSE      FALSE\nOwnYes          FALSE      FALSE\nStudentYes      FALSE      FALSE\nMarriedYes      FALSE      FALSE\nRegionSouth     FALSE      FALSE\nRegionWest      FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: backward\n         Income Limit Rating Cards Age Education OwnYes StudentYes MarriedYes RegionSouth\n1  ( 1 ) \" \"    \"*\"   \" \"    \" \"   \" \" \" \"       \" \"    \" \"        \" \"        \" \"        \n2  ( 1 ) \"*\"    \"*\"   \" \"    \" \"   \" \" \" \"       \" \"    \" \"        \" \"        \" \"        \n3  ( 1 ) \"*\"    \"*\"   \" \"    \" \"   \" \" \" \"       \" \"    \"*\"        \" \"        \" \"        \n4  ( 1 ) \"*\"    \"*\"   \" \"    \"*\"   \" \" \" \"       \" \"    \"*\"        \" \"        \" \"        \n5  ( 1 ) \"*\"    \"*\"   \" \"    \"*\"   \"*\" \" \"       \" \"    \"*\"        \" \"        \" \"        \n6  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \" \"    \"*\"        \" \"        \" \"        \n7  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \" \"    \"*\"        \"*\"        \" \"        \n8  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \" \"    \"*\"        \"*\"        \" \"        \n         RegionWest\n1  ( 1 ) \" \"       \n2  ( 1 ) \" \"       \n3  ( 1 ) \" \"       \n4  ( 1 ) \" \"       \n5  ( 1 ) \" \"       \n6  ( 1 ) \" \"       \n7  ( 1 ) \" \"       \n8  ( 1 ) \"*\""
  },
  {
    "objectID": "slides/06-subset_o.html",
    "href": "slides/06-subset_o.html",
    "title": "Chapter 6 Part 1",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR2)\nlibrary(leaps)"
  },
  {
    "objectID": "slides/06-subset_o.html#setup",
    "href": "slides/06-subset_o.html#setup",
    "title": "Chapter 6 Part 1",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR2)\nlibrary(leaps)"
  },
  {
    "objectID": "slides/06-subset_o.html#recall",
    "href": "slides/06-subset_o.html#recall",
    "title": "Chapter 6 Part 1",
    "section": "Recall",
    "text": "Recall\n\nLinear model \\[Y = \\beta_0 + \\beta_1X_1+...\\beta_pX_p+\\epsilon \\text{ where }\\epsilon \\sim N(0,\\sigma_{\\epsilon})\\]\nDespite its simplicity, the linear model has distinct advantages in terms of its interpretability and often shows good predictive performance.\n\n\n\nHow can we improve it?"
  },
  {
    "objectID": "slides/06-subset_o.html#why",
    "href": "slides/06-subset_o.html#why",
    "title": "Chapter 6 Part 1",
    "section": "Why",
    "text": "Why\n\nPrediction accuracy when \\(p&gt;n\\), to control variance\nRemoving irrelevant features, we can get a simpler model to interpret\nAutomating this procedure, by doing feature selection"
  },
  {
    "objectID": "slides/06-subset_o.html#three-classes-of-models",
    "href": "slides/06-subset_o.html#three-classes-of-models",
    "title": "Chapter 6 Part 1",
    "section": "Three classes of models",
    "text": "Three classes of models\n\nSubset Selection (today)\nShrinkage\nDimension Reduction"
  },
  {
    "objectID": "slides/06-subset_o.html#today-example-credit",
    "href": "slides/06-subset_o.html#today-example-credit",
    "title": "Chapter 6 Part 1",
    "section": "Today example: Credit",
    "text": "Today example: Credit\n\ndata(\"Credit\")\nset.seed(343543)\nc_split &lt;- initial_split(Credit,.75)\nc_train &lt;- training(c_split)\nc_test &lt;- testing(c_split)"
  },
  {
    "objectID": "slides/06-subset_o.html#subset-selection",
    "href": "slides/06-subset_o.html#subset-selection",
    "title": "Chapter 6 Part 1",
    "section": "Subset Selection",
    "text": "Subset Selection\n\nIdentify a subset of the \\(p\\) predictors that we believe are related to the response\nGenerally going to use least squares, with more or less predictors and compare"
  },
  {
    "objectID": "slides/06-subset_o.html#best-subset-selection",
    "href": "slides/06-subset_o.html#best-subset-selection",
    "title": "Chapter 6 Part 1",
    "section": "Best Subset Selection",
    "text": "Best Subset Selection\n\nStart with a null, constant model. Uses mean to predict for all values of response. Call it \\(\\mathcal{M}_0\\)\nFor each \\(k=1,2,...,p\\):\n\n\nFit all \\(\\binom{p}{k}\\) models that contain exactly \\(k\\) predictors\nPick the best model among them, best according to smallest RSS or largest \\(R^2\\). Call it \\(\\mathcal{M}_k\\)\n\n\nSelect a single best model among \\(\\mathcal{M}_0, \\mathcal{M}_1,...,\\mathcal{M}_p\\) using cross-validation prediction error, \\(C_p\\), AIC, BIC, or adjusted \\(R^2\\)"
  },
  {
    "objectID": "slides/06-subset_o.html#example---credit-data",
    "href": "slides/06-subset_o.html#example---credit-data",
    "title": "Chapter 6 Part 1",
    "section": "Example - Credit Data",
    "text": "Example - Credit Data\n\n\n\n\n\nRSS and \\(R^2\\) are displayed.\nRed frontier tracks the best model for a given number of predictors, according to RSS and \\(R^2\\).\nData set contains only ten predictors, the x-axis ranges from 1 to 11, since one of the variables is categorical and takes on three values, leading to the creation of two dummy variables"
  },
  {
    "objectID": "slides/06-subset_o.html#extensions",
    "href": "slides/06-subset_o.html#extensions",
    "title": "Chapter 6 Part 1",
    "section": "Extensions",
    "text": "Extensions\n\nThese approaches can be applied to other types of regression such as logistic\nIn other models, you would use deviance instead of RSS (lower is still better)"
  },
  {
    "objectID": "slides/06-subset_o.html#best-subsets---not-tidymodels",
    "href": "slides/06-subset_o.html#best-subsets---not-tidymodels",
    "title": "Chapter 6 Part 1",
    "section": "Best Subsets - not tidymodels",
    "text": "Best Subsets - not tidymodels\n\nPredict balance based on Income, Limit, Rating, Cards, Age, Education, Own, Married, and Region\nNot in tidymodels\n(*) indicates the given variable is in the corresponding model\n\n\n\nCode\nbest_ss_model &lt;- regsubsets(Balance ~.,data = c_train )\n\nsummary(best_ss_model)\n\n\nSubset selection object\nCall: regsubsets.formula(Balance ~ ., data = c_train)\n11 Variables  (and intercept)\n            Forced in Forced out\nIncome          FALSE      FALSE\nLimit           FALSE      FALSE\nRating          FALSE      FALSE\nCards           FALSE      FALSE\nAge             FALSE      FALSE\nEducation       FALSE      FALSE\nOwnYes          FALSE      FALSE\nStudentYes      FALSE      FALSE\nMarriedYes      FALSE      FALSE\nRegionSouth     FALSE      FALSE\nRegionWest      FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: exhaustive\n         Income Limit Rating Cards Age Education OwnYes StudentYes MarriedYes RegionSouth\n1  ( 1 ) \" \"    \" \"   \"*\"    \" \"   \" \" \" \"       \" \"    \" \"        \" \"        \" \"        \n2  ( 1 ) \"*\"    \" \"   \"*\"    \" \"   \" \" \" \"       \" \"    \" \"        \" \"        \" \"        \n3  ( 1 ) \"*\"    \"*\"   \" \"    \" \"   \" \" \" \"       \" \"    \"*\"        \" \"        \" \"        \n4  ( 1 ) \"*\"    \"*\"   \" \"    \"*\"   \" \" \" \"       \" \"    \"*\"        \" \"        \" \"        \n5  ( 1 ) \"*\"    \"*\"   \" \"    \"*\"   \"*\" \" \"       \" \"    \"*\"        \" \"        \" \"        \n6  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \" \"    \"*\"        \" \"        \" \"        \n7  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \" \"    \"*\"        \"*\"        \" \"        \n8  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \"*\"       \" \"    \"*\"        \"*\"        \" \"        \n         RegionWest\n1  ( 1 ) \" \"       \n2  ( 1 ) \" \"       \n3  ( 1 ) \" \"       \n4  ( 1 ) \" \"       \n5  ( 1 ) \" \"       \n6  ( 1 ) \" \"       \n7  ( 1 ) \" \"       \n8  ( 1 ) \" \""
  },
  {
    "objectID": "slides/06-subset_o.html#best-subsets---not-tidymodels-1",
    "href": "slides/06-subset_o.html#best-subsets---not-tidymodels-1",
    "title": "Chapter 6 Part 1",
    "section": "Best Subsets - not tidymodels",
    "text": "Best Subsets - not tidymodels\n\nSave the summary and see whats included\n\n\n\nCode\nreg_summary &lt;- summary(best_ss_model)\nnames(reg_summary) # See what metrics are included \n\n\n[1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"   \n\n\n\nWe can access each of these with $\nThis gives you the value for each of the best models with different number of predictors\n\n\n\nCode\nreg_summary$rsq\n\n\n[1] 0.7458484 0.8751179 0.9498788 0.9535800 0.9541606 0.9546879 0.9548167 0.9548880"
  },
  {
    "objectID": "slides/06-subset_o.html#best-subsets---function",
    "href": "slides/06-subset_o.html#best-subsets---function",
    "title": "Chapter 6 Part 1",
    "section": "Best Subsets - function",
    "text": "Best Subsets - function\n\nLet‚Äôs make the plots we saw before to help us choose.\n\n\n\nCode\ncreate_metrics_table &lt;- function(X){\n  K &lt;- length(X$rsq)\n  metrics_df &lt;- data.frame(num_pred= 1:K, # K different models\n                           Rsq = X$rsq,\n                           rss = X$rss,\n                           adjr2 = X$adjr2,\n                           cp = X$cp,\n                           bic = X$bic) |&gt;\n    pivot_longer(cols=Rsq:bic,\n                 names_to = \"metric\",values_to = \"metric_val\")\n    # This pivot puts the metric values in 1 column \n        # and creates another column for the name of \n        # the metric\n   return(metrics_df)\n}"
  },
  {
    "objectID": "slides/06-subset_o.html#best-subsets---graph",
    "href": "slides/06-subset_o.html#best-subsets---graph",
    "title": "Chapter 6 Part 1",
    "section": "Best Subsets - Graph",
    "text": "Best Subsets - Graph\n\n\nCode\nmetrics_df &lt;- create_metrics_table(reg_summary)\n\nmetrics_df |&gt; \n  ggplot(aes(y=metric_val,x=num_pred))+\n    geom_line() + geom_point()+\n    facet_wrap(~metric,scales = \"free_y\")"
  },
  {
    "objectID": "slides/06-subset_o.html#best-subsets---best-values",
    "href": "slides/06-subset_o.html#best-subsets---best-values",
    "title": "Chapter 6 Part 1",
    "section": "Best Subsets - Best Values",
    "text": "Best Subsets - Best Values\n\nWe can use the which function to tell us the vector element that has a max or min\n\n\n\nCode\nwhich.max(reg_summary$rsq)\n\n\n[1] 8"
  },
  {
    "objectID": "slides/06-subset_o.html#best-subsets---looking-at-model",
    "href": "slides/06-subset_o.html#best-subsets---looking-at-model",
    "title": "Chapter 6 Part 1",
    "section": "Best Subsets - Looking at model",
    "text": "Best Subsets - Looking at model\n\nWe can use the coef function to look at the coefficients of a model\n\n\n\nCode\ncoef(best_ss_model,8)\n\n\n (Intercept)       Income        Limit       Rating        Cards          Age \n-442.9663782   -7.6554801    0.2039346    0.8930414   17.5046106   -0.7496664 \n   Education   StudentYes   MarriedYes \n  -1.2622327  425.4323902  -12.4991226 \n\n\n\nOnce we choose the best model, we can refit it with tidymodels!"
  },
  {
    "objectID": "slides/06-subset_o.html#application-exercise",
    "href": "slides/06-subset_o.html#application-exercise",
    "title": "Chapter 6 Part 1",
    "section": " Application Exercise",
    "text": "Application Exercise\nTry best subset regression on the penguins data in the palmerpenguins package to find the best model to predict weight."
  },
  {
    "objectID": "slides/06-subset_o.html#stepwise-selection",
    "href": "slides/06-subset_o.html#stepwise-selection",
    "title": "Chapter 6 Part 1",
    "section": "Stepwise Selection",
    "text": "Stepwise Selection\n\nWhy is Best Subsets not ideal?\n\n\nBest subsets does not work well when \\(p\\) is large as it increases the chance we \\(overfit\\) on training data.\nThere is large search space for models!\nHigh variance of coefficient estimates\nstepwise methods, are far more restrictive and more attractable"
  },
  {
    "objectID": "slides/06-subset_o.html#forward-stepwise-selection",
    "href": "slides/06-subset_o.html#forward-stepwise-selection",
    "title": "Chapter 6 Part 1",
    "section": "Forward Stepwise Selection",
    "text": "Forward Stepwise Selection\n\nBegins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model.\nAt each step the variable that gives the greatest additional improvement to the fit is added to the model."
  },
  {
    "objectID": "slides/06-subset_o.html#forward-stepwise-selection-1",
    "href": "slides/06-subset_o.html#forward-stepwise-selection-1",
    "title": "Chapter 6 Part 1",
    "section": "Forward Stepwise Selection",
    "text": "Forward Stepwise Selection\n\nStart with a null, constant model. Uses mean to predict for all values of response. Call it \\(\\mathcal{M}_0\\)\nFor each \\(k=0,1,2,...,p-1\\):\n\n\nFit all \\(p-k\\) models that augment the predictors in \\(\\mathcal{M}_k\\) with one additional predictor.\nChose the best model among the \\(p-k\\) models and call it \\(\\mathcal{M}_{k+1}\\), best according to smallest RSS or largest \\(R^2\\).\n\n\nSelect a single best model among \\(\\mathcal{M}_0, \\mathcal{M}_1,...,\\mathcal{M}_p\\) using cross-validation prediction error, \\(C_p\\), AIC, BIC, or adjusted \\(R^2\\)"
  },
  {
    "objectID": "slides/06-subset_o.html#forward-stepwise-selection-2",
    "href": "slides/06-subset_o.html#forward-stepwise-selection-2",
    "title": "Chapter 6 Part 1",
    "section": "Forward Stepwise Selection",
    "text": "Forward Stepwise Selection\n\nComputational advantage over best subset\nNot guaranteed to find the best model of all \\(2^p\\) subsets of predictors.\n\n\n\nWhy not?"
  },
  {
    "objectID": "slides/06-subset_o.html#forward-stepwise-credit-example",
    "href": "slides/06-subset_o.html#forward-stepwise-credit-example",
    "title": "Chapter 6 Part 1",
    "section": "Forward Stepwise Credit Example",
    "text": "Forward Stepwise Credit Example\n\n\n\n\n\n\n\n\nNum Variables\nBest Subset\nForward Stepwise\n\n\n\n\nOne\nrating\nrating\n\n\nTwo\nrating, income\nrating, income\n\n\nThree\nrating, income, student\nrating, income, student\n\n\nFour\nstudent, limit\nstudent, limit\n\n\n\nThe first four selected models for best subset selection and forward stepwise selection on the Credit data set.\n\nThe first three models are identical but the fourth models differ."
  },
  {
    "objectID": "slides/06-subset_o.html#backward-stepwise-selection",
    "href": "slides/06-subset_o.html#backward-stepwise-selection",
    "title": "Chapter 6 Part 1",
    "section": "Backward Stepwise Selection",
    "text": "Backward Stepwise Selection\n\nLike forward stepwise selection, backward stepwise selection provides an efficient alternative to best subset selection.\nUnlike forward stepwise selection, it begins with the full least squares model containing all p predictors, and then iteratively removes the least useful predictor, one-at-a-time."
  },
  {
    "objectID": "slides/06-subset_o.html#backward-stepwise-selection-1",
    "href": "slides/06-subset_o.html#backward-stepwise-selection-1",
    "title": "Chapter 6 Part 1",
    "section": "Backward Stepwise Selection",
    "text": "Backward Stepwise Selection\n\nLet \\(\\mathcal{M}_p\\) denote the full model, which contains \\(p\\) predictors\nFor each \\(k=p,p-1,...,1\\):\n\n\nFit all \\(k\\) models that contain all but one of the predictors in \\(\\mathcal{M}_k\\), for a total of \\(k-1\\) predictors\nChose the best model among the \\(k\\) models and call it \\(\\mathcal{M}_{k-1}\\), best according to smallest RSS or largest \\(R^2\\).\n\n\nSelect a single best model among \\(\\mathcal{M}_0, \\mathcal{M}_1,...,\\mathcal{M}_p\\) using cross-validation prediction error, \\(C_p\\), AIC, BIC, or adjusted \\(R^2\\)"
  },
  {
    "objectID": "slides/06-subset_o.html#backward-stepwise-selection-2",
    "href": "slides/06-subset_o.html#backward-stepwise-selection-2",
    "title": "Chapter 6 Part 1",
    "section": "Backward Stepwise Selection",
    "text": "Backward Stepwise Selection\n\nLike forward stepwise selection, the backward selection approach searches through only \\(1+ p(p + 1)=2\\) models, and so can be applied in settings where \\(p\\) is too large to apply best subset selection\nLike forward stepwise selection, backward stepwise selection is not guaranteed to yield the best model containing a subset of the \\(p\\) predictors.\nBackward selection requires that the number of samples \\(n\\) is larger than the number of variables \\(p\\) (so that the full model can be fit). In contrast, forward stepwise can be used even when \\(n &lt; p\\), and so is the only viable subset method when \\(p\\) is very large."
  },
  {
    "objectID": "slides/06-subset_o.html#choosing-an-optimal-model",
    "href": "slides/06-subset_o.html#choosing-an-optimal-model",
    "title": "Chapter 6 Part 1",
    "section": "Choosing an Optimal Model",
    "text": "Choosing an Optimal Model\n\nThe model containing all of the predictors will always have the smallest RSS and the largest \\(R^2\\), since these quantities are related to the training error.\nWe wish to choose a model with low test error, not a model with low training error. Recall that training error is usually a poor estimate of test error.\nThus RSS and \\(R^2\\) are not suitable for selecting the best model among a collection of models with different numbers of predictors."
  },
  {
    "objectID": "slides/06-subset_o.html#estimating-test-error",
    "href": "slides/06-subset_o.html#estimating-test-error",
    "title": "Chapter 6 Part 1",
    "section": "Estimating Test error",
    "text": "Estimating Test error\n\nWe can indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting.\nWe can directly estimate the test error, using either a validation set approach or a cross-validation approach, as discussed in previous lectures."
  },
  {
    "objectID": "slides/06-subset_o.html#c_p-aic-bic-adjusted-r2",
    "href": "slides/06-subset_o.html#c_p-aic-bic-adjusted-r2",
    "title": "Chapter 6 Part 1",
    "section": "\\(C_p\\), AIC, BIC, Adjusted \\(R^2\\)",
    "text": "\\(C_p\\), AIC, BIC, Adjusted \\(R^2\\)\n\nThese adjust the training error for the model size, and can be used to select among a set of models with different numbers of variables."
  },
  {
    "objectID": "slides/06-subset_o.html#c_p-aic-bic-adjusted-r2-1",
    "href": "slides/06-subset_o.html#c_p-aic-bic-adjusted-r2-1",
    "title": "Chapter 6 Part 1",
    "section": "\\(C_p\\), AIC, BIC, Adjusted \\(R^2\\)",
    "text": "\\(C_p\\), AIC, BIC, Adjusted \\(R^2\\)\nThis displays \\(C_p\\), BIC, and adjusted \\(R^2\\) for the best model of each size produced by best subset selection on the Credit data set."
  },
  {
    "objectID": "slides/06-subset_o.html#mallows-c_p",
    "href": "slides/06-subset_o.html#mallows-c_p",
    "title": "Chapter 6 Part 1",
    "section": "Mallow‚Äôs \\(C_p\\)",
    "text": "Mallow‚Äôs \\(C_p\\)\n\\[C_p = \\frac{1}{n}(RSS + 2d\\hat{\\sigma}^2)\\]\n\nwhere \\(d\\) is the total number of parameters used and \\(\\hat{\\sigma}^2\\) is an estimate of the variance of the error \\(\\epsilon\\) associated with each response measurement."
  },
  {
    "objectID": "slides/06-subset_o.html#akaike-information-criterion-aic",
    "href": "slides/06-subset_o.html#akaike-information-criterion-aic",
    "title": "Chapter 6 Part 1",
    "section": "Akaike information criterion (AIC)",
    "text": "Akaike information criterion (AIC)\n\nDefined for large class of models fit by maximum likelihood:\n\\[AIC = -2log(L)+2\\cdot d\\] where \\(L\\) is the maximized value of the likelihood function for the estimated model.\nIn the case linear models with normal errors, maximum likelihood and least squares are the same thing, and \\(C_p\\) and AIC are equal."
  },
  {
    "objectID": "slides/06-subset_o.html#bayesian-information-criterion-bic",
    "href": "slides/06-subset_o.html#bayesian-information-criterion-bic",
    "title": "Chapter 6 Part 1",
    "section": "Bayesian information criterion (BIC)",
    "text": "Bayesian information criterion (BIC)\n\n\\[BIC = \\frac{1}{n}(RSS + log(n)d\\hat{\\sigma}^2)\\]\nLike \\(C_p\\), the BIC will tend to take on a small value for a model with low test error, and so generally we select the model that has the lowest BIC value\nNote that BIC replaces the \\(2d\\hat{\\sigma}^2\\) used by \\(C_p\\) with a \\(log(n)d\\hat{\\sigma}^2\\) term, where \\(n\\) is the number of observations\nSince \\(log(n)&gt;2\\) for any \\(n&gt;7\\), the BIC statistics places a heavier penalty on models with many variance and hence results in the selection of smaller models than \\(C_p\\)"
  },
  {
    "objectID": "slides/06-subset_o.html#adjusted-r2",
    "href": "slides/06-subset_o.html#adjusted-r2",
    "title": "Chapter 6 Part 1",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\nFor least squares model with \\(d\\) variables, the adjusted \\(R^2\\) is\n\\[\\text{Adjusted } R^2 = 1-\\frac{RSS/(n-d-1)}{TSS/(n-1)}\\]\nUnlike \\(C_p\\), AIC, and BIC, for which a small value indicates a model with a low test error, a large value of adjusted \\(R^2\\) indicates a model with a small test error.\nMaximizing the adjusted \\(R^2\\) is equivalent to minimizing \\(\\frac{RSS}{n-d-1}\\). While RSS always decreases as the number of variables in the model increases, \\(\\frac{RSS}{n-d-1}\\) may increase or decrease, due to the presence of \\(d\\) in the denominator.\nUnlike the \\(R^2\\) statistic, the adjusted \\(R^2\\) statistic pays a price for the inclusion of unnecessary variables in the model."
  },
  {
    "objectID": "slides/06-subset_o.html#validation-and-cv-sets",
    "href": "slides/06-subset_o.html#validation-and-cv-sets",
    "title": "Chapter 6 Part 1",
    "section": "Validation and CV Sets",
    "text": "Validation and CV Sets\n\nEach of the procedures returns a sequence of models \\(\\mathcal{M}_k\\) indexed by model size \\(k = 0, 1, 2,,,\\). We want to select \\(\\hat{k}\\). Once selected, we will return model \\(\\mathcal{M}_{\\hat{k}}\\)\nWe compute the validation set error or the cross-validation error for each model \\(\\mathcal{M}_k\\) under consideration, and then select the \\(k\\) for which the resulting estimated test error is smallest.\nThis procedure has an advantage relative to AIC, BIC, \\(C_p\\), and adjusted R2, in that it provides a direct estimate of the test error, and doesn‚Äôt require an estimate of the error variance \\(\\sigma^2\\).\nIt can also be used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom (e.g.¬†the number of predictors in the model) or hard to estimate the error variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "slides/06-subset_o.html#compare-credit-example",
    "href": "slides/06-subset_o.html#compare-credit-example",
    "title": "Chapter 6 Part 1",
    "section": "Compare Credit Example",
    "text": "Compare Credit Example\n\nFigureIntepretation\n\n\n\n\n\n\n\n\nThe validation errors were calculated by randomly selecting three-quarters of the observations as the training set, and the remainder as the validation set.\nThe cross-validation errors were computed using \\(k = 10 folds\\). In this case, the validation and cross-validation methods both result in a six-variable model.\nHowever, all three approaches suggest that the four-, five-, and six-variable models are roughly equivalent in terms of their test errors.\nIn this setting, we can select a model using the one-standard-error rule. We first calculate the standard error of the estimated test RSS for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve."
  },
  {
    "objectID": "slides/06-subset_o.html#forward-stepwise-in-r",
    "href": "slides/06-subset_o.html#forward-stepwise-in-r",
    "title": "Chapter 6 Part 1",
    "section": "Forward Stepwise in R",
    "text": "Forward Stepwise in R\n\nfsw_model &lt;- regsubsets(Balance ~.,data = c_train,method=\"forward\")\n\nsummary(fsw_model)\n\nSubset selection object\nCall: regsubsets.formula(Balance ~ ., data = c_train, method = \"forward\")\n11 Variables  (and intercept)\n            Forced in Forced out\nIncome          FALSE      FALSE\nLimit           FALSE      FALSE\nRating          FALSE      FALSE\nCards           FALSE      FALSE\nAge             FALSE      FALSE\nEducation       FALSE      FALSE\nOwnYes          FALSE      FALSE\nStudentYes      FALSE      FALSE\nMarriedYes      FALSE      FALSE\nRegionSouth     FALSE      FALSE\nRegionWest      FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: forward\n         Income Limit Rating Cards Age Education OwnYes StudentYes MarriedYes RegionSouth\n1  ( 1 ) \" \"    \" \"   \"*\"    \" \"   \" \" \" \"       \" \"    \" \"        \" \"        \" \"        \n2  ( 1 ) \"*\"    \" \"   \"*\"    \" \"   \" \" \" \"       \" \"    \" \"        \" \"        \" \"        \n3  ( 1 ) \"*\"    \" \"   \"*\"    \" \"   \" \" \" \"       \" \"    \"*\"        \" \"        \" \"        \n4  ( 1 ) \"*\"    \"*\"   \"*\"    \" \"   \" \" \" \"       \" \"    \"*\"        \" \"        \" \"        \n5  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \" \" \" \"       \" \"    \"*\"        \" \"        \" \"        \n6  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \" \"    \"*\"        \" \"        \" \"        \n7  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \" \"    \"*\"        \"*\"        \" \"        \n8  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \"*\"       \" \"    \"*\"        \"*\"        \" \"        \n         RegionWest\n1  ( 1 ) \" \"       \n2  ( 1 ) \" \"       \n3  ( 1 ) \" \"       \n4  ( 1 ) \" \"       \n5  ( 1 ) \" \"       \n6  ( 1 ) \" \"       \n7  ( 1 ) \" \"       \n8  ( 1 ) \" \""
  },
  {
    "objectID": "slides/06-subset_o.html#backward-stepwise-in-r",
    "href": "slides/06-subset_o.html#backward-stepwise-in-r",
    "title": "Chapter 6 Part 1",
    "section": "Backward Stepwise in R",
    "text": "Backward Stepwise in R\n\nbsw_model &lt;- regsubsets(Balance ~.,data = c_train,method=\"backward\")\n\nsummary(bsw_model)\n\nSubset selection object\nCall: regsubsets.formula(Balance ~ ., data = c_train, method = \"backward\")\n11 Variables  (and intercept)\n            Forced in Forced out\nIncome          FALSE      FALSE\nLimit           FALSE      FALSE\nRating          FALSE      FALSE\nCards           FALSE      FALSE\nAge             FALSE      FALSE\nEducation       FALSE      FALSE\nOwnYes          FALSE      FALSE\nStudentYes      FALSE      FALSE\nMarriedYes      FALSE      FALSE\nRegionSouth     FALSE      FALSE\nRegionWest      FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: backward\n         Income Limit Rating Cards Age Education OwnYes StudentYes MarriedYes RegionSouth\n1  ( 1 ) \" \"    \"*\"   \" \"    \" \"   \" \" \" \"       \" \"    \" \"        \" \"        \" \"        \n2  ( 1 ) \"*\"    \"*\"   \" \"    \" \"   \" \" \" \"       \" \"    \" \"        \" \"        \" \"        \n3  ( 1 ) \"*\"    \"*\"   \" \"    \" \"   \" \" \" \"       \" \"    \"*\"        \" \"        \" \"        \n4  ( 1 ) \"*\"    \"*\"   \" \"    \"*\"   \" \" \" \"       \" \"    \"*\"        \" \"        \" \"        \n5  ( 1 ) \"*\"    \"*\"   \" \"    \"*\"   \"*\" \" \"       \" \"    \"*\"        \" \"        \" \"        \n6  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \" \"    \"*\"        \" \"        \" \"        \n7  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \" \"    \"*\"        \"*\"        \" \"        \n8  ( 1 ) \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \" \"       \" \"    \"*\"        \"*\"        \" \"        \n         RegionWest\n1  ( 1 ) \" \"       \n2  ( 1 ) \" \"       \n3  ( 1 ) \" \"       \n4  ( 1 ) \" \"       \n5  ( 1 ) \" \"       \n6  ( 1 ) \" \"       \n7  ( 1 ) \" \"       \n8  ( 1 ) \"*\""
  },
  {
    "objectID": "slides/06-subset.html#application-exercise-1",
    "href": "slides/06-subset.html#application-exercise-1",
    "title": "Chapter 6 Part 1",
    "section": " Application Exercise",
    "text": "Application Exercise\nPerform forward stepwise and backward stepwise on the penguins dataset to find the best model to predict weight.\n\n\n\n\nüîó https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#recap",
    "href": "slides/04-2-lda-qda.qmd.html#recap",
    "title": "Chapter 4 Part 2",
    "section": "Recap",
    "text": "Recap\n\nWe had a logistic regression refresher\nNow‚Ä¶\n\nWhat if our response has more than two levels?\nWhat if logistic regression is a poor fit?"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#setup",
    "href": "slides/04-2-lda-qda.qmd.html#setup",
    "title": "Chapter 4 Part 2",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR)\nlibrary(Stat2Data)\n#install.packages(\"discrim\")"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#multinomial-logistic",
    "href": "slides/04-2-lda-qda.qmd.html#multinomial-logistic",
    "title": "Chapter 4 Part 2",
    "section": "Multinomial Logistic",
    "text": "Multinomial Logistic\n\nSo far we have discussed logistic regression with two classes.\nIt is easily generalized to more than two classes."
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#confounding",
    "href": "slides/04-2-lda-qda.qmd.html#confounding",
    "title": "Chapter 4 Part 2",
    "section": "Confounding",
    "text": "Confounding\nRecall our defaults data with variable default, student, and balance\n\n\nWhat is going on here?"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#confounding-1",
    "href": "slides/04-2-lda-qda.qmd.html#confounding-1",
    "title": "Chapter 4 Part 2",
    "section": "Confounding",
    "text": "Confounding\n\n\nStudents tend to have higher balances than non-students\nTheir marginal default rate is higher\nFor each level of balance, students default less\nTheir conditional default rate is lower"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#multiple-logistic-regression",
    "href": "slides/04-2-lda-qda.qmd.html#multiple-logistic-regression",
    "title": "Chapter 4 Part 2",
    "section": "Multiple logistic regression",
    "text": "Multiple logistic regression\n\\[\\log\\left(\\frac{p(X)}{1-p(X)}\\right)=\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p\\] \\[p(X) = \\frac{e^{\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p}}{1+e^{\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p}}\\]\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.8690452\n0.4922555\n-22.080088\n0.0000000\n\n\nbalance\n0.0057365\n0.0002319\n24.737563\n0.0000000\n\n\nincome\n0.0000030\n0.0000082\n0.369815\n0.7115203\n\n\nstudentYes\n-0.6467758\n0.2362525\n-2.737646\n0.0061881\n\n\n\n\n\n\n\n\n\nWhy is the coefficient for student negative now when it was positive before?"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#logistic-regression-for-more-than-two-classes",
    "href": "slides/04-2-lda-qda.qmd.html#logistic-regression-for-more-than-two-classes",
    "title": "Chapter 4 Part 2",
    "section": "Logistic regression for more than two classes",
    "text": "Logistic regression for more than two classes\n\\[P(Y=k|X) = \\frac{e ^{\\beta_{0k}+\\beta_{1k}X_1+\\dots+\\beta_{pk}X_p}}{\\sum_{l=1}^Ke^{\\beta_{0l}+\\beta_{1l}X_1+\\dots+\\beta_{pl}X_p}}\\]\n\nWe generalize this to situations with multiple classes\nHere we have a linear function for each of the \\(K\\) classes\nThis is known as multinomial logistic regression"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#lda-warmup",
    "href": "slides/04-2-lda-qda.qmd.html#lda-warmup",
    "title": "Chapter 4 Part 2",
    "section": "LDA Warmup",
    "text": "LDA Warmup\nTo give us a general overview, we are going to watch the StatQuest video on the topic: https://www.youtube.com/watch?v=azXCzI57Yfc"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#discriminant-analysis",
    "href": "slides/04-2-lda-qda.qmd.html#discriminant-analysis",
    "title": "Chapter 4 Part 2",
    "section": "Discriminant Analysis",
    "text": "Discriminant Analysis\n\nHere the approach is to model the distribution of X in each of the classes separately, and then use Bayes theorem to flip things around and obtain \\(P(Y|X)\\).\nWhen we use normal (Gaussian) distributions for each class, this leads to linear or quadratic discriminant analysis.\nHowever, this approach is quite general, and other distributions can be used as well. We will focus on normal distributions."
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#why-another-approach",
    "href": "slides/04-2-lda-qda.qmd.html#why-another-approach",
    "title": "Chapter 4 Part 2",
    "section": "Why Another Approach?",
    "text": "Why Another Approach?\n\nWhen the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.\nIf n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\nLinear discriminant analysis is popular when we have more than two response classes, because it also provides low-dimensional views of the data."
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#bayes-theorem-classification",
    "href": "slides/04-2-lda-qda.qmd.html#bayes-theorem-classification",
    "title": "Chapter 4 Part 2",
    "section": "Bayes Theorem (classification)",
    "text": "Bayes Theorem (classification)\nThomas Bayes was a famous mathematician whose name represents a big subfield of statistical and probabilistic modeling. Here we focus on a simple result, known as Bayes theorem:\n\\[P(Y=k|X=x) = \\frac{P(X=x|Y=k)\\cdot P(Y=k)}{P(X=x)}\\]"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#bayes-for-discriminant-analysis",
    "href": "slides/04-2-lda-qda.qmd.html#bayes-for-discriminant-analysis",
    "title": "Chapter 4 Part 2",
    "section": "Bayes for Discriminant Analysis",
    "text": "Bayes for Discriminant Analysis\n\\[P(Y=k|X=x) = \\frac{\\pi_kf_k(x)}{\\sum_{l=1}^K\\pi_lf_l(x)} \\text{, where}\\]\n\n\\(f_k(x)=P(X=x|Y=k)\\) is the density for \\(X\\) in class \\(k\\). Here we use normal‚Äôs but they could be other distributions (such as \\(\\chi^2\\))\n\\(\\pi_k = P(Y=k)\\) is the marginal or prior probability for class \\(k\\)."
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#classify-to-the-highest-density",
    "href": "slides/04-2-lda-qda.qmd.html#classify-to-the-highest-density",
    "title": "Chapter 4 Part 2",
    "section": "Classify to the highest density",
    "text": "Classify to the highest density\n\\[\\pi_1=.5, \\pi_2=.5\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe classify a new point according to which density is highest.\nWhen the priors are different, we take them into account as well, and compare \\(\\pi_kf_k(x)\\).\nOn the right, we favor the pink class - the decision boundary has shifted to the left."
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#lda-when-p1",
    "href": "slides/04-2-lda-qda.qmd.html#lda-when-p1",
    "title": "Chapter 4 Part 2",
    "section": "LDA (when \\(p=1\\))",
    "text": "LDA (when \\(p=1\\))\nThe Gaussian (normal) density has the form\n\\[f_k(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k}e^{-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma_k})^2}\\]\n\n\\(\\mu_k\\) is the mean, \\(\\sigma_k^2\\) the variance (in class \\(k\\))\nFor now, we assume \\(\\sigma_k=\\sigma\\) for all groups (we will need to check this with real data)"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#lda-when-p1-1",
    "href": "slides/04-2-lda-qda.qmd.html#lda-when-p1-1",
    "title": "Chapter 4 Part 2",
    "section": "LDA (when \\(p=1\\))",
    "text": "LDA (when \\(p=1\\))\nWe plug this \\(f_k(x)\\) into Bayes formula and after some simplifying we get:\n\\[p_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma_k})^2}}{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma_k})^2}}\\]"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#discriminant-function",
    "href": "slides/04-2-lda-qda.qmd.html#discriminant-function",
    "title": "Chapter 4 Part 2",
    "section": "Discriminant Function",
    "text": "Discriminant Function\nTo classify at the value X = x, we need to see which of the \\(p_k(x)\\) is largest. Taking logs, and discarding terms that do not depend on \\(k\\), we see that this is equivalent to assigning x to the class with the largest discriminant score:\n\\[\\delta_k(x) = x\\cdot \\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+log(\\pi_k)\\]\n\nImportantly, \\(\\delta_k(x)\\) is a linear function of \\(x\\).\nIf there are \\(K=2\\) classes and \\(\\pi_1=\\pi_2=.5\\), then the decision boundry is at\n\n\\[x=\\frac{\\mu_1+\\mu_2}{2}\\]"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#maximizing-delta_kx",
    "href": "slides/04-2-lda-qda.qmd.html#maximizing-delta_kx",
    "title": "Chapter 4 Part 2",
    "section": "Maximizing \\(\\delta_k(x)\\)",
    "text": "Maximizing \\(\\delta_k(x)\\)\n\nIn order to maximize this, we need estimates for all the parameters\n\n\nWhat should we estimate \\(\\hat{\\pi_k}\\), \\(\\mu_k\\), and \\(\\sigma^2\\) with?"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#maximizing-delta_kx-1",
    "href": "slides/04-2-lda-qda.qmd.html#maximizing-delta_kx-1",
    "title": "Chapter 4 Part 2",
    "section": "Maximizing \\(\\delta_k(x)\\)",
    "text": "Maximizing \\(\\delta_k(x)\\)\n\\[\\hat{\\pi}_k = \\frac{n_k}{n}\\]\n\\[\\hat{\\mu}_k = \\frac{1}{n_k}\\sum_{i:y_k=k}x_i\\]\n\\[\\hat{\\sigma}^2 = \\frac{1}{n-K}\\sum_{k=1}^K\\sum_{i:y_i=k}(x_i-\\hat{\\mu}_k)^2 = \\sum_{k=1}^K\\frac{n_k-1}{n-K}\\cdot \\hat{\\sigma}_k^2\\]\nWhere \\[\\hat{\\sigma}_k^2 = \\frac{1}{n_k-1}\\sum_{i:y_i=k}(x_i-\\hat{\\mu}_k)^2\\]"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#lda-in-r-example-p1",
    "href": "slides/04-2-lda-qda.qmd.html#lda-in-r-example-p1",
    "title": "Chapter 4 Part 2",
    "section": "LDA In R Example (\\(p=1\\))",
    "text": "LDA In R Example (\\(p=1\\))\n\ndata(\"BlueJays\") # Bring data into environment\nlibrary(Stat2Data)\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(discrim)\n\n\nCan we determine the sex of a blue jay by measuring the distance from the tip of the bill to the back of the head (Head)?"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#lda-bluejays-eda",
    "href": "slides/04-2-lda-qda.qmd.html#lda-bluejays-eda",
    "title": "Chapter 4 Part 2",
    "section": "LDA Bluejays EDA",
    "text": "LDA Bluejays EDA\n\ntt_split &lt;- initial_split(BlueJays,prop=.7)\n\nBJ_train &lt;- training(tt_split)\n\nBJ_train |&gt; ggplot(aes(x = Head,y=KnownSex)) +\n  geom_boxplot() +\n  theme_bw()"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#lda-bluejays-eda-1",
    "href": "slides/04-2-lda-qda.qmd.html#lda-bluejays-eda-1",
    "title": "Chapter 4 Part 2",
    "section": "LDA Bluejays EDA",
    "text": "LDA Bluejays EDA\n\nWhat assumptions should we check?\n\n\nNormality of our predictor\nConstant variance between groups."
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#lda-bluejays-eda---normality",
    "href": "slides/04-2-lda-qda.qmd.html#lda-bluejays-eda---normality",
    "title": "Chapter 4 Part 2",
    "section": "LDA Bluejays EDA - Normality",
    "text": "LDA Bluejays EDA - Normality\n\nlibrary(patchwork)\n\n(BJ_train |&gt; ggplot(aes(sample = Head)) + geom_qq())+\n  (BJ_train |&gt; ggplot(aes(x = Head)) + geom_histogram())"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#lda-bluejays-eda---variance",
    "href": "slides/04-2-lda-qda.qmd.html#lda-bluejays-eda---variance",
    "title": "Chapter 4 Part 2",
    "section": "LDA Bluejays EDA - Variance",
    "text": "LDA Bluejays EDA - Variance\n\nBJ_train |&gt; group_by(KnownSex)|&gt;\n  summarize(Head_sd = sd(Head))\n\n# A tibble: 2 √ó 2\n  KnownSex Head_sd\n  &lt;fct&gt;      &lt;dbl&gt;\n1 F           1.27\n2 M           1.25\n\n\n\nVery similar standard deviations (and thus variances)"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#fit-lda",
    "href": "slides/04-2-lda-qda.qmd.html#fit-lda",
    "title": "Chapter 4 Part 2",
    "section": "Fit LDA",
    "text": "Fit LDA\n\nlda_spec &lt;- discrim_linear() |&gt;\n  set_mode(\"classification\")|&gt;\n  set_engine(\"MASS\")\n\nlda_fit&lt;-  lda_spec |&gt; \n  fit(KnownSex ~ Head,\n      data = BJ_train)"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#check-the-fit",
    "href": "slides/04-2-lda-qda.qmd.html#check-the-fit",
    "title": "Chapter 4 Part 2",
    "section": "Check The Fit",
    "text": "Check The Fit\n\nBJTest &lt;- testing(tt_split)\n\nlda_fit |&gt; \n  augment(new_data = BJTest) %&gt;%\n  conf_mat(truth = KnownSex, estimate = .pred_class) \n\n          Truth\nPrediction  F  M\n         F 10  1\n         M  5 21\n\n\n\nlda_fit|&gt; \n  augment(new_data = BJTest) |&gt; \n  accuracy(truth = KnownSex,estimate=.pred_class)\n\n# A tibble: 1 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.838"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#compare-to-logistic",
    "href": "slides/04-2-lda-qda.qmd.html#compare-to-logistic",
    "title": "Chapter 4 Part 2",
    "section": "Compare to Logistic",
    "text": "Compare to Logistic\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(KnownSex ~ Head,\n      data = BJ_train) |&gt;\n  augment(new_data = BJTest)|&gt;\n    accuracy(truth = KnownSex,estimate=.pred_class)\n\n# A tibble: 1 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.865"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#application-exercise",
    "href": "slides/04-2-lda-qda.qmd.html#application-exercise",
    "title": "Chapter 4 Part 2",
    "section": " Application Exercise",
    "text": "Application Exercise\nWe are going to use the penguins data from the palmerpeguins package.\n\nConduct basic EDA to see if penguin bill length is different by sex and if LDA is an appropriate model choice.\nUse LDA to predict penguin sex based on their bill length using training and testing data. Get the accuracy on the testing set.\nFit a logistic regression model and get it‚Äôs accuracy to see which did better."
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#lda-with-p1",
    "href": "slides/04-2-lda-qda.qmd.html#lda-with-p1",
    "title": "Chapter 4 Part 2",
    "section": "LDA with \\(p>1\\)",
    "text": "LDA with \\(p&gt;1\\)\n\nWhen we have 2 or more predictors, the distribution becomes multivariate.\nIf the covariance between predictors is 0 within each class of the response, LDA is still appropriate."
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#lda-with-p1-1",
    "href": "slides/04-2-lda-qda.qmd.html#lda-with-p1-1",
    "title": "Chapter 4 Part 2",
    "section": "LDA with \\(p>1\\)",
    "text": "LDA with \\(p&gt;1\\)\n\nFor example, with 2 normal predictors, their distributions in 3d would like like this:\n\n\n\nLuckily, the discriminate function remains linear\n\n\\[\\delta_k(x) = c_{k0} + c_{k1}x_1+...c_{kp}x_p\\]"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#example-p2k3",
    "href": "slides/04-2-lda-qda.qmd.html#example-p2k3",
    "title": "Chapter 4 Part 2",
    "section": "Example: \\(p=2,K=3\\)",
    "text": "Example: \\(p=2,K=3\\)\n\nThere is no limit on the number of levels of the categorical response for LDA\nSuppose \\(\\pi_1=\\pi_2\\pi_3=1/3\\)\n\n - The dashed lines are known as the Bayes decision boundaries"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#probabilities",
    "href": "slides/04-2-lda-qda.qmd.html#probabilities",
    "title": "Chapter 4 Part 2",
    "section": "Probabilities",
    "text": "Probabilities\n\nOnce the estimates for the \\(\\hat{\\delta}_k(x)\\) have been found, we can plug them in and get:\n\\[\\hat{P}(Y=k|X=x)=\\frac{e^{\\hat{\\delta}_k(x)}}{\\sum_{l=1}^Ke^{\\hat{\\delta}^l(x)}}\\]\nClassifying to the largest \\(\\hat{\\delta}_k(x)\\) amounts to classifying to the class for which \\(\\hat{P}(Y = k|X = x)\\) is largest.\nWhen \\(K = 2\\), we classify to class 2 if \\(\\hat{P}(Y = 2|X = x)\\geq 0.5\\), else to class 1."
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#example---credit-card-fraud",
    "href": "slides/04-2-lda-qda.qmd.html#example---credit-card-fraud",
    "title": "Chapter 4 Part 2",
    "section": "Example - Credit Card Fraud",
    "text": "Example - Credit Card Fraud\n\n\n          Truth\nPrediction   No  Yes\n       No  2889   83\n       Yes    2   26\n\n\n# A tibble: 1 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.972\n\n\n\nAccuracy of 97.7% on a testing set!\nWhat about the different types of errors?"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#errors",
    "href": "slides/04-2-lda-qda.qmd.html#errors",
    "title": "Chapter 4 Part 2",
    "section": "Errors",
    "text": "Errors\n\nOf the true yes, we made errors at the rate of 83/(83+26), 76%\nOf the true no, we made errors at the rate of 2/(2889+2), .069%"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#types-of-errors",
    "href": "slides/04-2-lda-qda.qmd.html#types-of-errors",
    "title": "Chapter 4 Part 2",
    "section": "Types of Errors",
    "text": "Types of Errors\n\nRecall:\n\nFalse positive rate: The fraction of negative examples that are classified as positive.\nFalse negative rate: The fraction of positive examples that are classified as negative.\n\nRemember the model gave probabilities, the final yes or no is decided by\n\\[\\hat{P}(Default=Yes|Balance,Student) \\geq .5\\]\n\nWe can adjust our error rates by changing that threshold"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#sensitivity-and-specificity",
    "href": "slides/04-2-lda-qda.qmd.html#sensitivity-and-specificity",
    "title": "Chapter 4 Part 2",
    "section": "Sensitivity and Specificity",
    "text": "Sensitivity and Specificity\n\n\n          Truth\nPrediction   No  Yes\n       No  2889   83\n       Yes    2   26\n\n\n\nThe sensitivity is the true positive rate.\n\nThe rate at which we correctly predict a person will default.\n26/(83+26) = .24 (24%)\n\nThe specificity is the true negative rate.\n\nThe rate at which we correctly predict a person will not default.\n2889/(2889+2) =.999 (99.9%)"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#varying-the-threshold",
    "href": "slides/04-2-lda-qda.qmd.html#varying-the-threshold",
    "title": "Chapter 4 Part 2",
    "section": "Varying the threshold",
    "text": "Varying the threshold\n\nIn order to determine the best threshold, we want to maximize the specificty and sensitivity.\nOR - maximize the sensitivty and minimize 1-specificity.\nIn order to to do this we use an \\(ROC\\) curve which stands for receiver operating characteristic curve."
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#roc-curve",
    "href": "slides/04-2-lda-qda.qmd.html#roc-curve",
    "title": "Chapter 4 Part 2",
    "section": "ROC Curve",
    "text": "ROC Curve\nWe want to maximize the area under this curve, called ROC AUC\n\nlda_roc&lt;- lda_fit_2 |&gt;\n  augment(new_data = testing(def_splits)) |&gt;\n  roc_curve(truth = default,.pred_No) \n\nhead(lda_roc)\n\n# A tibble: 6 √ó 3\n  .threshold specificity sensitivity\n       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1  -Inf          0                 1\n2     0.0819     0                 1\n3     0.129      0.00917           1\n4     0.151      0.0183            1\n5     0.154      0.0275            1\n6     0.201      0.0367            1"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#roc-curve-1",
    "href": "slides/04-2-lda-qda.qmd.html#roc-curve-1",
    "title": "Chapter 4 Part 2",
    "section": "ROC Curve",
    "text": "ROC Curve\n\nautoplot(lda_roc)"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#roc-auc",
    "href": "slides/04-2-lda-qda.qmd.html#roc-auc",
    "title": "Chapter 4 Part 2",
    "section": "ROC AUC",
    "text": "ROC AUC\n\nlda_fit_2 |&gt;\n  augment(new_data = testing(def_splits)) |&gt;\n  roc_auc(truth = default,.pred_No) \n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.957\n\n\nIf we want to tune our model better, we can optimize the ROC AUC by changing the threshold.\n\nI did a train/test approach here. What should I do different if I want to tune for threshold?"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#qda",
    "href": "slides/04-2-lda-qda.qmd.html#qda",
    "title": "Chapter 4 Part 2",
    "section": "QDA",
    "text": "QDA\n\nQDA arises when \\(p&gt;1\\) and the is a covariance structure between the predictors within the same level of the response.\nThis introduces a squared term into the maximization problem of the \\(\\delta_k(x)\\), thus the name."
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#qda-in-r",
    "href": "slides/04-2-lda-qda.qmd.html#qda-in-r",
    "title": "Chapter 4 Part 2",
    "section": "QDA In R",
    "text": "QDA In R\n\nqda_fit&lt;-discrim_quad() |&gt;\n  set_mode(\"classification\")|&gt;\n  set_engine(\"MASS\")|&gt; \n  fit(default ~ balance + student,\n      data = training(def_splits))\n\nqda_fit |&gt;\n  augment(new_data = testing(def_splits)) %&gt;%\n  conf_mat(truth = default, estimate = .pred_class) \n\n          Truth\nPrediction   No  Yes\n       No  2888   80\n       Yes    3   29\n\nqda_fit |&gt;\n  augment(new_data = testing(def_splits)) %&gt;%\n  roc_auc(truth = default,.pred_No) \n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.957"
  },
  {
    "objectID": "slides/04-2-lda-qda.qmd.html#tuning-by-threhold",
    "href": "slides/04-2-lda-qda.qmd.html#tuning-by-threhold",
    "title": "Chapter 4 Part 2",
    "section": "Tuning By Threhold",
    "text": "Tuning By Threhold\nhttps://www.tidymodels.org/start/case-study/\n\n\n\n\nüîó https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#recap",
    "href": "slides/04-logistic.qmd.html#recap",
    "title": "Chapter 4 Part 1",
    "section": "Recap",
    "text": "Recap\n\nWe had a linear regression refresher\nLinear regression is a great tool when we have a continuous outcome\nWe are going to learn some fancy ways to do even better in the future\n\nSetup:\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR)"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#classification-1",
    "href": "slides/04-logistic.qmd.html#classification-1",
    "title": "Chapter 4 Part 1",
    "section": "Classification",
    "text": "Classification\n\nWhat are some examples of classification problems?\n\n\nQualitative response variable in an unordered set, \\(\\mathcal{C}\\)\neye color \\(\\in\\) {blue, brown, green}\nemail \\(\\in\\) {spam, not spam}\nResponse, \\(Y\\) takes on values in \\(\\mathcal{C}\\)\nPredictors are a vector, \\(X\\)\nThe task: build a function \\(C(X)\\) that takes \\(X\\) and predicts \\(Y\\), \\(C(X)\\in\\mathcal{C}\\)\nMany times we are actually more interested in the probabilities that \\(X\\) belongs to each category in \\(\\mathcal{C}\\)"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#example-credit-card-default",
    "href": "slides/04-logistic.qmd.html#example-credit-card-default",
    "title": "Chapter 4 Part 1",
    "section": "Example: Credit card default",
    "text": "Example: Credit card default\n\n\nCode\nset.seed(1)\nDefault |&gt;\n  sample_frac(size = 0.25) |&gt;\n  ggplot(aes(balance, income, color = default)) +\n  geom_point(pch = 4) +\n  scale_color_manual(values = c(\"cornflower blue\", \"red\")) +\n  theme_classic() +\n  theme(legend.position = \"top\") -&gt; p1\n\np2 &lt;- ggplot(Default, aes(x = default, y = balance, fill = default)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"cornflower blue\", \"red\")) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\np3 &lt;- ggplot(Default, aes(x = default, y = income, fill = default)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"cornflower blue\", \"red\")) +\n  theme_classic() +\n  theme(legend.position = \"none\")\ngrid.arrange(p1, p2, p3, ncol = 3, widths = c(2, 1, 1))"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#can-we-use-linear-regression",
    "href": "slides/04-logistic.qmd.html#can-we-use-linear-regression",
    "title": "Chapter 4 Part 1",
    "section": "Can we use linear regression?",
    "text": "Can we use linear regression?\nWe can code Default as\n\\[Y = \\begin{cases} 0 & \\textrm{if }\\texttt{No}\\\\ 1&\\textrm{if }\\texttt{Yes}\\end{cases}\\] Can we fit a linear regression of \\(Y\\) on \\(X\\) and classify as Yes if \\(\\hat{Y}&gt; 0.5\\)?\n\nIn this case of a binary outcome, linear regression is okay (it is equivalent to linear discriminant analysis, you can read more about that in your book!)\n\\(E[Y|X=x] = P(Y=1|X=x)\\), so it seems like this is a pretty good idea!\nThe problem: Linear regression can produce probabilities less than 0 or greater than 1 üò±"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#can-we-use-linear-regression-1",
    "href": "slides/04-logistic.qmd.html#can-we-use-linear-regression-1",
    "title": "Chapter 4 Part 1",
    "section": "Can we use linear regression?",
    "text": "Can we use linear regression?\nWe can code Default as\n\\[Y = \\begin{cases} 0 & \\textrm{if }\\texttt{No}\\\\ 1&\\textrm{if }\\texttt{Yes}\\end{cases}\\] Can we fit a linear regression of \\(Y\\) on \\(X\\) and classify as Yes if \\(\\hat{Y}&gt; 0.5\\)?\n\nWhat may do a better job?\n\n\nLogistic regression!"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#linear-versus-logistic-regression",
    "href": "slides/04-logistic.qmd.html#linear-versus-logistic-regression",
    "title": "Chapter 4 Part 1",
    "section": "Linear versus logistic regression",
    "text": "Linear versus logistic regression\n\n\nCode\nDefault &lt;- Default |&gt;\n  mutate(\n  p = glm(default ~ balance, data = Default, family = \"binomial\") |&gt;\n  predict(type = \"response\"),\n  p2 = lm(I(default == \"Yes\") ~ balance, data = Default) |&gt; predict(),\n  def = ifelse(default == \"Yes\", 1, 0)\n)\n\n\nDefault |&gt;\n  sample_frac(0.25) |&gt;\nggplot(aes(balance, p2)) +\n  geom_hline(yintercept = c(0, 1), lty = 2, size = 0.2) +\n  geom_line(color = \"cornflower blue\") +\n  geom_point(aes(balance, def), shape = \"|\", color = \"orange\") +\n  theme_classic() +\n  labs(y = \"probability of default\") -&gt; p1\n\nDefault |&gt;\n  sample_frac(0.25) |&gt;\nggplot(aes(balance, p)) +\n  geom_hline(yintercept = c(0, 1), lty = 2, size = 0.2) +\n  geom_line(color = \"cornflower blue\") +\n  geom_point(aes(balance, def), shape = \"|\", color = \"orange\") +\n  theme_classic() +\n  labs(y = \"probability of default\") -&gt; p2\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\nWhich does a better job at predicting the probability of default?\n\n\nThe orange marks represent the response \\(Y\\in\\{0,1\\}\\)"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#linear-regression",
    "href": "slides/04-logistic.qmd.html#linear-regression",
    "title": "Chapter 4 Part 1",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhat if we have \\(&gt;2\\) possible outcomes? For example, someone comes to the emergency room and we need to classify them according to their symptoms\n\\[\n\\begin{align}\nY = \\begin{cases} 1& \\textrm{if }\\texttt{stroke}\\\\2&\\textrm{if }\\texttt{drug overdose}\\\\3&\\textrm{if }\\texttt{epileptic seizure}\\end{cases}\n\\end{align}\n\\]\n\nWhat could go wrong here?\n\n\nThe coding implies an ordering\nThe coding implies equal spacing (that is the difference between stroke and drug overdose is the same as drug overdose and epileptic seizure)"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#linear-regression-1",
    "href": "slides/04-logistic.qmd.html#linear-regression-1",
    "title": "Chapter 4 Part 1",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhat if we have \\(&gt;2\\) possible outcomes? For example, someone comes to the emergency room and we need to classify them according to their symptoms\n\\[\n\\begin{align}\nY = \\begin{cases} 1& \\textrm{if }\\texttt{stroke}\\\\2&\\textrm{if }\\texttt{drug overdose}\\\\3&\\textrm{if }\\texttt{epileptic seizure}\\end{cases}\n\\end{align}\n\\]\n\nLinear regression is not appropriate here\nMutliclass logistic regression or discriminant analysis are more appropriate"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#logistic-regression",
    "href": "slides/04-logistic.qmd.html#logistic-regression",
    "title": "Chapter 4 Part 1",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\\[\np(X) = \\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}\n\\]\n\nNote: \\(p(X)\\) is shorthand for \\(P(Y=1|X)\\)\nNo matter what values \\(\\beta_0\\), \\(\\beta_1\\), or \\(X\\) take \\(p(X)\\) will always be between 0 and 1"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#logistic-regression-1",
    "href": "slides/04-logistic.qmd.html#logistic-regression-1",
    "title": "Chapter 4 Part 1",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\\[\np(X) = \\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}\n\\]\nWe can rearrange this into the following form:\n\\[\n\\log\\left(\\frac{p(X)}{1-p(X)}\\right) = \\beta_0 + \\beta_1 X\n\\]\n\nWhat is this transformation called?\n\n\nThis is a log odds or logit transformation of \\(p(X)\\)"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#linear-versus-logistic-regression-1",
    "href": "slides/04-logistic.qmd.html#linear-versus-logistic-regression-1",
    "title": "Chapter 4 Part 1",
    "section": "Linear versus logistic regression",
    "text": "Linear versus logistic regression\n\nLogistic regression ensures that our estimates for \\(p(X)\\) are between 0 and 1 üéâ"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#maximum-likelihood",
    "href": "slides/04-logistic.qmd.html#maximum-likelihood",
    "title": "Chapter 4 Part 1",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\nRefresher: How did we estimate \\(\\hat\\beta\\) in linear regression?"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#maximum-likelihood-1",
    "href": "slides/04-logistic.qmd.html#maximum-likelihood-1",
    "title": "Chapter 4 Part 1",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\nRefresher: How did we estimate \\(\\hat\\beta\\) in linear regression?\n\nIn logistic regression, we use maximum likelihood to estimate the parameters\n\\[\\mathcal{l}(\\beta_0,\\beta_1)=\\prod_{i:y_i=1}p(x_i)\\prod_{i:y_i=0}(1-p(x_i))\\]\n\nThis likelihood give the probability of the observed ones and zeros in the data\nWe pick \\(\\beta_0\\) and \\(\\beta_1\\) to maximize the likelihood\nWe‚Äôll let R do the heavy lifting here"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#lets-see-it-in-r",
    "href": "slides/04-logistic.qmd.html#lets-see-it-in-r",
    "title": "Chapter 4 Part 1",
    "section": "Let‚Äôs see it in R",
    "text": "Let‚Äôs see it in R\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(default ~ balance, \n      data = Default) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term         estimate std.error statistic   p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) -10.7      0.361        -29.5 3.62e-191\n2 balance       0.00550  0.000220      25.0 1.98e-137\n\n\n\nUse the logistic_reg() function in R with the glm engine"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#making-predictions",
    "href": "slides/04-logistic.qmd.html#making-predictions",
    "title": "Chapter 4 Part 1",
    "section": "Making predictions",
    "text": "Making predictions\n\nWhat is our estimated probability of default for someone with a balance of $1000?\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.6513306\n0.3611574\n-29.49221\n0\n\n\nbalance\n0.0054989\n0.0002204\n24.95309\n0\n\n\n\n\n\n\n\n\n\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}{1+e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}=\\frac{e^{-10.65+0.0055\\times 1000}}{1+e^{-10.65+0.0055\\times 1000}}=0.006\n\\]"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#making-predictions-1",
    "href": "slides/04-logistic.qmd.html#making-predictions-1",
    "title": "Chapter 4 Part 1",
    "section": "Making predictions",
    "text": "Making predictions\n\nWhat is our estimated probability of default for someone with a balance of $2000?\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.6513306\n0.3611574\n-29.49221\n0\n\n\nbalance\n0.0054989\n0.0002204\n24.95309\n0\n\n\n\n\n\n\n\n\n\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}{1+e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}=\\frac{e^{-10.65+0.0055\\times 2000}}{1+e^{-10.65+0.0055\\times 2000}}=0.586\n\\]"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#logistic-regression-example",
    "href": "slides/04-logistic.qmd.html#logistic-regression-example",
    "title": "Chapter 4 Part 1",
    "section": "Logistic regression example",
    "text": "Logistic regression example\nLet‚Äôs refit the model to predict the probability of default given the customer is a student\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-3.5041278\n0.0707130\n-49.554219\n0.0000000\n\n\nstudentYes\n0.4048871\n0.1150188\n3.520181\n0.0004313\n\n\n\n\n\n\n\n\n\\[P(\\texttt{default = Yes}|\\texttt{student = Yes}) = \\frac{e^{-3.5041+0.4049\\times1}}{1+e^{-3.5041+0.4049\\times1}}=0.0431\\]\n\n\nHow will this change if student = No?\n\n\n\n\\[P(\\texttt{default = Yes}|\\texttt{student = No}) = \\frac{e^{-3.5041+0.4049\\times0}}{1+e^{-3.5041+0.4049\\times0}}=0.0292\\]"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#potential-confounding",
    "href": "slides/04-logistic.qmd.html#potential-confounding",
    "title": "Chapter 4 Part 1",
    "section": "Potential Confounding",
    "text": "Potential Confounding\n\n\nWhat is going on here?"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#confounding",
    "href": "slides/04-logistic.qmd.html#confounding",
    "title": "Chapter 4 Part 1",
    "section": "Confounding",
    "text": "Confounding\n\n\nStudents tend to have higher balances than non-students\nTheir marginal default rate is higher\nFor each level of balance, students default less\nTheir conditional default rate is lower"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#a-bit-about-odds",
    "href": "slides/04-logistic.qmd.html#a-bit-about-odds",
    "title": "Chapter 4 Part 1",
    "section": "A bit about ‚Äúodds‚Äù",
    "text": "A bit about ‚Äúodds‚Äù\n\nThe ‚Äúodds‚Äù tell you how likely an event is\nüåÇ Let‚Äôs say there is a 60% chance of rain today * What is the probability that it will rain?\n\\(p = 0.6\\)\nWhat is the probability that it won‚Äôt rain?\n\\(1-p = 0.4\\)\nWhat are the odds that it will rain?\n3 to 2, 3:2, \\(\\frac{0.6}{0.4} = 1.5\\)"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#transforming-logs",
    "href": "slides/04-logistic.qmd.html#transforming-logs",
    "title": "Chapter 4 Part 1",
    "section": "Transforming logs",
    "text": "Transforming logs\n\nHow do you ‚Äúundo‚Äù a \\(\\log\\) base \\(e\\)?\nUse \\(e\\)! For example:\n\\(e^{\\log(10)} = 10\\)\n\\(e^{\\log(1283)} = 1283\\)\n\\(e^{\\log(x)} = x\\)"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#transforming-logs-1",
    "href": "slides/04-logistic.qmd.html#transforming-logs-1",
    "title": "Chapter 4 Part 1",
    "section": "Transforming logs",
    "text": "Transforming logs\n\nHow would you get the odds from the log(odds)?\n\n\n\nHow do you ‚Äúundo‚Äù a \\(\\log\\) base \\(e\\)?\nUse \\(e\\)! For example:\n\\(e^{\\log(10)} = 10\\)\n\\(e^{\\log(1283)} = 1283\\)\n\\(e^{\\log(x)} = x\\)\n\n\n\n\\(e^{\\log(odds)}\\) = odds"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#transforming-odds",
    "href": "slides/04-logistic.qmd.html#transforming-odds",
    "title": "Chapter 4 Part 1",
    "section": "Transforming odds",
    "text": "Transforming odds\n\nodds = \\(\\frac{\\pi}{1-\\pi}\\)\nSolving for \\(\\pi\\)\n\\(\\pi = \\frac{\\textrm{odds}}{1+\\textrm{odds}}\\)\nPlugging in \\(e^{\\log(odds)}\\) = odds\n\\(\\pi = \\frac{e^{\\log(odds)}}{1+e^{\\log(odds)}}\\)\nPlugging in \\(\\log(odds) = \\beta_0 + \\beta_1x\\)\n\\(\\pi = \\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}\\)"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#the-logistic-model",
    "href": "slides/04-logistic.qmd.html#the-logistic-model",
    "title": "Chapter 4 Part 1",
    "section": "The logistic model",
    "text": "The logistic model\n\n‚úåÔ∏è forms\n\n\n\n\n\n\n\n\nForm\nModel\n\n\n\n\nLogit form\n\\(\\log\\left(\\frac{\\pi}{1-\\pi}\\right) = \\beta_0 + \\beta_1x\\)\n\n\nProbability form\n\\(\\Large\\pi = \\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}\\)"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#the-logistic-model-1",
    "href": "slides/04-logistic.qmd.html#the-logistic-model-1",
    "title": "Chapter 4 Part 1",
    "section": "The logistic model",
    "text": "The logistic model\n\n\n\nprobability\nodds\nlog(odds)\n\n\n\n\n\\(\\pi\\)\n\\(\\frac{\\pi}{1-\\pi}\\)\n\\(\\log\\left(\\frac{\\pi}{1-\\pi}\\right)=l\\)\n\n\n\n‚¨ÖÔ∏è\n\n\n\nlog(odds)\nodds\nprobability\n\n\n\n\n\\(l\\)\n\\(e^l\\)\n\\(\\frac{e^l}{1+e^l} = \\pi\\)"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#the-logistic-model-2",
    "href": "slides/04-logistic.qmd.html#the-logistic-model-2",
    "title": "Chapter 4 Part 1",
    "section": "The logistic model",
    "text": "The logistic model\n\n‚úåÔ∏è forms\nlog(odds): \\(l = \\beta_0 + \\beta_1x\\)\nP(Outcome = Yes): \\(\\Large\\pi =\\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}\\)"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#odds-ratios",
    "href": "slides/04-logistic.qmd.html#odds-ratios",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later. (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#odds-ratios-1",
    "href": "slides/04-logistic.qmd.html#odds-ratios-1",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat is the explanatory variable?\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#odds-ratios-2",
    "href": "slides/04-logistic.qmd.html#odds-ratios-2",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat type of variable is this?\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#odds-ratios-3",
    "href": "slides/04-logistic.qmd.html#odds-ratios-3",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat is the outcome variable?\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#odds-ratios-4",
    "href": "slides/04-logistic.qmd.html#odds-ratios-4",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat type of variable is this?\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#odds-ratios-5",
    "href": "slides/04-logistic.qmd.html#odds-ratios-5",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\n\n\nTreatment\nTMS\nPlacebo\nTotal\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\nWe can compare the results using odds\nWhat are the odds of being pain-free for the placebo group?\n\\((22/100)/(78/100) = 22/78 = 0.282\\)\nWhat are the odds of being pain-free for the treatment group?\n\\(39/61 = 0.639\\)\nComparing the odds what can we conclude?\nTMS increases the likelihood of success"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#odds-ratios-6",
    "href": "slides/04-logistic.qmd.html#odds-ratios-6",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\n\n\nTreatment\nTMS\nPlacebo\nTotal\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\nWe can summarize this relationship with an odds ratio: the ratio of the two odds\n\\(\\Large OR = \\frac{39/61}{22/78} = \\frac{0.639}{0.282} = 2.27\\)\n‚Äúthe odds of being pain free were 2.27 times higher with TMS than with the placebo‚Äù"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#odds-ratios-7",
    "href": "slides/04-logistic.qmd.html#odds-ratios-7",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat if we wanted to calculate this in terms of Not pain-free (with pain-free) as the referent?\n\n\n\n\nTreatment\nTMS\nPlacebo\nTotal\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\n\\(\\Large OR = \\frac{61/39}{78/22} = \\frac{1.564}{3.545} = 0.441\\)\nthe odds for still being in pain for the TMS group are 0.441 times the odds of being in pain for the placebo group"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#odds-ratios-8",
    "href": "slides/04-logistic.qmd.html#odds-ratios-8",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat changed here?\n\n\n\n\nTreatment\nTMS\nPlacebo\nTotal\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\n\\(\\Large OR = \\frac{78/22}{61/39} = \\frac{3.545}{1.564} = 2.27\\)\nthe odds for still being in pain for the placebo group are 2.27 times the odds of being in pain for the TMS group"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#odds-ratios-9",
    "href": "slides/04-logistic.qmd.html#odds-ratios-9",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nIn general, it‚Äôs more natural to interpret odds ratios &gt; 1, you can flip the referent to do so\n\n\n\nTreatment\nTMS\nPlacebo\nTotal\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\\(\\Large OR = \\frac{78/22}{61/39} = \\frac{3.545}{1.564} = 2.27\\)\nthe odds for still being in pain for the placebo group are 2.27 times the odds of being in pain for the TMS group"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#odds-ratios-10",
    "href": "slides/04-logistic.qmd.html#odds-ratios-10",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nLet‚Äôs look at some Titanic data. We are interested in whether the passenger reported being female is related to whether they survived.\n\n\n\n\nFemale\nMale\nTotal\n\n\n\n\nSurvived\n308\n142\n450\n\n\nDied\n154\n709\n863\n\n\nTotal\n462\n851\n1313"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#odds-ratios-11",
    "href": "slides/04-logistic.qmd.html#odds-ratios-11",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat are the odds of surviving for females versus males?\n\n\n\n\n\nFemale\nMale\nTotal\n\n\n\n\nSurvived\n308\n142\n450\n\n\nDied\n154\n709\n863\n\n\nTotal\n462\n851\n1313\n\n\n\n\\[\\Large OR = \\frac{308/154}{142/709} = \\frac{2}{0.2} = 9.99\\]"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#odds-ratios-12",
    "href": "slides/04-logistic.qmd.html#odds-ratios-12",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nHow do you interpret this?\n\n\n\n\n\nFemale\nMale\nTotal\n\n\n\n\nSurvived\n308\n142\n450\n\n\nDied\n154\n709\n863\n\n\nTotal\n462\n851\n1313\n\n\n\n\\[\\Large OR = \\frac{308/154}{142/709} = \\frac{2}{0.2} = 9.99\\] the odds of surviving for the female passengers was 9.99 times the odds of surviving for the male passengers"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#odds-ratios-13",
    "href": "slides/04-logistic.qmd.html#odds-ratios-13",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat if we wanted to fit a model? What would the equation be?\n\n\n\n\n\nFemale\nMale\nTotal\n\n\n\n\nSurvived\n308\n142\n450\n\n\nDied\n154\n709\n863\n\n\nTotal\n462\n851\n1313\n\n\n\n\n\\[\\Large \\log(\\textrm{odds of survival}) = \\beta_0 + \\beta_1 \\textrm{Female}\\]"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#odds-ratios-14",
    "href": "slides/04-logistic.qmd.html#odds-ratios-14",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\\[\\Large \\log(\\textrm{odds of survival}) = \\beta_0 + \\beta_1 \\textrm{Female}\\]\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Survived ~ Sex, data = Titanic) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -1.61    0.0919     -17.5 1.70e-68\n2 Sexfemale       2.30    0.135       17.1 2.91e-65"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#odds-ratios-15",
    "href": "slides/04-logistic.qmd.html#odds-ratios-15",
    "title": "Chapter 4 Part 1",
    "section": "Odds Ratios",
    "text": "Odds Ratios\n\nHow do you interpret this result?\n\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Survived ~ Sex, data = Titanic) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -1.61    0.0919     -17.5 1.70e-68\n2 Sexfemale       2.30    0.135       17.1 2.91e-65"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#odds-ratios-16",
    "href": "slides/04-logistic.qmd.html#odds-ratios-16",
    "title": "Chapter 4 Part 1",
    "section": "Odds Ratios",
    "text": "Odds Ratios\n\nHow do you interpret this result?\n\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Survived ~ Sex, data = Titanic) |&gt;\n  tidy(exponentiate = TRUE) \n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.200    0.0919     -17.5 1.70e-68\n2 Sexfemale      9.99     0.135       17.1 2.91e-65\n\nexp(2.301176)\n\n[1] 9.99"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#odds-ratios-17",
    "href": "slides/04-logistic.qmd.html#odds-ratios-17",
    "title": "Chapter 4 Part 1",
    "section": "Odds Ratios",
    "text": "Odds Ratios\n\nHow do you interpret this result?\n\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Survived ~ Sex, data = Titanic) |&gt;\n  tidy(exponentiate = TRUE) \n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.200    0.0919     -17.5 1.70e-68\n2 Sexfemale      9.99     0.135       17.1 2.91e-65\n\nexp(2.301176)\n\n[1] 9.99\n\n\nthe odds of surviving for the female passengers was 9.99 times the odds of surviving for the male passengers"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#odds-ratios-18",
    "href": "slides/04-logistic.qmd.html#odds-ratios-18",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nWhat if the explanatory variable is continuous?\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Acceptance ~ GPA, data = MedGPA) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -19.2       5.63     -3.41 0.000644\n2 GPA             5.45      1.58      3.45 0.000553\n\n\nA one unit increase in GPA yields a 5.45 increase in the log odds of acceptance"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#odds-ratios-19",
    "href": "slides/04-logistic.qmd.html#odds-ratios-19",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nWhat if the explanatory variable is continuous?\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Acceptance ~ GPA, data = MedGPA) |&gt;\n  tidy(exponentiate = TRUE) \n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  4.56e-9      5.63     -3.41 0.000644\n2 GPA          2.34e+2      1.58      3.45 0.000553\n\n\nA one unit increase in GPA yields a 234-fold increase in the odds of acceptance\n\nüò± that seems huge! Remember: the interpretation of these coefficients depends on your units (the same as in ordinary linear regression)."
  },
  {
    "objectID": "slides/04-logistic.qmd.html#odds-ratios-20",
    "href": "slides/04-logistic.qmd.html#odds-ratios-20",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nHow could we get the odds associated with increasing GPA by 0.1?\n\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Acceptance ~ GPA, data = MedGPA) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -19.2       5.63     -3.41 0.000644\n2 GPA             5.45      1.58      3.45 0.000553\n\n\n\nexp(5.454) ## a one unit increase in GPA\n\n[1] 234\n\nexp(5.454 * 0.1) ## a 0.1 increase in GPA\n\n[1] 1.73\n\n\nA one-tenth unit increase in GPA yields a 1.73-fold increase in the odds of acceptance"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#odds-ratios-21",
    "href": "slides/04-logistic.qmd.html#odds-ratios-21",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nHow could we get the odds associated with increasing GPA by 0.1?\n\n\nMedGPA &lt;- MedGPA |&gt;\n  mutate(GPA_10 = GPA * 10)\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Acceptance ~ GPA_10, data = MedGPA) |&gt;\n  tidy(exponentiate = TRUE)\n\n# A tibble: 2 √ó 5\n  term             estimate std.error statistic  p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) 0.00000000456     5.63      -3.41 0.000644\n2 GPA_10      1.73              0.158      3.45 0.000553\n\n\nA one-tenth unit increase in GPA yields a 1.73-fold increase in the odds of acceptance"
  },
  {
    "objectID": "slides/04-logistic.qmd.html#application-exercise",
    "href": "slides/04-logistic.qmd.html#application-exercise",
    "title": "Chapter 4 Part 1",
    "section": " Application Exercise",
    "text": "Application Exercise\nUsing the Default data from the ISLR package. Fit two logistic regression models that predict whether a customer defaults\n\nOne model with student as a predictor. Interpret the coefficient of studentYes.\nAnother model with balance as a predictor. Interpret the coefficient of balance.\n\nHere is some code to get you started:\n\nlibrary(ISLR)\ndata(\"Default\")\n\n\n\n\n\nüîó https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "slides/06-subset_o.html#application-exercise-1",
    "href": "slides/06-subset_o.html#application-exercise-1",
    "title": "Chapter 6 Part 1",
    "section": " Application Exercise",
    "text": "Application Exercise\nPerform forward stepwise and backward stepwise on the penguins dataset to find the best model to predict weight."
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html",
    "href": "slides/04-2-lda-qda_o.qmd.html",
    "title": "Chapter 4 Part 2",
    "section": "",
    "text": "We had a logistic regression refresher\nNow‚Ä¶\n\nWhat if our response has more than two levels?\nWhat if logistic regression is a poor fit?"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#recap",
    "href": "slides/04-2-lda-qda_o.qmd.html#recap",
    "title": "Chapter 4 Part 2",
    "section": "",
    "text": "We had a logistic regression refresher\nNow‚Ä¶\n\nWhat if our response has more than two levels?\nWhat if logistic regression is a poor fit?"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#setup",
    "href": "slides/04-2-lda-qda_o.qmd.html#setup",
    "title": "Chapter 4 Part 2",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR)\nlibrary(Stat2Data)\n#install.packages(\"discrim\")"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#multinomial-logistic",
    "href": "slides/04-2-lda-qda_o.qmd.html#multinomial-logistic",
    "title": "Chapter 4 Part 2",
    "section": "Multinomial Logistic",
    "text": "Multinomial Logistic\n\nSo far we have discussed logistic regression with two classes.\nIt is easily generalized to more than two classes."
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#confounding",
    "href": "slides/04-2-lda-qda_o.qmd.html#confounding",
    "title": "Chapter 4 Part 2",
    "section": "Confounding",
    "text": "Confounding\nRecall our defaults data with variable default, student, and balance\n\n\n\n\n\n\n\n\n\n\nWhat is going on here?"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#confounding-1",
    "href": "slides/04-2-lda-qda_o.qmd.html#confounding-1",
    "title": "Chapter 4 Part 2",
    "section": "Confounding",
    "text": "Confounding\n\n\n\n\n\n\n\n\n\n\nStudents tend to have higher balances than non-students\nTheir marginal default rate is higher\nFor each level of balance, students default less\nTheir conditional default rate is lower"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#multiple-logistic-regression",
    "href": "slides/04-2-lda-qda_o.qmd.html#multiple-logistic-regression",
    "title": "Chapter 4 Part 2",
    "section": "Multiple logistic regression",
    "text": "Multiple logistic regression\n\\[\\log\\left(\\frac{p(X)}{1-p(X)}\\right)=\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p\\] \\[p(X) = \\frac{e^{\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p}}{1+e^{\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p}}\\]\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.8690452\n0.4922555\n-22.080088\n0.0000000\n\n\nbalance\n0.0057365\n0.0002319\n24.737563\n0.0000000\n\n\nincome\n0.0000030\n0.0000082\n0.369815\n0.7115203\n\n\nstudentYes\n-0.6467758\n0.2362525\n-2.737646\n0.0061881\n\n\n\n\n\n\n\n\n\nWhy is the coefficient for student negative now when it was positive before?"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#logistic-regression-for-more-than-two-classes",
    "href": "slides/04-2-lda-qda_o.qmd.html#logistic-regression-for-more-than-two-classes",
    "title": "Chapter 4 Part 2",
    "section": "Logistic regression for more than two classes",
    "text": "Logistic regression for more than two classes\n\\[P(Y=k|X) = \\frac{e ^{\\beta_{0k}+\\beta_{1k}X_1+\\dots+\\beta_{pk}X_p}}{\\sum_{l=1}^Ke^{\\beta_{0l}+\\beta_{1l}X_1+\\dots+\\beta_{pl}X_p}}\\]\n\nWe generalize this to situations with multiple classes\nHere we have a linear function for each of the \\(K\\) classes\nThis is known as multinomial logistic regression"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#lda-warmup",
    "href": "slides/04-2-lda-qda_o.qmd.html#lda-warmup",
    "title": "Chapter 4 Part 2",
    "section": "LDA Warmup",
    "text": "LDA Warmup\nTo give us a general overview, we are going to watch the StatQuest video on the topic: https://www.youtube.com/watch?v=azXCzI57Yfc"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#discriminant-analysis",
    "href": "slides/04-2-lda-qda_o.qmd.html#discriminant-analysis",
    "title": "Chapter 4 Part 2",
    "section": "Discriminant Analysis",
    "text": "Discriminant Analysis\n\nHere the approach is to model the distribution of X in each of the classes separately, and then use Bayes theorem to flip things around and obtain \\(P(Y|X)\\).\nWhen we use normal (Gaussian) distributions for each class, this leads to linear or quadratic discriminant analysis.\nHowever, this approach is quite general, and other distributions can be used as well. We will focus on normal distributions."
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#why-another-approach",
    "href": "slides/04-2-lda-qda_o.qmd.html#why-another-approach",
    "title": "Chapter 4 Part 2",
    "section": "Why Another Approach?",
    "text": "Why Another Approach?\n\nWhen the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.\nIf n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\nLinear discriminant analysis is popular when we have more than two response classes, because it also provides low-dimensional views of the data."
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#bayes-theorem-classification",
    "href": "slides/04-2-lda-qda_o.qmd.html#bayes-theorem-classification",
    "title": "Chapter 4 Part 2",
    "section": "Bayes Theorem (classification)",
    "text": "Bayes Theorem (classification)\nThomas Bayes was a famous mathematician whose name represents a big subfield of statistical and probabilistic modeling. Here we focus on a simple result, known as Bayes theorem:\n\\[P(Y=k|X=x) = \\frac{P(X=x|Y=k)\\cdot P(Y=k)}{P(X=x)}\\]"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#bayes-for-discriminant-analysis",
    "href": "slides/04-2-lda-qda_o.qmd.html#bayes-for-discriminant-analysis",
    "title": "Chapter 4 Part 2",
    "section": "Bayes for Discriminant Analysis",
    "text": "Bayes for Discriminant Analysis\n\\[P(Y=k|X=x) = \\frac{\\pi_kf_k(x)}{\\sum_{l=1}^K\\pi_lf_l(x)} \\text{, where}\\]\n\n\\(f_k(x)=P(X=x|Y=k)\\) is the density for \\(X\\) in class \\(k\\). Here we use normal‚Äôs but they could be other distributions (such as \\(\\chi^2\\))\n\\(\\pi_k = P(Y=k)\\) is the marginal or prior probability for class \\(k\\)."
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#classify-to-the-highest-density",
    "href": "slides/04-2-lda-qda_o.qmd.html#classify-to-the-highest-density",
    "title": "Chapter 4 Part 2",
    "section": "Classify to the highest density",
    "text": "Classify to the highest density\n\\[\\pi_1=.5, \\pi_2=.5\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe classify a new point according to which density is highest.\nWhen the priors are different, we take them into account as well, and compare \\(\\pi_kf_k(x)\\).\nOn the right, we favor the pink class - the decision boundary has shifted to the left."
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#lda-when-p1",
    "href": "slides/04-2-lda-qda_o.qmd.html#lda-when-p1",
    "title": "Chapter 4 Part 2",
    "section": "LDA (when \\(p=1\\))",
    "text": "LDA (when \\(p=1\\))\nThe Gaussian (normal) density has the form\n\\[f_k(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k}e^{-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma_k})^2}\\]\n\n\\(\\mu_k\\) is the mean, \\(\\sigma_k^2\\) the variance (in class \\(k\\))\nFor now, we assume \\(\\sigma_k=\\sigma\\) for all groups (we will need to check this with real data)"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#lda-when-p1-1",
    "href": "slides/04-2-lda-qda_o.qmd.html#lda-when-p1-1",
    "title": "Chapter 4 Part 2",
    "section": "LDA (when \\(p=1\\))",
    "text": "LDA (when \\(p=1\\))\nWe plug this \\(f_k(x)\\) into Bayes formula and after some simplifying we get:\n\\[p_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma_k})^2}}{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma_k})^2}}\\]"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#discriminant-function",
    "href": "slides/04-2-lda-qda_o.qmd.html#discriminant-function",
    "title": "Chapter 4 Part 2",
    "section": "Discriminant Function",
    "text": "Discriminant Function\nTo classify at the value X = x, we need to see which of the \\(p_k(x)\\) is largest. Taking logs, and discarding terms that do not depend on \\(k\\), we see that this is equivalent to assigning x to the class with the largest discriminant score:\n\\[\\delta_k(x) = x\\cdot \\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+log(\\pi_k)\\]\n\nImportantly, \\(\\delta_k(x)\\) is a linear function of \\(x\\).\nIf there are \\(K=2\\) classes and \\(\\pi_1=\\pi_2=.5\\), then the decision boundry is at\n\n\\[x=\\frac{\\mu_1+\\mu_2}{2}\\]"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#maximizing-delta_kx",
    "href": "slides/04-2-lda-qda_o.qmd.html#maximizing-delta_kx",
    "title": "Chapter 4 Part 2",
    "section": "Maximizing \\(\\delta_k(x)\\)",
    "text": "Maximizing \\(\\delta_k(x)\\)\n\nIn order to maximize this, we need estimates for all the parameters\n\n\nWhat should we estimate \\(\\hat{\\pi_k}\\), \\(\\mu_k\\), and \\(\\sigma^2\\) with?"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#maximizing-delta_kx-1",
    "href": "slides/04-2-lda-qda_o.qmd.html#maximizing-delta_kx-1",
    "title": "Chapter 4 Part 2",
    "section": "Maximizing \\(\\delta_k(x)\\)",
    "text": "Maximizing \\(\\delta_k(x)\\)\n\\[\\hat{\\pi}_k = \\frac{n_k}{n}\\]\n\\[\\hat{\\mu}_k = \\frac{1}{n_k}\\sum_{i:y_k=k}x_i\\]\n\\[\\hat{\\sigma}^2 = \\frac{1}{n-K}\\sum_{k=1}^K\\sum_{i:y_i=k}(x_i-\\hat{\\mu}_k)^2 = \\sum_{k=1}^K\\frac{n_k-1}{n-K}\\cdot \\hat{\\sigma}_k^2\\]\nWhere \\[\\hat{\\sigma}_k^2 = \\frac{1}{n_k-1}\\sum_{i:y_i=k}(x_i-\\hat{\\mu}_k)^2\\]"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#lda-in-r-example-p1",
    "href": "slides/04-2-lda-qda_o.qmd.html#lda-in-r-example-p1",
    "title": "Chapter 4 Part 2",
    "section": "LDA In R Example (\\(p=1\\))",
    "text": "LDA In R Example (\\(p=1\\))\n\ndata(\"BlueJays\") # Bring data into environment\nlibrary(Stat2Data)\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(discrim)\n\n\nCan we determine the sex of a blue jay by measuring the distance from the tip of the bill to the back of the head (Head)?"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#lda-bluejays-eda",
    "href": "slides/04-2-lda-qda_o.qmd.html#lda-bluejays-eda",
    "title": "Chapter 4 Part 2",
    "section": "LDA Bluejays EDA",
    "text": "LDA Bluejays EDA\n\ntt_split &lt;- initial_split(BlueJays,prop=.7)\n\nBJ_train &lt;- training(tt_split)\n\nBJ_train |&gt; ggplot(aes(x = Head,y=KnownSex)) +\n  geom_boxplot() +\n  theme_bw()"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#lda-bluejays-eda-1",
    "href": "slides/04-2-lda-qda_o.qmd.html#lda-bluejays-eda-1",
    "title": "Chapter 4 Part 2",
    "section": "LDA Bluejays EDA",
    "text": "LDA Bluejays EDA\n\nWhat assumptions should we check?\n\n\nNormality of our predictor\nConstant variance between groups."
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#lda-bluejays-eda---normality",
    "href": "slides/04-2-lda-qda_o.qmd.html#lda-bluejays-eda---normality",
    "title": "Chapter 4 Part 2",
    "section": "LDA Bluejays EDA - Normality",
    "text": "LDA Bluejays EDA - Normality\n\nlibrary(patchwork)\n\n(BJ_train |&gt; ggplot(aes(sample = Head)) + geom_qq())+\n  (BJ_train |&gt; ggplot(aes(x = Head)) + geom_histogram())"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#lda-bluejays-eda---variance",
    "href": "slides/04-2-lda-qda_o.qmd.html#lda-bluejays-eda---variance",
    "title": "Chapter 4 Part 2",
    "section": "LDA Bluejays EDA - Variance",
    "text": "LDA Bluejays EDA - Variance\n\nBJ_train |&gt; group_by(KnownSex)|&gt;\n  summarize(Head_sd = sd(Head))\n\n# A tibble: 2 √ó 2\n  KnownSex Head_sd\n  &lt;fct&gt;      &lt;dbl&gt;\n1 F           1.27\n2 M           1.25\n\n\n\nVery similar standard deviations (and thus variances)"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#fit-lda",
    "href": "slides/04-2-lda-qda_o.qmd.html#fit-lda",
    "title": "Chapter 4 Part 2",
    "section": "Fit LDA",
    "text": "Fit LDA\n\nlda_spec &lt;- discrim_linear() |&gt;\n  set_mode(\"classification\")|&gt;\n  set_engine(\"MASS\")\n\nlda_fit&lt;-  lda_spec |&gt; \n  fit(KnownSex ~ Head,\n      data = BJ_train)"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#check-the-fit",
    "href": "slides/04-2-lda-qda_o.qmd.html#check-the-fit",
    "title": "Chapter 4 Part 2",
    "section": "Check The Fit",
    "text": "Check The Fit\n\nBJTest &lt;- testing(tt_split)\n\nlda_fit |&gt; \n  augment(new_data = BJTest) %&gt;%\n  conf_mat(truth = KnownSex, estimate = .pred_class) \n\n          Truth\nPrediction  F  M\n         F 10  1\n         M  5 21\n\n\n\nlda_fit|&gt; \n  augment(new_data = BJTest) |&gt; \n  accuracy(truth = KnownSex,estimate=.pred_class)\n\n# A tibble: 1 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.838"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#compare-to-logistic",
    "href": "slides/04-2-lda-qda_o.qmd.html#compare-to-logistic",
    "title": "Chapter 4 Part 2",
    "section": "Compare to Logistic",
    "text": "Compare to Logistic\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(KnownSex ~ Head,\n      data = BJ_train) |&gt;\n  augment(new_data = BJTest)|&gt;\n    accuracy(truth = KnownSex,estimate=.pred_class)\n\n# A tibble: 1 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.865"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#application-exercise",
    "href": "slides/04-2-lda-qda_o.qmd.html#application-exercise",
    "title": "Chapter 4 Part 2",
    "section": " Application Exercise",
    "text": "Application Exercise\nWe are going to use the penguins data from the palmerpeguins package.\n\nConduct basic EDA to see if penguin bill length is different by sex and if LDA is an appropriate model choice.\nUse LDA to predict penguin sex based on their bill length using training and testing data. Get the accuracy on the testing set.\nFit a logistic regression model and get it‚Äôs accuracy to see which did better."
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#lda-with-p1",
    "href": "slides/04-2-lda-qda_o.qmd.html#lda-with-p1",
    "title": "Chapter 4 Part 2",
    "section": "LDA with \\(p>1\\)",
    "text": "LDA with \\(p&gt;1\\)\n\nWhen we have 2 or more predictors, the distribution becomes multivariate.\nIf the covariance between predictors is 0 within each class of the response, LDA is still appropriate."
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#lda-with-p1-1",
    "href": "slides/04-2-lda-qda_o.qmd.html#lda-with-p1-1",
    "title": "Chapter 4 Part 2",
    "section": "LDA with \\(p>1\\)",
    "text": "LDA with \\(p&gt;1\\)\n\nFor example, with 2 normal predictors, their distributions in 3d would like like this:\n\n\n\nLuckily, the discriminate function remains linear\n\n\\[\\delta_k(x) = c_{k0} + c_{k1}x_1+...c_{kp}x_p\\]"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#example-p2k3",
    "href": "slides/04-2-lda-qda_o.qmd.html#example-p2k3",
    "title": "Chapter 4 Part 2",
    "section": "Example: \\(p=2,K=3\\)",
    "text": "Example: \\(p=2,K=3\\)\n\nThere is no limit on the number of levels of the categorical response for LDA\nSuppose \\(\\pi_1=\\pi_2\\pi_3=1/3\\)\n\n - The dashed lines are known as the Bayes decision boundaries"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#probabilities",
    "href": "slides/04-2-lda-qda_o.qmd.html#probabilities",
    "title": "Chapter 4 Part 2",
    "section": "Probabilities",
    "text": "Probabilities\n\nOnce the estimates for the \\(\\hat{\\delta}_k(x)\\) have been found, we can plug them in and get:\n\\[\\hat{P}(Y=k|X=x)=\\frac{e^{\\hat{\\delta}_k(x)}}{\\sum_{l=1}^Ke^{\\hat{\\delta}^l(x)}}\\]\nClassifying to the largest \\(\\hat{\\delta}_k(x)\\) amounts to classifying to the class for which \\(\\hat{P}(Y = k|X = x)\\) is largest.\nWhen \\(K = 2\\), we classify to class 2 if \\(\\hat{P}(Y = 2|X = x)\\geq 0.5\\), else to class 1."
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#example---credit-card-fraud",
    "href": "slides/04-2-lda-qda_o.qmd.html#example---credit-card-fraud",
    "title": "Chapter 4 Part 2",
    "section": "Example - Credit Card Fraud",
    "text": "Example - Credit Card Fraud\n\n\n          Truth\nPrediction   No  Yes\n       No  2889   83\n       Yes    2   26\n\n\n# A tibble: 1 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.972\n\n\n\nAccuracy of 97.7% on a testing set!\nWhat about the different types of errors?"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#errors",
    "href": "slides/04-2-lda-qda_o.qmd.html#errors",
    "title": "Chapter 4 Part 2",
    "section": "Errors",
    "text": "Errors\n\nOf the true yes, we made errors at the rate of 83/(83+26), 76%\nOf the true no, we made errors at the rate of 2/(2889+2), .069%"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#types-of-errors",
    "href": "slides/04-2-lda-qda_o.qmd.html#types-of-errors",
    "title": "Chapter 4 Part 2",
    "section": "Types of Errors",
    "text": "Types of Errors\n\nRecall:\n\nFalse positive rate: The fraction of negative examples that are classified as positive.\nFalse negative rate: The fraction of positive examples that are classified as negative.\n\nRemember the model gave probabilities, the final yes or no is decided by\n\\[\\hat{P}(Default=Yes|Balance,Student) \\geq .5\\]\n\nWe can adjust our error rates by changing that threshold"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#sensitivity-and-specificity",
    "href": "slides/04-2-lda-qda_o.qmd.html#sensitivity-and-specificity",
    "title": "Chapter 4 Part 2",
    "section": "Sensitivity and Specificity",
    "text": "Sensitivity and Specificity\n\n\n          Truth\nPrediction   No  Yes\n       No  2889   83\n       Yes    2   26\n\n\n\nThe sensitivity is the true positive rate.\n\nThe rate at which we correctly predict a person will default.\n26/(83+26) = .24 (24%)\n\nThe specificity is the true negative rate.\n\nThe rate at which we correctly predict a person will not default.\n2889/(2889+2) =.999 (99.9%)"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#varying-the-threshold",
    "href": "slides/04-2-lda-qda_o.qmd.html#varying-the-threshold",
    "title": "Chapter 4 Part 2",
    "section": "Varying the threshold",
    "text": "Varying the threshold\n\nIn order to determine the best threshold, we want to maximize the specificty and sensitivity.\nOR - maximize the sensitivty and minimize 1-specificity.\nIn order to to do this we use an \\(ROC\\) curve which stands for receiver operating characteristic curve."
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#roc-curve",
    "href": "slides/04-2-lda-qda_o.qmd.html#roc-curve",
    "title": "Chapter 4 Part 2",
    "section": "ROC Curve",
    "text": "ROC Curve\nWe want to maximize the area under this curve, called ROC AUC\n\nlda_roc&lt;- lda_fit_2 |&gt;\n  augment(new_data = testing(def_splits)) |&gt;\n  roc_curve(truth = default,.pred_No) \n\nhead(lda_roc)\n\n# A tibble: 6 √ó 3\n  .threshold specificity sensitivity\n       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1  -Inf          0                 1\n2     0.0819     0                 1\n3     0.129      0.00917           1\n4     0.151      0.0183            1\n5     0.154      0.0275            1\n6     0.201      0.0367            1"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#roc-curve-1",
    "href": "slides/04-2-lda-qda_o.qmd.html#roc-curve-1",
    "title": "Chapter 4 Part 2",
    "section": "ROC Curve",
    "text": "ROC Curve\n\nautoplot(lda_roc)"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#roc-auc",
    "href": "slides/04-2-lda-qda_o.qmd.html#roc-auc",
    "title": "Chapter 4 Part 2",
    "section": "ROC AUC",
    "text": "ROC AUC\n\nlda_fit_2 |&gt;\n  augment(new_data = testing(def_splits)) |&gt;\n  roc_auc(truth = default,.pred_No) \n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.957\n\n\nIf we want to tune our model better, we can optimize the ROC AUC by changing the threshold.\n\nI did a train/test approach here. What should I do different if I want to tune for threshold?"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#qda",
    "href": "slides/04-2-lda-qda_o.qmd.html#qda",
    "title": "Chapter 4 Part 2",
    "section": "QDA",
    "text": "QDA\n\nQDA arises when \\(p&gt;1\\) and the is a covariance structure between the predictors within the same level of the response.\nThis introduces a squared term into the maximization problem of the \\(\\delta_k(x)\\), thus the name."
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#qda-in-r",
    "href": "slides/04-2-lda-qda_o.qmd.html#qda-in-r",
    "title": "Chapter 4 Part 2",
    "section": "QDA In R",
    "text": "QDA In R\n\nqda_fit&lt;-discrim_quad() |&gt;\n  set_mode(\"classification\")|&gt;\n  set_engine(\"MASS\")|&gt; \n  fit(default ~ balance + student,\n      data = training(def_splits))\n\nqda_fit |&gt;\n  augment(new_data = testing(def_splits)) %&gt;%\n  conf_mat(truth = default, estimate = .pred_class) \n\n          Truth\nPrediction   No  Yes\n       No  2888   80\n       Yes    3   29\n\nqda_fit |&gt;\n  augment(new_data = testing(def_splits)) %&gt;%\n  roc_auc(truth = default,.pred_No) \n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.957"
  },
  {
    "objectID": "slides/04-2-lda-qda_o.qmd.html#tuning-by-threhold",
    "href": "slides/04-2-lda-qda_o.qmd.html#tuning-by-threhold",
    "title": "Chapter 4 Part 2",
    "section": "Tuning By Threhold",
    "text": "Tuning By Threhold\nhttps://www.tidymodels.org/start/case-study/"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html",
    "href": "slides/04-logistic_o.qmd.html",
    "title": "Chapter 4 Part 1",
    "section": "",
    "text": "We had a linear regression refresher\nLinear regression is a great tool when we have a continuous outcome\nWe are going to learn some fancy ways to do even better in the future\n\nSetup:\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR)"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#recap",
    "href": "slides/04-logistic_o.qmd.html#recap",
    "title": "Chapter 4 Part 1",
    "section": "",
    "text": "We had a linear regression refresher\nLinear regression is a great tool when we have a continuous outcome\nWe are going to learn some fancy ways to do even better in the future\n\nSetup:\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR)"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#classification-1",
    "href": "slides/04-logistic_o.qmd.html#classification-1",
    "title": "Chapter 4 Part 1",
    "section": "Classification",
    "text": "Classification\n\nWhat are some examples of classification problems?\n\n\nQualitative response variable in an unordered set, \\(\\mathcal{C}\\)\neye color \\(\\in\\) {blue, brown, green}\nemail \\(\\in\\) {spam, not spam}\nResponse, \\(Y\\) takes on values in \\(\\mathcal{C}\\)\nPredictors are a vector, \\(X\\)\nThe task: build a function \\(C(X)\\) that takes \\(X\\) and predicts \\(Y\\), \\(C(X)\\in\\mathcal{C}\\)\nMany times we are actually more interested in the probabilities that \\(X\\) belongs to each category in \\(\\mathcal{C}\\)"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#example-credit-card-default",
    "href": "slides/04-logistic_o.qmd.html#example-credit-card-default",
    "title": "Chapter 4 Part 1",
    "section": "Example: Credit card default",
    "text": "Example: Credit card default\n\n\nCode\nset.seed(1)\nDefault |&gt;\n  sample_frac(size = 0.25) |&gt;\n  ggplot(aes(balance, income, color = default)) +\n  geom_point(pch = 4) +\n  scale_color_manual(values = c(\"cornflower blue\", \"red\")) +\n  theme_classic() +\n  theme(legend.position = \"top\") -&gt; p1\n\np2 &lt;- ggplot(Default, aes(x = default, y = balance, fill = default)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"cornflower blue\", \"red\")) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\np3 &lt;- ggplot(Default, aes(x = default, y = income, fill = default)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"cornflower blue\", \"red\")) +\n  theme_classic() +\n  theme(legend.position = \"none\")\ngrid.arrange(p1, p2, p3, ncol = 3, widths = c(2, 1, 1))"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#can-we-use-linear-regression",
    "href": "slides/04-logistic_o.qmd.html#can-we-use-linear-regression",
    "title": "Chapter 4 Part 1",
    "section": "Can we use linear regression?",
    "text": "Can we use linear regression?\nWe can code Default as\n\\[Y = \\begin{cases} 0 & \\textrm{if }\\texttt{No}\\\\ 1&\\textrm{if }\\texttt{Yes}\\end{cases}\\] Can we fit a linear regression of \\(Y\\) on \\(X\\) and classify as Yes if \\(\\hat{Y}&gt; 0.5\\)?\n\nIn this case of a binary outcome, linear regression is okay (it is equivalent to linear discriminant analysis, you can read more about that in your book!)\n\\(E[Y|X=x] = P(Y=1|X=x)\\), so it seems like this is a pretty good idea!\nThe problem: Linear regression can produce probabilities less than 0 or greater than 1 üò±"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#can-we-use-linear-regression-1",
    "href": "slides/04-logistic_o.qmd.html#can-we-use-linear-regression-1",
    "title": "Chapter 4 Part 1",
    "section": "Can we use linear regression?",
    "text": "Can we use linear regression?\nWe can code Default as\n\\[Y = \\begin{cases} 0 & \\textrm{if }\\texttt{No}\\\\ 1&\\textrm{if }\\texttt{Yes}\\end{cases}\\] Can we fit a linear regression of \\(Y\\) on \\(X\\) and classify as Yes if \\(\\hat{Y}&gt; 0.5\\)?\n\nWhat may do a better job?\n\n\nLogistic regression!"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#linear-versus-logistic-regression",
    "href": "slides/04-logistic_o.qmd.html#linear-versus-logistic-regression",
    "title": "Chapter 4 Part 1",
    "section": "Linear versus logistic regression",
    "text": "Linear versus logistic regression\n\n\nCode\nDefault &lt;- Default |&gt;\n  mutate(\n  p = glm(default ~ balance, data = Default, family = \"binomial\") |&gt;\n  predict(type = \"response\"),\n  p2 = lm(I(default == \"Yes\") ~ balance, data = Default) |&gt; predict(),\n  def = ifelse(default == \"Yes\", 1, 0)\n)\n\n\nDefault |&gt;\n  sample_frac(0.25) |&gt;\nggplot(aes(balance, p2)) +\n  geom_hline(yintercept = c(0, 1), lty = 2, size = 0.2) +\n  geom_line(color = \"cornflower blue\") +\n  geom_point(aes(balance, def), shape = \"|\", color = \"orange\") +\n  theme_classic() +\n  labs(y = \"probability of default\") -&gt; p1\n\nDefault |&gt;\n  sample_frac(0.25) |&gt;\nggplot(aes(balance, p)) +\n  geom_hline(yintercept = c(0, 1), lty = 2, size = 0.2) +\n  geom_line(color = \"cornflower blue\") +\n  geom_point(aes(balance, def), shape = \"|\", color = \"orange\") +\n  theme_classic() +\n  labs(y = \"probability of default\") -&gt; p2\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\nWhich does a better job at predicting the probability of default?\n\n\nThe orange marks represent the response \\(Y\\in\\{0,1\\}\\)"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#linear-regression",
    "href": "slides/04-logistic_o.qmd.html#linear-regression",
    "title": "Chapter 4 Part 1",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhat if we have \\(&gt;2\\) possible outcomes? For example, someone comes to the emergency room and we need to classify them according to their symptoms\n\\[\n\\begin{align}\nY = \\begin{cases} 1& \\textrm{if }\\texttt{stroke}\\\\2&\\textrm{if }\\texttt{drug overdose}\\\\3&\\textrm{if }\\texttt{epileptic seizure}\\end{cases}\n\\end{align}\n\\]\n\nWhat could go wrong here?\n\n\nThe coding implies an ordering\nThe coding implies equal spacing (that is the difference between stroke and drug overdose is the same as drug overdose and epileptic seizure)"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#linear-regression-1",
    "href": "slides/04-logistic_o.qmd.html#linear-regression-1",
    "title": "Chapter 4 Part 1",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhat if we have \\(&gt;2\\) possible outcomes? For example, someone comes to the emergency room and we need to classify them according to their symptoms\n\\[\n\\begin{align}\nY = \\begin{cases} 1& \\textrm{if }\\texttt{stroke}\\\\2&\\textrm{if }\\texttt{drug overdose}\\\\3&\\textrm{if }\\texttt{epileptic seizure}\\end{cases}\n\\end{align}\n\\]\n\nLinear regression is not appropriate here\nMutliclass logistic regression or discriminant analysis are more appropriate"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#logistic-regression",
    "href": "slides/04-logistic_o.qmd.html#logistic-regression",
    "title": "Chapter 4 Part 1",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\\[\np(X) = \\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}\n\\]\n\nNote: \\(p(X)\\) is shorthand for \\(P(Y=1|X)\\)\nNo matter what values \\(\\beta_0\\), \\(\\beta_1\\), or \\(X\\) take \\(p(X)\\) will always be between 0 and 1"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#logistic-regression-1",
    "href": "slides/04-logistic_o.qmd.html#logistic-regression-1",
    "title": "Chapter 4 Part 1",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\\[\np(X) = \\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}\n\\]\nWe can rearrange this into the following form:\n\\[\n\\log\\left(\\frac{p(X)}{1-p(X)}\\right) = \\beta_0 + \\beta_1 X\n\\]\n\nWhat is this transformation called?\n\n\nThis is a log odds or logit transformation of \\(p(X)\\)"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#linear-versus-logistic-regression-1",
    "href": "slides/04-logistic_o.qmd.html#linear-versus-logistic-regression-1",
    "title": "Chapter 4 Part 1",
    "section": "Linear versus logistic regression",
    "text": "Linear versus logistic regression\n\n\n\n\n\n\n\n\n\nLogistic regression ensures that our estimates for \\(p(X)\\) are between 0 and 1 üéâ"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#maximum-likelihood",
    "href": "slides/04-logistic_o.qmd.html#maximum-likelihood",
    "title": "Chapter 4 Part 1",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\nRefresher: How did we estimate \\(\\hat\\beta\\) in linear regression?"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#maximum-likelihood-1",
    "href": "slides/04-logistic_o.qmd.html#maximum-likelihood-1",
    "title": "Chapter 4 Part 1",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\nRefresher: How did we estimate \\(\\hat\\beta\\) in linear regression?\n\nIn logistic regression, we use maximum likelihood to estimate the parameters\n\\[\\mathcal{l}(\\beta_0,\\beta_1)=\\prod_{i:y_i=1}p(x_i)\\prod_{i:y_i=0}(1-p(x_i))\\]\n\nThis likelihood give the probability of the observed ones and zeros in the data\nWe pick \\(\\beta_0\\) and \\(\\beta_1\\) to maximize the likelihood\nWe‚Äôll let R do the heavy lifting here"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#lets-see-it-in-r",
    "href": "slides/04-logistic_o.qmd.html#lets-see-it-in-r",
    "title": "Chapter 4 Part 1",
    "section": "Let‚Äôs see it in R",
    "text": "Let‚Äôs see it in R\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(default ~ balance, \n      data = Default) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term         estimate std.error statistic   p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) -10.7      0.361        -29.5 3.62e-191\n2 balance       0.00550  0.000220      25.0 1.98e-137\n\n\n\nUse the logistic_reg() function in R with the glm engine"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#making-predictions",
    "href": "slides/04-logistic_o.qmd.html#making-predictions",
    "title": "Chapter 4 Part 1",
    "section": "Making predictions",
    "text": "Making predictions\n\nWhat is our estimated probability of default for someone with a balance of $1000?\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.6513306\n0.3611574\n-29.49221\n0\n\n\nbalance\n0.0054989\n0.0002204\n24.95309\n0\n\n\n\n\n\n\n\n\n. . .\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}{1+e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}=\\frac{e^{-10.65+0.0055\\times 1000}}{1+e^{-10.65+0.0055\\times 1000}}=0.006\n\\]"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#making-predictions-1",
    "href": "slides/04-logistic_o.qmd.html#making-predictions-1",
    "title": "Chapter 4 Part 1",
    "section": "Making predictions",
    "text": "Making predictions\n\nWhat is our estimated probability of default for someone with a balance of $2000?\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.6513306\n0.3611574\n-29.49221\n0\n\n\nbalance\n0.0054989\n0.0002204\n24.95309\n0\n\n\n\n\n\n\n\n\n. . .\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}{1+e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}=\\frac{e^{-10.65+0.0055\\times 2000}}{1+e^{-10.65+0.0055\\times 2000}}=0.586\n\\]"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#logistic-regression-example",
    "href": "slides/04-logistic_o.qmd.html#logistic-regression-example",
    "title": "Chapter 4 Part 1",
    "section": "Logistic regression example",
    "text": "Logistic regression example\nLet‚Äôs refit the model to predict the probability of default given the customer is a student\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-3.5041278\n0.0707130\n-49.554219\n0.0000000\n\n\nstudentYes\n0.4048871\n0.1150188\n3.520181\n0.0004313\n\n\n\n\n\n\n\n\n\\[P(\\texttt{default = Yes}|\\texttt{student = Yes}) = \\frac{e^{-3.5041+0.4049\\times1}}{1+e^{-3.5041+0.4049\\times1}}=0.0431\\]\n. . .\n\nHow will this change if student = No?\n\n. . .\n\\[P(\\texttt{default = Yes}|\\texttt{student = No}) = \\frac{e^{-3.5041+0.4049\\times0}}{1+e^{-3.5041+0.4049\\times0}}=0.0292\\]"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#potential-confounding",
    "href": "slides/04-logistic_o.qmd.html#potential-confounding",
    "title": "Chapter 4 Part 1",
    "section": "Potential Confounding",
    "text": "Potential Confounding\n\n\n\n\n\n\n\n\n\n\nWhat is going on here?"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#confounding",
    "href": "slides/04-logistic_o.qmd.html#confounding",
    "title": "Chapter 4 Part 1",
    "section": "Confounding",
    "text": "Confounding\n\n\n\n\n\n\n\n\n\n\nStudents tend to have higher balances than non-students\nTheir marginal default rate is higher\nFor each level of balance, students default less\nTheir conditional default rate is lower"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#a-bit-about-odds",
    "href": "slides/04-logistic_o.qmd.html#a-bit-about-odds",
    "title": "Chapter 4 Part 1",
    "section": "A bit about ‚Äúodds‚Äù",
    "text": "A bit about ‚Äúodds‚Äù\n\nThe ‚Äúodds‚Äù tell you how likely an event is\nüåÇ Let‚Äôs say there is a 60% chance of rain today * What is the probability that it will rain?\n\\(p = 0.6\\)\nWhat is the probability that it won‚Äôt rain?\n\\(1-p = 0.4\\)\nWhat are the odds that it will rain?\n3 to 2, 3:2, \\(\\frac{0.6}{0.4} = 1.5\\)"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#transforming-logs",
    "href": "slides/04-logistic_o.qmd.html#transforming-logs",
    "title": "Chapter 4 Part 1",
    "section": "Transforming logs",
    "text": "Transforming logs\n\nHow do you ‚Äúundo‚Äù a \\(\\log\\) base \\(e\\)?\nUse \\(e\\)! For example:\n\\(e^{\\log(10)} = 10\\)\n\\(e^{\\log(1283)} = 1283\\)\n\\(e^{\\log(x)} = x\\)"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#transforming-logs-1",
    "href": "slides/04-logistic_o.qmd.html#transforming-logs-1",
    "title": "Chapter 4 Part 1",
    "section": "Transforming logs",
    "text": "Transforming logs\n\nHow would you get the odds from the log(odds)?\n\n\n\nHow do you ‚Äúundo‚Äù a \\(\\log\\) base \\(e\\)?\nUse \\(e\\)! For example:\n\\(e^{\\log(10)} = 10\\)\n\\(e^{\\log(1283)} = 1283\\)\n\\(e^{\\log(x)} = x\\)\n\n\n\n\\(e^{\\log(odds)}\\) = odds"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#transforming-odds",
    "href": "slides/04-logistic_o.qmd.html#transforming-odds",
    "title": "Chapter 4 Part 1",
    "section": "Transforming odds",
    "text": "Transforming odds\n\nodds = \\(\\frac{\\pi}{1-\\pi}\\)\nSolving for \\(\\pi\\)\n\\(\\pi = \\frac{\\textrm{odds}}{1+\\textrm{odds}}\\)\nPlugging in \\(e^{\\log(odds)}\\) = odds\n\\(\\pi = \\frac{e^{\\log(odds)}}{1+e^{\\log(odds)}}\\)\nPlugging in \\(\\log(odds) = \\beta_0 + \\beta_1x\\)\n\\(\\pi = \\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}\\)"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#the-logistic-model",
    "href": "slides/04-logistic_o.qmd.html#the-logistic-model",
    "title": "Chapter 4 Part 1",
    "section": "The logistic model",
    "text": "The logistic model\n\n‚úåÔ∏è forms\n\n\n\n\n\n\n\n\nForm\nModel\n\n\n\n\nLogit form\n\\(\\log\\left(\\frac{\\pi}{1-\\pi}\\right) = \\beta_0 + \\beta_1x\\)\n\n\nProbability form\n\\(\\Large\\pi = \\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}\\)"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#the-logistic-model-1",
    "href": "slides/04-logistic_o.qmd.html#the-logistic-model-1",
    "title": "Chapter 4 Part 1",
    "section": "The logistic model",
    "text": "The logistic model\n\n\n\nprobability\nodds\nlog(odds)\n\n\n\n\n\\(\\pi\\)\n\\(\\frac{\\pi}{1-\\pi}\\)\n\\(\\log\\left(\\frac{\\pi}{1-\\pi}\\right)=l\\)\n\n\n\n‚¨ÖÔ∏è\n\n\n\nlog(odds)\nodds\nprobability\n\n\n\n\n\\(l\\)\n\\(e^l\\)\n\\(\\frac{e^l}{1+e^l} = \\pi\\)"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#the-logistic-model-2",
    "href": "slides/04-logistic_o.qmd.html#the-logistic-model-2",
    "title": "Chapter 4 Part 1",
    "section": "The logistic model",
    "text": "The logistic model\n\n‚úåÔ∏è forms\nlog(odds): \\(l = \\beta_0 + \\beta_1x\\)\nP(Outcome = Yes): \\(\\Large\\pi =\\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}\\)"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#odds-ratios",
    "href": "slides/04-logistic_o.qmd.html#odds-ratios",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later. (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#odds-ratios-1",
    "href": "slides/04-logistic_o.qmd.html#odds-ratios-1",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat is the explanatory variable?\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#odds-ratios-2",
    "href": "slides/04-logistic_o.qmd.html#odds-ratios-2",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat type of variable is this?\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#odds-ratios-3",
    "href": "slides/04-logistic_o.qmd.html#odds-ratios-3",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat is the outcome variable?\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#odds-ratios-4",
    "href": "slides/04-logistic_o.qmd.html#odds-ratios-4",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat type of variable is this?\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person‚Äôs head might be an effective treatment for migraine headaches.\n\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#odds-ratios-5",
    "href": "slides/04-logistic_o.qmd.html#odds-ratios-5",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\n\n\nTreatment\nTMS\nPlacebo\nTotal\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\nWe can compare the results using odds\nWhat are the odds of being pain-free for the placebo group?\n\\((22/100)/(78/100) = 22/78 = 0.282\\)\nWhat are the odds of being pain-free for the treatment group?\n\\(39/61 = 0.639\\)\nComparing the odds what can we conclude?\nTMS increases the likelihood of success"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#odds-ratios-6",
    "href": "slides/04-logistic_o.qmd.html#odds-ratios-6",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\n\n\nTreatment\nTMS\nPlacebo\nTotal\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\nWe can summarize this relationship with an odds ratio: the ratio of the two odds\n\\(\\Large OR = \\frac{39/61}{22/78} = \\frac{0.639}{0.282} = 2.27\\)\n‚Äúthe odds of being pain free were 2.27 times higher with TMS than with the placebo‚Äù"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#odds-ratios-7",
    "href": "slides/04-logistic_o.qmd.html#odds-ratios-7",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat if we wanted to calculate this in terms of Not pain-free (with pain-free) as the referent?\n\n\n\n\nTreatment\nTMS\nPlacebo\nTotal\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\n\\(\\Large OR = \\frac{61/39}{78/22} = \\frac{1.564}{3.545} = 0.441\\)\nthe odds for still being in pain for the TMS group are 0.441 times the odds of being in pain for the placebo group"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#odds-ratios-8",
    "href": "slides/04-logistic_o.qmd.html#odds-ratios-8",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat changed here?\n\n\n\n\nTreatment\nTMS\nPlacebo\nTotal\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\n\\(\\Large OR = \\frac{78/22}{61/39} = \\frac{3.545}{1.564} = 2.27\\)\nthe odds for still being in pain for the placebo group are 2.27 times the odds of being in pain for the TMS group"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#odds-ratios-9",
    "href": "slides/04-logistic_o.qmd.html#odds-ratios-9",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nIn general, it‚Äôs more natural to interpret odds ratios &gt; 1, you can flip the referent to do so\n\n\n\nTreatment\nTMS\nPlacebo\nTotal\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\\(\\Large OR = \\frac{78/22}{61/39} = \\frac{3.545}{1.564} = 2.27\\)\nthe odds for still being in pain for the placebo group are 2.27 times the odds of being in pain for the TMS group"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#odds-ratios-10",
    "href": "slides/04-logistic_o.qmd.html#odds-ratios-10",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nLet‚Äôs look at some Titanic data. We are interested in whether the passenger reported being female is related to whether they survived.\n\n\n\n\nFemale\nMale\nTotal\n\n\n\n\nSurvived\n308\n142\n450\n\n\nDied\n154\n709\n863\n\n\nTotal\n462\n851\n1313"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#odds-ratios-11",
    "href": "slides/04-logistic_o.qmd.html#odds-ratios-11",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat are the odds of surviving for females versus males?\n\n\n\n\n\nFemale\nMale\nTotal\n\n\n\n\nSurvived\n308\n142\n450\n\n\nDied\n154\n709\n863\n\n\nTotal\n462\n851\n1313\n\n\n\n\\[\\Large OR = \\frac{308/154}{142/709} = \\frac{2}{0.2} = 9.99\\]"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#odds-ratios-12",
    "href": "slides/04-logistic_o.qmd.html#odds-ratios-12",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nHow do you interpret this?\n\n\n\n\n\nFemale\nMale\nTotal\n\n\n\n\nSurvived\n308\n142\n450\n\n\nDied\n154\n709\n863\n\n\nTotal\n462\n851\n1313\n\n\n\n\\[\\Large OR = \\frac{308/154}{142/709} = \\frac{2}{0.2} = 9.99\\] the odds of surviving for the female passengers was 9.99 times the odds of surviving for the male passengers"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#odds-ratios-13",
    "href": "slides/04-logistic_o.qmd.html#odds-ratios-13",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat if we wanted to fit a model? What would the equation be?\n\n\n\n\n\nFemale\nMale\nTotal\n\n\n\n\nSurvived\n308\n142\n450\n\n\nDied\n154\n709\n863\n\n\nTotal\n462\n851\n1313\n\n\n\n. . .\n\\[\\Large \\log(\\textrm{odds of survival}) = \\beta_0 + \\beta_1 \\textrm{Female}\\]"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#odds-ratios-14",
    "href": "slides/04-logistic_o.qmd.html#odds-ratios-14",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\\[\\Large \\log(\\textrm{odds of survival}) = \\beta_0 + \\beta_1 \\textrm{Female}\\]\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Survived ~ Sex, data = Titanic) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -1.61    0.0919     -17.5 1.70e-68\n2 Sexfemale       2.30    0.135       17.1 2.91e-65"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#odds-ratios-15",
    "href": "slides/04-logistic_o.qmd.html#odds-ratios-15",
    "title": "Chapter 4 Part 1",
    "section": "Odds Ratios",
    "text": "Odds Ratios\n\nHow do you interpret this result?\n\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Survived ~ Sex, data = Titanic) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -1.61    0.0919     -17.5 1.70e-68\n2 Sexfemale       2.30    0.135       17.1 2.91e-65"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#odds-ratios-16",
    "href": "slides/04-logistic_o.qmd.html#odds-ratios-16",
    "title": "Chapter 4 Part 1",
    "section": "Odds Ratios",
    "text": "Odds Ratios\n\nHow do you interpret this result?\n\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Survived ~ Sex, data = Titanic) |&gt;\n  tidy(exponentiate = TRUE) \n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.200    0.0919     -17.5 1.70e-68\n2 Sexfemale      9.99     0.135       17.1 2.91e-65\n\nexp(2.301176)\n\n[1] 9.99"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#odds-ratios-17",
    "href": "slides/04-logistic_o.qmd.html#odds-ratios-17",
    "title": "Chapter 4 Part 1",
    "section": "Odds Ratios",
    "text": "Odds Ratios\n\nHow do you interpret this result?\n\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Survived ~ Sex, data = Titanic) |&gt;\n  tidy(exponentiate = TRUE) \n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.200    0.0919     -17.5 1.70e-68\n2 Sexfemale      9.99     0.135       17.1 2.91e-65\n\nexp(2.301176)\n\n[1] 9.99\n\n\nthe odds of surviving for the female passengers was 9.99 times the odds of surviving for the male passengers"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#odds-ratios-18",
    "href": "slides/04-logistic_o.qmd.html#odds-ratios-18",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nWhat if the explanatory variable is continuous?\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Acceptance ~ GPA, data = MedGPA) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -19.2       5.63     -3.41 0.000644\n2 GPA             5.45      1.58      3.45 0.000553\n\n\nA one unit increase in GPA yields a 5.45 increase in the log odds of acceptance"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#odds-ratios-19",
    "href": "slides/04-logistic_o.qmd.html#odds-ratios-19",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nWhat if the explanatory variable is continuous?\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Acceptance ~ GPA, data = MedGPA) |&gt;\n  tidy(exponentiate = TRUE) \n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  4.56e-9      5.63     -3.41 0.000644\n2 GPA          2.34e+2      1.58      3.45 0.000553\n\n\nA one unit increase in GPA yields a 234-fold increase in the odds of acceptance\n\nüò± that seems huge! Remember: the interpretation of these coefficients depends on your units (the same as in ordinary linear regression)."
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#odds-ratios-20",
    "href": "slides/04-logistic_o.qmd.html#odds-ratios-20",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nHow could we get the odds associated with increasing GPA by 0.1?\n\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Acceptance ~ GPA, data = MedGPA) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -19.2       5.63     -3.41 0.000644\n2 GPA             5.45      1.58      3.45 0.000553\n\n\n\nexp(5.454) ## a one unit increase in GPA\n\n[1] 234\n\nexp(5.454 * 0.1) ## a 0.1 increase in GPA\n\n[1] 1.73\n\n\nA one-tenth unit increase in GPA yields a 1.73-fold increase in the odds of acceptance"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#odds-ratios-21",
    "href": "slides/04-logistic_o.qmd.html#odds-ratios-21",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nHow could we get the odds associated with increasing GPA by 0.1?\n\n\nMedGPA &lt;- MedGPA |&gt;\n  mutate(GPA_10 = GPA * 10)\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Acceptance ~ GPA_10, data = MedGPA) |&gt;\n  tidy(exponentiate = TRUE)\n\n# A tibble: 2 √ó 5\n  term             estimate std.error statistic  p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) 0.00000000456     5.63      -3.41 0.000644\n2 GPA_10      1.73              0.158      3.45 0.000553\n\n\nA one-tenth unit increase in GPA yields a 1.73-fold increase in the odds of acceptance"
  },
  {
    "objectID": "slides/04-logistic_o.qmd.html#application-exercise",
    "href": "slides/04-logistic_o.qmd.html#application-exercise",
    "title": "Chapter 4 Part 1",
    "section": " Application Exercise",
    "text": "Application Exercise\nUsing the Default data from the ISLR package. Fit two logistic regression models that predict whether a customer defaults\n\nOne model with student as a predictor. Interpret the coefficient of studentYes.\nAnother model with balance as a predictor. Interpret the coefficient of balance.\n\nHere is some code to get you started:\n\nlibrary(ISLR)\ndata(\"Default\")"
  },
  {
    "objectID": "slides/06-subset.html#best-subsets",
    "href": "slides/06-subset.html#best-subsets",
    "title": "Chapter 6 Part 1",
    "section": "Best Subsets",
    "text": "Best Subsets\n\nSave the summary and see whats included\n\n\n\nCode\nreg_summary &lt;- summary(best_ss_model)\nnames(reg_summary) # See what metrics are included \n\n\n[1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"   \n\n\n\nWe can access each of these with $\nThis gives you the value for each of the best models with different number of predictors\n\n\n\nCode\nreg_summary$rsq\n\n\n[1] 0.7419596 0.8645677 0.9470542 0.9514489 0.9521160 0.9525191 0.9526977 0.9527695"
  },
  {
    "objectID": "slides/06-subset_o.html#best-subsets",
    "href": "slides/06-subset_o.html#best-subsets",
    "title": "Chapter 6 Part 1",
    "section": "Best Subsets",
    "text": "Best Subsets\n\nSave the summary and see whats included\n\n\n\nCode\nreg_summary &lt;- summary(best_ss_model)\nnames(reg_summary) # See what metrics are included \n\n\n[1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"   \n\n\n\nWe can access each of these with $\nThis gives you the value for each of the best models with different number of predictors\n\n\n\nCode\nreg_summary$rsq\n\n\n[1] 0.7419596 0.8645677 0.9470542 0.9514489 0.9521160 0.9525191 0.9526977 0.9527695"
  },
  {
    "objectID": "slides/06-2-shrinkage.html#setup",
    "href": "slides/06-2-shrinkage.html#setup",
    "title": "Chapter 6 Part 2",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR2)\nlibrary(leaps)"
  },
  {
    "objectID": "slides/06-2-shrinkage.html#shrinkage-methods",
    "href": "slides/06-2-shrinkage.html#shrinkage-methods",
    "title": "Chapter 6 Part 2",
    "section": "Shrinkage Methods",
    "text": "Shrinkage Methods\nRidge regression and Lasso - The subset selection methods use least squares to fit a linear model that contains a subset of the predictors.\n\nAs an alternative, we can fit a model containing all p predictors using a technique that constrains or regularizes the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero.\nIt may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficient estimates can significantly reduce their variance."
  },
  {
    "objectID": "slides/06-2-shrinkage.html#another-reason",
    "href": "slides/06-2-shrinkage.html#another-reason",
    "title": "Chapter 6 Part 2",
    "section": "Another Reason",
    "text": "Another Reason\n\nSometimes we can‚Äôt solve for \\(\\hat\\beta\\)\n\n\n\nWhy?\n\n\n\nWe have more variables than observations ( \\(p &gt; n\\) )\nThe variables are linear combinations of one another\nThe variance can blow up"
  },
  {
    "objectID": "slides/06-2-shrinkage.html#ridge-regression",
    "href": "slides/06-2-shrinkage.html#ridge-regression",
    "title": "Chapter 6 Part 2",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nWhat if we add an additional penalty to keep the \\(\\hat\\beta\\) coefficients small (this will keep the variance from blowing up!)\nInstead of minimizing \\(RSS\\), like we do with linear regression, let‚Äôs minimize \\(RSS\\) PLUS some penalty function\n\\(RSS + \\underbrace{\\lambda\\sum_{j=1}^p\\beta^2_j}_{\\textrm{shrinkage penalty}}\\)\n\n\n\nWhat happens when \\(\\lambda=0\\)? What happens as \\(\\lambda\\rightarrow\\infty\\)?"
  },
  {
    "objectID": "slides/06-2-shrinkage.html#ridge-regression-1",
    "href": "slides/06-2-shrinkage.html#ridge-regression-1",
    "title": "Chapter 6 Part 2",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nLRRidge\n\n\n\nRecall, the least squares fitting procedure estimates \\(\\beta_0,...,\\beta_p\\) using the values that minimize \\[RSS = \\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\\]\n\n\n\n\nRidge regression coefficient estimates, \\(\\hat{\\beta}^R\\) are the values that minimize\n\n\\[\\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right)^2+\\lambda\\sum_{j=1}^p\\beta_j^2\\]\n\\[ = RSS + \\lambda\\sum_{j=1}^p\\beta_j^2\\]\nwhere \\(\\lambda\\geq 0\\) is a tuning parameter, to be determined separately"
  },
  {
    "objectID": "slides/06-2-shrinkage.html#more-on-ridge",
    "href": "slides/06-2-shrinkage.html#more-on-ridge",
    "title": "Chapter 6 Part 2",
    "section": "More on Ridge",
    "text": "More on Ridge\n\nLike least squares, ridge regression seeks coefficient estimates that fit the data well by making the RSS small.\nThe second term \\(\\lambda\\sum_j\\beta_j^2\\) is called a shrinkage penalty, is small when \\(\\beta_1,...\\beta_p\\) are close to 0, and so it has the effect of shrinking the estimates of \\(\\beta_j\\) toward 0."
  },
  {
    "objectID": "slides/06-2-shrinkage.html#shinkage",
    "href": "slides/06-2-shrinkage.html#shinkage",
    "title": "Chapter 6 Part 2",
    "section": "Shinkage",
    "text": "Shinkage\n\nEach curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of \\(\\lambda\\)."
  },
  {
    "objectID": "slides/06-2-shrinkage.html#choosing-lambda",
    "href": "slides/06-2-shrinkage.html#choosing-lambda",
    "title": "Chapter 6 Part 2",
    "section": "Choosing \\(\\lambda\\)",
    "text": "Choosing \\(\\lambda\\)\n\n\\(\\lambda\\) is known as a tuning parameter and is selected using cross validation\nFor example, choose the \\(\\lambda\\) that results in the smallest estimated test error\nAfterwards we refit using all available observations (from training set)\n\n\n\n\n\nüîó https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "slides/06-2-shrinkage.html#bias-variance-tradeoff",
    "href": "slides/06-2-shrinkage.html#bias-variance-tradeoff",
    "title": "Chapter 6 Part 2",
    "section": "Bias-variance tradeoff",
    "text": "Bias-variance tradeoff\n\nHow do you think ridge regression fits into the bias-variance trade-off?\n\n\nAs \\(\\lambda\\) ‚òùÔ∏è, bias ‚òùÔ∏è, variance üëá"
  },
  {
    "objectID": "slides/06-2-shrinkage.html#ridge-regression-2",
    "href": "slides/06-2-shrinkage.html#ridge-regression-2",
    "title": "Chapter 6 Part 2",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nIMPORTANT: When doing ridge regression, it is important to standardize your variables (divide by the standard deviation)"
  },
  {
    "objectID": "slides/06-2-shrinkage.html#shinkage-coeff",
    "href": "slides/06-2-shrinkage.html#shinkage-coeff",
    "title": "Chapter 6 Part 2",
    "section": "Shinkage Coeff",
    "text": "Shinkage Coeff\n\n\n\n\n\nThis displays the same ridge coefficient estimates as the previous graphs, but instead of displaying \\(\\lambda\\) on the x-axis, we now display \\(||\\hat{\\beta}_\\lambda^R||_2/||\\hat{\\beta}||_2\\), where \\(\\hat{\\beta}\\) denotes the vector of the least squares coefficient estimates.\nThe notation \\(||\\beta||_2\\) denotes the"
  },
  {
    "objectID": "slides/06-2-shrinkage.html#ridge---scalling-predictors",
    "href": "slides/06-2-shrinkage.html#ridge---scalling-predictors",
    "title": "Chapter 6 Part 2",
    "section": "Ridge - Scalling Predictors",
    "text": "Ridge - Scalling Predictors\n\nThe standard least squares coefficient estimates are scale equivalent: multiplying \\(X_j\\) by a constant c simply leads to a scaling of the least squares coefficient estimates by a factor of \\(1=c\\). In other words, regardless of how the \\(j\\)th predictor is scaled, \\(X_j\\hat{\\beta}_j\\) will remain the same.\nIn contrast, the ridge regression coefficient estimates can change substantially when multiplying a given predictor by a constant, due to the sum of squared coefficients term in the penalty part of the ridge regression objective function.\nTherefore, it is best to apply ridge regression after standardizing the predictors, using the formula\n\\[\\tilde{x}_{ij} = \\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)^2}}\\]"
  },
  {
    "objectID": "slides/06-2-shrinkage.html#ridge-bias-variance-tradeoff",
    "href": "slides/06-2-shrinkage.html#ridge-bias-variance-tradeoff",
    "title": "Chapter 6 Part 2",
    "section": "Ridge Bias-Variance tradeoff",
    "text": "Ridge Bias-Variance tradeoff\n\n\nSimulated data with n = 50 observations, p = 45 predictors, all having nonzero coefficients.\nSquared bias (black), variance (green), and test mean squared error (purple) for the ridge regression predictions on a simulated data set, as a function of \\(\\lambda\\) and \\(||\\hat{\\beta}_\\lambda^R||_2/||\\hat{\\beta}||_2\\). The horizontal dashed lines indicate the minimum possible MSE. The purple crosses indicate the ridge regression models for which the MSE is smallest."
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html",
    "href": "slides/06-2-shrinkage_o.html",
    "title": "Chapter 6 Part 2",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR2)\nlibrary(leaps)"
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#setup",
    "href": "slides/06-2-shrinkage_o.html#setup",
    "title": "Chapter 6 Part 2",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR2)\nlibrary(leaps)"
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#shrinkage-methods",
    "href": "slides/06-2-shrinkage_o.html#shrinkage-methods",
    "title": "Chapter 6 Part 2",
    "section": "Shrinkage Methods",
    "text": "Shrinkage Methods\nRidge regression and Lasso - The subset selection methods use least squares to fit a linear model that contains a subset of the predictors.\n\nAs an alternative, we can fit a model containing all p predictors using a technique that constrains or regularizes the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero.\nIt may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficient estimates can significantly reduce their variance."
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#another-reason",
    "href": "slides/06-2-shrinkage_o.html#another-reason",
    "title": "Chapter 6 Part 2",
    "section": "Another Reason",
    "text": "Another Reason\n\nSometimes we can‚Äôt solve for \\(\\hat\\beta\\)\n\n\n\nWhy?\n\n\n\nWe have more variables than observations ( \\(p &gt; n\\) )\nThe variables are linear combinations of one another\nThe variance can blow up"
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#ridge-regression",
    "href": "slides/06-2-shrinkage_o.html#ridge-regression",
    "title": "Chapter 6 Part 2",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nWhat if we add an additional penalty to keep the \\(\\hat\\beta\\) coefficients small (this will keep the variance from blowing up!)\nInstead of minimizing \\(RSS\\), like we do with linear regression, let‚Äôs minimize \\(RSS\\) PLUS some penalty function\n\\(RSS + \\underbrace{\\lambda\\sum_{j=1}^p\\beta^2_j}_{\\textrm{shrinkage penalty}}\\)\n\n\n\nWhat happens when \\(\\lambda=0\\)? What happens as \\(\\lambda\\rightarrow\\infty\\)?"
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#ridge-regression-1",
    "href": "slides/06-2-shrinkage_o.html#ridge-regression-1",
    "title": "Chapter 6 Part 2",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nLRRidge\n\n\n\nRecall, the least squares fitting procedure estimates \\(\\beta_0,...,\\beta_p\\) using the values that minimize \\[RSS = \\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\\]\n\n\n\n\nRidge regression coefficient estimates, \\(\\hat{\\beta}^R\\) are the values that minimize\n\n\\[\\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right)^2+\\lambda\\sum_{j=1}^p\\beta_j^2\\]\n\\[ = RSS + \\lambda\\sum_{j=1}^p\\beta_j^2\\]\nwhere \\(\\lambda\\geq 0\\) is a tuning parameter, to be determined separately"
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#more-on-ridge",
    "href": "slides/06-2-shrinkage_o.html#more-on-ridge",
    "title": "Chapter 6 Part 2",
    "section": "More on Ridge",
    "text": "More on Ridge\n\nLike least squares, ridge regression seeks coefficient estimates that fit the data well by making the RSS small.\nThe second term \\(\\lambda\\sum_j\\beta_j^2\\) is called a shrinkage penalty, is small when \\(\\beta_1,...\\beta_p\\) are close to 0, and so it has the effect of shrinking the estimates of \\(\\beta_j\\) toward 0."
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#shinkage",
    "href": "slides/06-2-shrinkage_o.html#shinkage",
    "title": "Chapter 6 Part 2",
    "section": "Shinkage",
    "text": "Shinkage\n\nEach curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of \\(\\lambda\\)."
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#shinkage-coeff",
    "href": "slides/06-2-shrinkage_o.html#shinkage-coeff",
    "title": "Chapter 6 Part 2",
    "section": "Shinkage Coeff",
    "text": "Shinkage Coeff\n\n\n\n\n\nThis displays the same ridge coefficient estimates as the previous graphs, but instead of displaying \\(\\lambda\\) on the x-axis, we now display \\(||\\hat{\\beta}_\\lambda^R||_2/||\\hat{\\beta}||_2\\), where \\(\\hat{\\beta}\\) denotes the vector of the least squares coefficient estimates.\nThe notation \\(||\\beta||_2\\) denotes the"
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#ridge---scalling-predictors",
    "href": "slides/06-2-shrinkage_o.html#ridge---scalling-predictors",
    "title": "Chapter 6 Part 2",
    "section": "Ridge - Scalling Predictors",
    "text": "Ridge - Scalling Predictors\n\nThe standard least squares coefficient estimates are scale equivalent: multiplying \\(X_j\\) by a constant c simply leads to a scaling of the least squares coefficient estimates by a factor of \\(1=c\\). In other words, regardless of how the \\(j\\)th predictor is scaled, \\(X_j\\hat{\\beta}_j\\) will remain the same.\nIn contrast, the ridge regression coefficient estimates can change substantially when multiplying a given predictor by a constant, due to the sum of squared coefficients term in the penalty part of the ridge regression objective function.\nTherefore, it is best to apply ridge regression after standardizing the predictors, using the formula\n\\[\\tilde{x}_{ij} = \\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)^2}}\\]"
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#ridge-regression-2",
    "href": "slides/06-2-shrinkage_o.html#ridge-regression-2",
    "title": "Chapter 6 Part 2",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nIMPORTANT: When doing ridge regression, it is important to standardize your variables (divide by the standard deviation)"
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#choosing-lambda",
    "href": "slides/06-2-shrinkage_o.html#choosing-lambda",
    "title": "Chapter 6 Part 2",
    "section": "Choosing \\(\\lambda\\)",
    "text": "Choosing \\(\\lambda\\)\n\n\\(\\lambda\\) is known as a tuning parameter and is selected using cross validation\nFor example, choose the \\(\\lambda\\) that results in the smallest estimated test error\nAfterwards we refit using all available observations (from training set)"
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#bias-variance-tradeoff",
    "href": "slides/06-2-shrinkage_o.html#bias-variance-tradeoff",
    "title": "Chapter 6 Part 2",
    "section": "Bias-variance tradeoff",
    "text": "Bias-variance tradeoff\n\nHow do you think ridge regression fits into the bias-variance trade-off?\n\n\nAs \\(\\lambda\\) ‚òùÔ∏è, bias ‚òùÔ∏è, variance üëá"
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#ridge-bias-variance-tradeoff",
    "href": "slides/06-2-shrinkage_o.html#ridge-bias-variance-tradeoff",
    "title": "Chapter 6 Part 2",
    "section": "Ridge Bias-Variance tradeoff",
    "text": "Ridge Bias-Variance tradeoff\n\n\nSimulated data with n = 50 observations, p = 45 predictors, all having nonzero coefficients.\nSquared bias (black), variance (green), and test mean squared error (purple) for the ridge regression predictions on a simulated data set, as a function of \\(\\lambda\\) and \\(||\\hat{\\beta}_\\lambda^R||_2/||\\hat{\\beta}||_2\\). The horizontal dashed lines indicate the minimum possible MSE. The purple crosses indicate the ridge regression models for which the MSE is smallest."
  },
  {
    "objectID": "slides/06-2-shrinkage.html#lasso-continued",
    "href": "slides/06-2-shrinkage.html#lasso-continued",
    "title": "Chapter 6 Part 2",
    "section": "Lasso Continued",
    "text": "Lasso Continued\n\nAs with ridge regression, the lasso shrinks the coefficient estimates towards zero.\nIn the case of the lasso, the \\(\\ell_1\\) penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter \\(\\lambda\\) is sufficiently large.\nLike best subset selection, the lasso performs variable selection.\nWe say that the lasso yields sparse models - that is, models that involve only a subset of the variables.\nAs in ridge regression, selecting a good value of \\(\\lambda\\) for the lasso is critical; cross-validation is again the method of choice."
  },
  {
    "objectID": "slides/06-2-shrinkage.html#ridge-shrinkage",
    "href": "slides/06-2-shrinkage.html#ridge-shrinkage",
    "title": "Chapter 6 Part 2",
    "section": "Ridge Shrinkage",
    "text": "Ridge Shrinkage\n\n\n\n\nEach curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of \\(\\lambda\\)."
  },
  {
    "objectID": "slides/06-2-shrinkage.html#ridge-shinkage-coeff",
    "href": "slides/06-2-shrinkage.html#ridge-shinkage-coeff",
    "title": "Chapter 6 Part 2",
    "section": "Ridge Shinkage Coeff",
    "text": "Ridge Shinkage Coeff\n\n\n\n\n\nThis displays the same ridge coefficient estimates as the previous graphs, but instead of displaying \\(\\lambda\\) on the x-axis, we now display \\(||\\hat{\\beta}_\\lambda^R||_2/||\\hat{\\beta}||_2\\), where \\(\\hat{\\beta}\\) denotes the vector of the least squares coefficient estimates.\nIn statistics lingo, the ridge uses an \\(\\ell_2\\) (pronounced ‚Äúell 2‚Äù) penalty of the betas, written \\(||\\beta||_2\\)."
  },
  {
    "objectID": "slides/06-2-shrinkage.html#lasso-shrinkage",
    "href": "slides/06-2-shrinkage.html#lasso-shrinkage",
    "title": "Chapter 6 Part 2",
    "section": "Lasso Shrinkage",
    "text": "Lasso Shrinkage\n\nCoefficient ShrinkCoefficient Ratio"
  },
  {
    "objectID": "slides/06-2-shrinkage.html#ridge-vs-lasso",
    "href": "slides/06-2-shrinkage.html#ridge-vs-lasso",
    "title": "Chapter 6 Part 2",
    "section": "Ridge vs Lasso",
    "text": "Ridge vs Lasso\n\nWhy does lasso, unlike ridge, result in coefficient estimates that are exactly zero?"
  },
  {
    "objectID": "slides/06-2-shrinkage.html#ridge-vs-lasso-2",
    "href": "slides/06-2-shrinkage.html#ridge-vs-lasso-2",
    "title": "Chapter 6 Part 2",
    "section": "Ridge vs Lasso 2",
    "text": "Ridge vs Lasso 2\nThey each are a minimization problem\nLasso: \\[\\text{minimize}_\\beta\\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\\text{ subject to }\\sum_{j=1}^p|\\beta_j|\\leq s\\]\nRidge: \\[\\text{minimize}_\\beta\\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\\text{ subject to }\\sum_{j=1}^p\\beta_j^2\\leq s\\]"
  },
  {
    "objectID": "slides/06-2-shrinkage.html#ridge-vs-lasso-graphs",
    "href": "slides/06-2-shrinkage.html#ridge-vs-lasso-graphs",
    "title": "Chapter 6 Part 2",
    "section": "Ridge vs Lasso Graphs",
    "text": "Ridge vs Lasso Graphs"
  },
  {
    "objectID": "slides/06-2-shrinkage.html#ridge-vs-lasso-lambda-vs-mse",
    "href": "slides/06-2-shrinkage.html#ridge-vs-lasso-lambda-vs-mse",
    "title": "Chapter 6 Part 2",
    "section": "Ridge vs Lasso \\(\\lambda\\) vs MSE",
    "text": "Ridge vs Lasso \\(\\lambda\\) vs MSE\n\n\n\n\nPlots of squared bias (black), variance (green), and test MSE (purple) for the lasso on simulated data set."
  },
  {
    "objectID": "slides/06-2-shrinkage.html#ridge-vs-lasso-r2-vs-mse",
    "href": "slides/06-2-shrinkage.html#ridge-vs-lasso-r2-vs-mse",
    "title": "Chapter 6 Part 2",
    "section": "Ridge vs Lasso \\(R^2\\) vs MSE",
    "text": "Ridge vs Lasso \\(R^2\\) vs MSE\n\n\n\n\nComparison of squared bias, variance and test MSE between lasso (solid) and ridge (dashed). Both are plotted against their \\(R^2\\) on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest."
  },
  {
    "objectID": "slides/06-2-shrinkage.html#ridge-vs-lasso-lambda-vs-mse-scenario-2",
    "href": "slides/06-2-shrinkage.html#ridge-vs-lasso-lambda-vs-mse-scenario-2",
    "title": "Chapter 6 Part 2",
    "section": "Ridge vs Lasso \\(\\lambda\\) vs MSE Scenario 2",
    "text": "Ridge vs Lasso \\(\\lambda\\) vs MSE Scenario 2\n\n\n\n\nPlots of squared bias (black), variance (green), and test MSE (purple) for the lasso. The simulated data is similar to that before, except that now only two predictors are related to the response."
  },
  {
    "objectID": "slides/06-2-shrinkage.html#ridge-vs-lasso-r2-vs-mse-scenario-2",
    "href": "slides/06-2-shrinkage.html#ridge-vs-lasso-r2-vs-mse-scenario-2",
    "title": "Chapter 6 Part 2",
    "section": "Ridge vs Lasso \\(R^2\\) vs MSE Scenario 2",
    "text": "Ridge vs Lasso \\(R^2\\) vs MSE Scenario 2\n\n\n\n\nComparison of squared bias, variance and test MSE between lasso (solid) and ridge (dashed). Both are plotted against their \\(R^2\\) on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest."
  },
  {
    "objectID": "slides/06-2-shrinkage.html#lasso-vs-ridge-summary",
    "href": "slides/06-2-shrinkage.html#lasso-vs-ridge-summary",
    "title": "Chapter 6 Part 2",
    "section": "Lasso vs Ridge Summary",
    "text": "Lasso vs Ridge Summary\n\nThese two examples illustrate that neither ridge regression nor the lasso will universally dominate the other.\nIn general, one might expect the lasso to perform better when the response is a function of only a relatively small number of predictors.\nHowever, the number of predictors that is related to the response is never known a priori for real data sets.\nA technique such as cross-validation can be used in order to determine which approach is better on a particular data set."
  },
  {
    "objectID": "slides/06-2-shrinkage.html#lasso",
    "href": "slides/06-2-shrinkage.html#lasso",
    "title": "Chapter 6 Part 2",
    "section": "Lasso",
    "text": "Lasso\n\nRidge regression does have one obvious disadvantage: unlike subset selection, which will generally select models that involve just a subset of the variables, ridge regression will include all p predictors in the final model\nThe Lasso is a relatively recent alternative to ridge regression that overcomes this disadvantage. The lasso coefficients, \\(\\hat{\\beta}_\\lambda^L\\), minimize the quantity\n\\[\\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right)^2+\\lambda\\sum_{j=1}^p|\\beta_j|= RSS + \\lambda\\sum_{j=1}^p|\\beta_j|\\]\nwhere \\(\\lambda\\geq 0\\) is a tuning parameter, to be determined separately\nIn statistics lingo, the lasso uses an \\(\\ell_1\\) (pronounced ‚Äúell 1‚Äù) penalty instead of an \\(\\ell_2\\) penalty. The \\(\\ell_1\\) norm of a coefficient vector \\(\\beta\\) is given by \\(||\\beta||_1 = \\sum|\\beta_j|\\)"
  },
  {
    "objectID": "slides/06-2-shrinkage.html#lasso-regression",
    "href": "slides/06-2-shrinkage.html#lasso-regression",
    "title": "Chapter 6 Part 2",
    "section": "Lasso Regression",
    "text": "Lasso Regression\n\nIMPORTANT: When doing lasso regression, it is important to standardize your variables (divide by the standard deviation)"
  },
  {
    "objectID": "slides/06-2-shrinkage.html#lasso-coefficient-shrink",
    "href": "slides/06-2-shrinkage.html#lasso-coefficient-shrink",
    "title": "Chapter 6 Part 2",
    "section": "Lasso Coefficient Shrink",
    "text": "Lasso Coefficient Shrink"
  },
  {
    "objectID": "slides/06-2-shrinkage.html#lasso-coefficient-ratio",
    "href": "slides/06-2-shrinkage.html#lasso-coefficient-ratio",
    "title": "Chapter 6 Part 2",
    "section": "Lasso Coefficient Ratio",
    "text": "Lasso Coefficient Ratio"
  },
  {
    "objectID": "slides/06-2-shrinkage.html#ridge-vs-lasso-lambda-vs-mse-ex-2",
    "href": "slides/06-2-shrinkage.html#ridge-vs-lasso-lambda-vs-mse-ex-2",
    "title": "Chapter 6 Part 2",
    "section": "Ridge vs Lasso \\(\\lambda\\) vs MSE Ex 2",
    "text": "Ridge vs Lasso \\(\\lambda\\) vs MSE Ex 2\n\n\n\n\nPlots of squared bias (black), variance (green), and test MSE (purple) for the lasso. The simulated data is similar to that before, except that now only two predictors are related to the response."
  },
  {
    "objectID": "slides/06-2-shrinkage.html#ridge-vs-lasso-r2-vs-mse-ex-2",
    "href": "slides/06-2-shrinkage.html#ridge-vs-lasso-r2-vs-mse-ex-2",
    "title": "Chapter 6 Part 2",
    "section": "Ridge vs Lasso \\(R^2\\) vs MSE Ex 2",
    "text": "Ridge vs Lasso \\(R^2\\) vs MSE Ex 2\n\n\n\n\nComparison of squared bias, variance and test MSE between lasso (solid) and ridge (dashed). Both are plotted against their \\(R^2\\) on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest."
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#ridge-shrinkage",
    "href": "slides/06-2-shrinkage_o.html#ridge-shrinkage",
    "title": "Chapter 6 Part 2",
    "section": "Ridge Shrinkage",
    "text": "Ridge Shrinkage\n\n\n\n\nEach curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of \\(\\lambda\\)."
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#ridge-shinkage-coeff",
    "href": "slides/06-2-shrinkage_o.html#ridge-shinkage-coeff",
    "title": "Chapter 6 Part 2",
    "section": "Ridge Shinkage Coeff",
    "text": "Ridge Shinkage Coeff\n\n\n\n\n\nThis displays the same ridge coefficient estimates as the previous graphs, but instead of displaying \\(\\lambda\\) on the x-axis, we now display \\(||\\hat{\\beta}_\\lambda^R||_2/||\\hat{\\beta}||_2\\), where \\(\\hat{\\beta}\\) denotes the vector of the least squares coefficient estimates.\nIn statistics lingo, the ridge uses an \\(\\ell_2\\) (pronounced ‚Äúell 2‚Äù) penalty of the betas, written \\(||\\beta||_2\\)."
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#lasso",
    "href": "slides/06-2-shrinkage_o.html#lasso",
    "title": "Chapter 6 Part 2",
    "section": "Lasso",
    "text": "Lasso\n\nRidge regression does have one obvious disadvantage: unlike subset selection, which will generally select models that involve just a subset of the variables, ridge regression will include all p predictors in the final model\nThe Lasso is a relatively recent alternative to ridge regression that overcomes this disadvantage. The lasso coefficients, \\(\\hat{\\beta}_\\lambda^L\\), minimize the quantity\n\\[\\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right)^2+\\lambda\\sum_{j=1}^p|\\beta_j|= RSS + \\lambda\\sum_{j=1}^p|\\beta_j|\\]\nwhere \\(\\lambda\\geq 0\\) is a tuning parameter, to be determined separately\nIn statistics lingo, the lasso uses an \\(\\ell_1\\) (pronounced ‚Äúell 1‚Äù) penalty instead of an \\(\\ell_2\\) penalty. The \\(\\ell_1\\) norm of a coefficient vector \\(\\beta\\) is given by \\(||\\beta||_1 = \\sum|\\beta_j|\\)"
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#lasso-continued",
    "href": "slides/06-2-shrinkage_o.html#lasso-continued",
    "title": "Chapter 6 Part 2",
    "section": "Lasso Continued",
    "text": "Lasso Continued\n\nAs with ridge regression, the lasso shrinks the coefficient estimates towards zero.\nIn the case of the lasso, the \\(\\ell_1\\) penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter \\(\\lambda\\) is sufficiently large.\nLike best subset selection, the lasso performs variable selection.\nWe say that the lasso yields sparse models - that is, models that involve only a subset of the variables.\nAs in ridge regression, selecting a good value of \\(\\lambda\\) for the lasso is critical; cross-validation is again the method of choice."
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#lasso-coefficient-shrink",
    "href": "slides/06-2-shrinkage_o.html#lasso-coefficient-shrink",
    "title": "Chapter 6 Part 2",
    "section": "Lasso Coefficient Shrink",
    "text": "Lasso Coefficient Shrink"
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#lasso-coefficient-ratio",
    "href": "slides/06-2-shrinkage_o.html#lasso-coefficient-ratio",
    "title": "Chapter 6 Part 2",
    "section": "Lasso Coefficient Ratio",
    "text": "Lasso Coefficient Ratio"
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#lasso-regression",
    "href": "slides/06-2-shrinkage_o.html#lasso-regression",
    "title": "Chapter 6 Part 2",
    "section": "Lasso Regression",
    "text": "Lasso Regression\n\nIMPORTANT: When doing lasso regression, it is important to standardize your variables (divide by the standard deviation)"
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#ridge-vs-lasso",
    "href": "slides/06-2-shrinkage_o.html#ridge-vs-lasso",
    "title": "Chapter 6 Part 2",
    "section": "Ridge vs Lasso",
    "text": "Ridge vs Lasso\n\nWhy does lasso, unlike ridge, result in coefficient estimates that are exactly zero?"
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#ridge-vs-lasso-2",
    "href": "slides/06-2-shrinkage_o.html#ridge-vs-lasso-2",
    "title": "Chapter 6 Part 2",
    "section": "Ridge vs Lasso 2",
    "text": "Ridge vs Lasso 2\nThey each are a minimization problem\nLasso: \\[\\text{minimize}_\\beta\\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\\text{ subject to }\\sum_{j=1}^p|\\beta_j|\\leq s\\]\nRidge: \\[\\text{minimize}_\\beta\\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\\text{ subject to }\\sum_{j=1}^p\\beta_j^2\\leq s\\]"
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#ridge-vs-lasso-graphs",
    "href": "slides/06-2-shrinkage_o.html#ridge-vs-lasso-graphs",
    "title": "Chapter 6 Part 2",
    "section": "Ridge vs Lasso Graphs",
    "text": "Ridge vs Lasso Graphs"
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#ridge-vs-lasso-lambda-vs-mse",
    "href": "slides/06-2-shrinkage_o.html#ridge-vs-lasso-lambda-vs-mse",
    "title": "Chapter 6 Part 2",
    "section": "Ridge vs Lasso \\(\\lambda\\) vs MSE",
    "text": "Ridge vs Lasso \\(\\lambda\\) vs MSE\n\n\n\n\nPlots of squared bias (black), variance (green), and test MSE (purple) for the lasso on simulated data set."
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#ridge-vs-lasso-r2-vs-mse",
    "href": "slides/06-2-shrinkage_o.html#ridge-vs-lasso-r2-vs-mse",
    "title": "Chapter 6 Part 2",
    "section": "Ridge vs Lasso \\(R^2\\) vs MSE",
    "text": "Ridge vs Lasso \\(R^2\\) vs MSE\n\n\n\n\nComparison of squared bias, variance and test MSE between lasso (solid) and ridge (dashed). Both are plotted against their \\(R^2\\) on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest."
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#ridge-vs-lasso-lambda-vs-mse-ex-2",
    "href": "slides/06-2-shrinkage_o.html#ridge-vs-lasso-lambda-vs-mse-ex-2",
    "title": "Chapter 6 Part 2",
    "section": "Ridge vs Lasso \\(\\lambda\\) vs MSE Ex 2",
    "text": "Ridge vs Lasso \\(\\lambda\\) vs MSE Ex 2\n\n\n\n\nPlots of squared bias (black), variance (green), and test MSE (purple) for the lasso. The simulated data is similar to that before, except that now only two predictors are related to the response."
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#ridge-vs-lasso-r2-vs-mse-ex-2",
    "href": "slides/06-2-shrinkage_o.html#ridge-vs-lasso-r2-vs-mse-ex-2",
    "title": "Chapter 6 Part 2",
    "section": "Ridge vs Lasso \\(R^2\\) vs MSE Ex 2",
    "text": "Ridge vs Lasso \\(R^2\\) vs MSE Ex 2\n\n\n\n\nComparison of squared bias, variance and test MSE between lasso (solid) and ridge (dashed). Both are plotted against their \\(R^2\\) on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest."
  },
  {
    "objectID": "slides/06-2-shrinkage_o.html#lasso-vs-ridge-summary",
    "href": "slides/06-2-shrinkage_o.html#lasso-vs-ridge-summary",
    "title": "Chapter 6 Part 2",
    "section": "Lasso vs Ridge Summary",
    "text": "Lasso vs Ridge Summary\n\nThese two examples illustrate that neither ridge regression nor the lasso will universally dominate the other.\nIn general, one might expect the lasso to perform better when the response is a function of only a relatively small number of predictors.\nHowever, the number of predictors that is related to the response is never known a priori for real data sets.\nA technique such as cross-validation can be used in order to determine which approach is better on a particular data set."
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#setup",
    "href": "slides/06-3-lasso_ridge_tm.html#setup",
    "title": "Chapter 6 Part 3",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ISLR2)"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#lasso-and-ridge---statquest",
    "href": "slides/06-3-lasso_ridge_tm.html#lasso-and-ridge---statquest",
    "title": "Chapter 6 Part 3",
    "section": "Lasso and Ridge - Statquest",
    "text": "Lasso and Ridge - Statquest\nhttps://www.youtube.com/watch?v=Q81RR3yKn30&t=7s\nhttps://www.youtube.com/watch?v=NGf0voTMlcs\nOn your own:\nRidge Vs Lasso https://www.youtube.com/watch?v=Xm2C_gTAl8c\nElastic Net https://www.youtube.com/watch?v=1dKRdX9bfIo"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#ridge-regression",
    "href": "slides/06-3-lasso_ridge_tm.html#ridge-regression",
    "title": "Chapter 6 Part 3",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\n\nPros\n\nCan be used when \\(p &gt; n\\)\nCan be used to help with multicollinearity\nWill decrease variance (as \\(\\lambda \\rightarrow \\infty\\) )\n\n\nCons\n\nWill have increased bias (compared to least squares)\nDoes not really help with variable selection (all variables are included in some regard, even if their \\(\\beta\\) coefficients are really small)"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#lasso",
    "href": "slides/06-3-lasso_ridge_tm.html#lasso",
    "title": "Chapter 6 Part 3",
    "section": "Lasso",
    "text": "Lasso\n\n\nPros\n\nCan be used when \\(p &gt; n\\)\nCan be used to help with multicollinearity\nWill decrease variance (as \\(\\lambda \\rightarrow \\infty\\) )\nCan be used for variable selection, since it will make some \\(\\beta\\) coefficients exactly 0\n\n\nCons\n\nWill have increased bias (compared to least squares)\nIf \\(p&gt;n\\) the lasso can select at most \\(n\\) variables"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#what-if-we-want-to-do-both",
    "href": "slides/06-3-lasso_ridge_tm.html#what-if-we-want-to-do-both",
    "title": "Chapter 6 Part 3",
    "section": "What if we want to do both?",
    "text": "What if we want to do both?\n\nElastic net!\n\\(RSS + \\lambda_1\\sum_{j=1}^p\\beta^2_j+\\lambda_2\\sum_{j=1}^p|\\beta_j|\\)\n\n\n\nWhat is the \\(\\ell_1\\) part of the penalty?\n\n\n\n\nWhat is the \\(\\ell_2\\) part of the penalty"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#elastic-net",
    "href": "slides/06-3-lasso_ridge_tm.html#elastic-net",
    "title": "Chapter 6 Part 3",
    "section": "Elastic net",
    "text": "Elastic net\n\\[RSS + \\lambda_1\\sum_{j=1}^p\\beta^2_j+\\lambda_2\\sum_{j=1}^p|\\beta_j|\\]\n\nWhen will this be equivalent to Ridge Regression?"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#elastic-net-1",
    "href": "slides/06-3-lasso_ridge_tm.html#elastic-net-1",
    "title": "Chapter 6 Part 3",
    "section": "Elastic net",
    "text": "Elastic net\n\\[RSS + \\lambda_1\\sum_{j=1}^p\\beta^2_j+\\lambda_2\\sum_{j=1}^p|\\beta_j|\\]\n\nWhen will this be equivalent to Lasso?"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#elastic-net-2",
    "href": "slides/06-3-lasso_ridge_tm.html#elastic-net-2",
    "title": "Chapter 6 Part 3",
    "section": "Elastic Net",
    "text": "Elastic Net\n\\[RSS + \\lambda_1\\sum_{j=1}^p\\beta^2_j+\\lambda_2\\sum_{j=1}^p|\\beta_j|\\]\n\nThe \\(\\ell_1\\) part of the penalty will generate a sparse model (shrink some \\(\\beta\\) coefficients to exactly 0)\nThe \\(\\ell_2\\) part of the penalty removes the limitation on the number of variables selected (can be \\(&gt;n\\) now)"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#tidymodels",
    "href": "slides/06-3-lasso_ridge_tm.html#tidymodels",
    "title": "Chapter 6 Part 3",
    "section": "tidymodels",
    "text": "tidymodels\n\nlm_spec &lt;- \n  linear_reg() |&gt; # Pick linear regression\n  set_engine(engine = \"lm\") # set engine\nlm_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\nlm_fit &lt;- fit(lm_spec,\n              mpg ~ horsepower,\n              data = Auto)"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#validation-set-approach",
    "href": "slides/06-3-lasso_ridge_tm.html#validation-set-approach",
    "title": "Chapter 6 Part 3",
    "section": "Validation set approach",
    "text": "Validation set approach\n\nAuto_split &lt;- initial_split(Auto, prop = 0.5)\nAuto_split\n\n&lt;Training/Testing/Total&gt;\n&lt;196/196/392&gt;\n\n\n\nExtract the training and testing data\n\ntraining(Auto_split)\ntesting(Auto_split)"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#a-faster-way",
    "href": "slides/06-3-lasso_ridge_tm.html#a-faster-way",
    "title": "Chapter 6 Part 3",
    "section": "A faster way!",
    "text": "A faster way!\n\nYou can use last_fit() and specify the split\nThis will automatically train the data on the train data from the split\nInstead of specifying which metric to calculate (with rmse as before) you can just use collect_metrics() and it will automatically calculate the metrics on the test data from the split"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#a-faster-way-1",
    "href": "slides/06-3-lasso_ridge_tm.html#a-faster-way-1",
    "title": "Chapter 6 Part 3",
    "section": "A faster way!",
    "text": "A faster way!\n\nset.seed(100000)\n\nAuto_split &lt;- initial_split(Auto, prop = 0.5)\nlm_fit &lt;- last_fit(lm_spec,\n                   mpg ~ horsepower,\n                   split = Auto_split) \n\nlm_fit |&gt;\n  collect_metrics() \n\n# A tibble: 2 √ó 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       4.77  Preprocessor1_Model1\n2 rsq     standard       0.634 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#what-about-cross-validation",
    "href": "slides/06-3-lasso_ridge_tm.html#what-about-cross-validation",
    "title": "Chapter 6 Part 3",
    "section": "What about cross validation?",
    "text": "What about cross validation?\n\nAuto_cv &lt;- vfold_cv(Auto, v = 5)\nAuto_cv\n\n#  5-fold cross-validation \n# A tibble: 5 √ó 2\n  splits           id   \n  &lt;list&gt;           &lt;chr&gt;\n1 &lt;split [313/79]&gt; Fold1\n2 &lt;split [313/79]&gt; Fold2\n3 &lt;split [314/78]&gt; Fold3\n4 &lt;split [314/78]&gt; Fold4\n5 &lt;split [314/78]&gt; Fold5"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#what-if-we-wanted-to-do-some-preprocessing",
    "href": "slides/06-3-lasso_ridge_tm.html#what-if-we-wanted-to-do-some-preprocessing",
    "title": "Chapter 6 Part 3",
    "section": "What if we wanted to do some preprocessing",
    "text": "What if we wanted to do some preprocessing\n\nFor the shrinkage methods we discussed it was important to scale the variables\n\n\n\n\nWhat would happen if we scale before doing cross-validation? Will we get different answers?"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#what-if-we-wanted-to-do-some-preprocessing-1",
    "href": "slides/06-3-lasso_ridge_tm.html#what-if-we-wanted-to-do-some-preprocessing-1",
    "title": "Chapter 6 Part 3",
    "section": "What if we wanted to do some preprocessing",
    "text": "What if we wanted to do some preprocessing\n\nAuto_scaled &lt;- Auto |&gt;\n  mutate(horsepower = scale(horsepower))\n\nsd(Auto_scaled$horsepower)\n\n[1] 1\n\n\n\nAuto_cv_scaled &lt;- vfold_cv(Auto_scaled, v = 5)\n\n# Will not actually use:\nmap_dbl(Auto_cv_scaled$splits,\n        function(x) {\n          dat &lt;- as.data.frame(x)$horsepower\n          sd(dat)\n        })\n\n[1] 0.9698961 1.0060047 0.9967524 1.0339029 0.9922419"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#what-if-we-wanted-to-do-some-preprocessing-2",
    "href": "slides/06-3-lasso_ridge_tm.html#what-if-we-wanted-to-do-some-preprocessing-2",
    "title": "Chapter 6 Part 3",
    "section": "What if we wanted to do some preprocessing",
    "text": "What if we wanted to do some preprocessing\n\nrecipe()!\nUsing the recipe() function along with step_*() functions, we can specify preprocessing steps and R will automagically apply them to each fold appropriately.\n\n\n\nrec &lt;- recipe(mpg ~ horsepower, data = Auto) |&gt;\n  step_scale(horsepower) \n\n\nYou can find all of the potential preprocessing steps here: https://tidymodels.github.io/recipes/reference/index.html"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#where-do-we-plug-in-this-recipe",
    "href": "slides/06-3-lasso_ridge_tm.html#where-do-we-plug-in-this-recipe",
    "title": "Chapter 6 Part 3",
    "section": "Where do we plug in this recipe?",
    "text": "Where do we plug in this recipe?\n\nThe recipe gets plugged into the fit_resamples() function\n\n\n\nAuto_cv &lt;- vfold_cv(Auto, v = 5)\n\nrec &lt;- recipe(mpg ~ horsepower, data = Auto) |&gt;\n  step_scale(horsepower)\n\nresults &lt;- fit_resamples(lm_spec,\n                         preprocessor = rec,\n                         resamples = Auto_cv)\n\nresults |&gt;\n  collect_metrics()\n\n# A tibble: 2 √ó 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4.92      5  0.0744 Preprocessor1_Model1\n2 rsq     standard   0.611     5  0.0158 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#what-if-we-want-to-predict-mpg-with-more-variables",
    "href": "slides/06-3-lasso_ridge_tm.html#what-if-we-want-to-predict-mpg-with-more-variables",
    "title": "Chapter 6 Part 3",
    "section": "What if we want to predict mpg with more variables",
    "text": "What if we want to predict mpg with more variables\n\nNow we still want to add a step to scale predictors\nWe could either write out all predictors individually to scale them\nOR we could use the all_predictors() short hand.\n\n\n\nrec &lt;- recipe(mpg ~ horsepower + displacement + weight, data = Auto) |&gt;\n  step_scale(all_predictors())"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#putting-it-together",
    "href": "slides/06-3-lasso_ridge_tm.html#putting-it-together",
    "title": "Chapter 6 Part 3",
    "section": "Putting it together",
    "text": "Putting it together\n\nrec &lt;- recipe(mpg ~ horsepower + displacement + weight, data = Auto) |&gt;\n  step_scale(all_predictors())\n\nresults &lt;- fit_resamples(lm_spec,\n                         preprocessor = rec,\n                         resamples = Auto_cv)\n\nresults |&gt;\n  collect_metrics()\n\n# A tibble: 2 √ó 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4.23      5  0.157  Preprocessor1_Model1\n2 rsq     standard   0.704     5  0.0253 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#application-exercise",
    "href": "slides/06-3-lasso_ridge_tm.html#application-exercise",
    "title": "Chapter 6 Part 3",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nExamine the Hitters dataset by running ?Hitters in the Console\nWe want to predict a major league player‚Äôs Salary from all of the other 19 variables in this dataset. Create a visualization of Salary.\nCreate a recipe to estimate this model.\nAdd a preprocessing step to your recipe, scaling each of the predictors"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#what-if-we-have-categorical-variables",
    "href": "slides/06-3-lasso_ridge_tm.html#what-if-we-have-categorical-variables",
    "title": "Chapter 6 Part 3",
    "section": "What if we have categorical variables?",
    "text": "What if we have categorical variables?\n\nWe can turn the categorical variables into indicator (‚Äúdummy‚Äù) variables in the recipe\n\n\n\nrec &lt;- recipe(mpg ~ horsepower + displacement + weight, data = Auto) |&gt;\n  step_dummy(all_nominal()) |&gt;\n  step_scale(all_predictors())"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#what-if-we-have-missing-data",
    "href": "slides/06-3-lasso_ridge_tm.html#what-if-we-have-missing-data",
    "title": "Chapter 6 Part 3",
    "section": "What if we have missing data?",
    "text": "What if we have missing data?\n\nWe can remove any rows with missing data\n\n\n\nrec &lt;- recipe(mpg ~ horsepower + displacement + weight, data = Auto) |&gt;\n  step_dummy(all_nominal()) |&gt;\n  step_naomit(everything()) |&gt;\n  step_scale(all_predictors())"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#what-if-we-have-missing-data-1",
    "href": "slides/06-3-lasso_ridge_tm.html#what-if-we-have-missing-data-1",
    "title": "Chapter 6 Part 3",
    "section": "What if we have missing data?",
    "text": "What if we have missing data?\n\nrec &lt;- recipe(mpg ~ horsepower + displacement + weight, data = Auto) |&gt;\n  step_dummy(all_nominal()) |&gt;\n  step_naomit(all_outcomes()) |&gt;\n  step_impute_mean(all_predictors()) |&gt;\n  step_scale(all_predictors())"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#application-exercise-1",
    "href": "slides/06-3-lasso_ridge_tm.html#application-exercise-1",
    "title": "Chapter 6 Part 3",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nAdd a preprocessing step to your recipe to convert nominal variables into indicators\nAdd a step to your recipe to remove missing values for the outcome\nAdd a step to your recipe to impute missing values for the predictors using the average for the remaining values NOTE THIS IS NOT THE BEST WAY TO DO THIS!"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#ridge-lasso-and-elastic-net",
    "href": "slides/06-3-lasso_ridge_tm.html#ridge-lasso-and-elastic-net",
    "title": "Chapter 6 Part 3",
    "section": "Ridge, Lasso, and Elastic net",
    "text": "Ridge, Lasso, and Elastic net\nWhen specifying your model, you can indicate whether you would like to use ridge, lasso, or elastic net. We can write a general equation to minimize:\n\\[RSS + \\lambda\\left((1-\\alpha)\\sum_{i=1}^p\\beta_j^2+\\alpha\\sum_{i=1}^p|\\beta_j|\\right)\\]\n\nlm_spec &lt;- linear_reg() |&gt;\n  set_engine(\"glmnet\") \n\n\nFirst specify the engine. We‚Äôll use glmnet\nThe linear_reg() function has two additional parameters, penalty and mixture\npenalty is \\(\\lambda\\) from our equation.\nmixture is a number between 0 and 1 representing \\(\\alpha\\)"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#ridge-lasso-and-elastic-net-1",
    "href": "slides/06-3-lasso_ridge_tm.html#ridge-lasso-and-elastic-net-1",
    "title": "Chapter 6 Part 3",
    "section": "Ridge, Lasso, and Elastic net",
    "text": "Ridge, Lasso, and Elastic net\n\\[RSS + \\lambda\\left((1-\\alpha)\\sum_{i=1}^p\\beta_j^2+\\alpha\\sum_{i=1}^p|\\beta_j|\\right)\\]\n\nWhat would we set mixture to in order to perform Ridge regression?\n\n\n\nridge_spec &lt;- linear_reg(penalty = 100, mixture = 0) |&gt; \n  set_engine(\"glmnet\")"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#application-exercise-2",
    "href": "slides/06-3-lasso_ridge_tm.html#application-exercise-2",
    "title": "Chapter 6 Part 3",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nSet a seed set.seed(1)\nCreate a cross validation object for the Hitters dataset\nUsing the recipe from the previous exercise, fit the model using Ridge regression with a penalty \\(\\lambda\\) = 300\nWhat is the estimate of the test RMSE for this model?"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#ridge-lasso-and-elastic-net-2",
    "href": "slides/06-3-lasso_ridge_tm.html#ridge-lasso-and-elastic-net-2",
    "title": "Chapter 6 Part 3",
    "section": "Ridge, Lasso, and Elastic net",
    "text": "Ridge, Lasso, and Elastic net\n\\[RSS + \\lambda\\left((1-\\alpha)\\sum_{i=1}^p\\beta_j^2+\\alpha\\sum_{i=1}^p|\\beta_j|\\right)\\]\n\nridge_spec &lt;- linear_reg(penalty = 100, mixture = 0) |&gt; \n  set_engine(\"glmnet\") \n\n\n\nlasso_spec &lt;- linear_reg(penalty = 5, mixture = 1) |&gt;\n  set_engine(\"glmnet\") \n\n\n\n\nenet_spec &lt;- linear_reg(penalty = 60, mixture = 0.7) |&gt; \n  set_engine(\"glmnet\")"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#okay-but-we-wanted-to-look-at-3-different-models",
    "href": "slides/06-3-lasso_ridge_tm.html#okay-but-we-wanted-to-look-at-3-different-models",
    "title": "Chapter 6 Part 3",
    "section": "Okay, but we wanted to look at 3 different models!",
    "text": "Okay, but we wanted to look at 3 different models!\n\nridge_spec &lt;- linear_reg(penalty = 100, mixture = 0) |&gt;\n  set_engine(\"glmnet\") \n\nresults &lt;- fit_resamples(ridge_spec,\n                         preprocessor = rec,\n                         resamples = Auto_cv)\n\n\n\nlasso_spec &lt;- linear_reg(penalty = 5, mixture = 1) |&gt;\n  set_engine(\"glmnet\") \n\nresults &lt;- fit_resamples(lasso_spec,\n                         preprocessor = rec,\n                         resamples = Auto_cv)\n\n\n\n\nelastic_spec &lt;- linear_reg(penalty = 40, mixture = 0.1) |&gt;\n  set_engine(\"glmnet\") \n\nresults &lt;- fit_resamples(elastic_spec,\n                         preprocessor = rec,\n                         resamples = Auto_cv)\n\n\nüò± this looks like copy + pasting!"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#tune",
    "href": "slides/06-3-lasso_ridge_tm.html#tune",
    "title": "Chapter 6 Part 3",
    "section": "tune üé∂",
    "text": "tune üé∂\n\npenalty_spec &lt;- \n  linear_reg(penalty = tune(), mixture = tune()) |&gt; \n  set_engine(\"glmnet\") \n\n\nNotice the code above has tune() for the the penalty and the mixture. Those are the things we want to vary!"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#tune-1",
    "href": "slides/06-3-lasso_ridge_tm.html#tune-1",
    "title": "Chapter 6 Part 3",
    "section": "tune üé∂",
    "text": "tune üé∂\n\nNow we need to create a grid of potential penalties ( \\(\\lambda\\) ) and mixtures ( \\(\\alpha\\) ) that we want to test\nInstead of fit_resamples() we are going to use tune_grid()\n\n\n\ngrid &lt;- expand_grid(penalty = seq(0, 100, by = 10),\n                    mixture = seq(0, 1, by = 0.25))\n\nresults &lt;- tune_grid(penalty_spec,\n                     preprocessor = rec,\n                     grid = grid, \n                     resamples = Auto_cv)"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#tune-2",
    "href": "slides/06-3-lasso_ridge_tm.html#tune-2",
    "title": "Chapter 6 Part 3",
    "section": "tune üé∂",
    "text": "tune üé∂\n\nresults |&gt;\n  collect_metrics()\n\n# A tibble: 110 √ó 8\n   penalty mixture .metric .estimator  mean     n std_err .config              \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1       0       0 rmse    standard   4.30      1      NA Preprocessor1_Model01\n 2       0       0 rsq     standard   0.699     1      NA Preprocessor1_Model01\n 3      10       0 rmse    standard   4.86      1      NA Preprocessor1_Model02\n 4      10       0 rsq     standard   0.692     1      NA Preprocessor1_Model02\n 5      20       0 rmse    standard   5.41      1      NA Preprocessor1_Model03\n 6      20       0 rsq     standard   0.691     1      NA Preprocessor1_Model03\n 7      30       0 rmse    standard   5.81      1      NA Preprocessor1_Model04\n 8      30       0 rsq     standard   0.691     1      NA Preprocessor1_Model04\n 9      40       0 rmse    standard   6.10      1      NA Preprocessor1_Model05\n10      40       0 rsq     standard   0.691     1      NA Preprocessor1_Model05\n# ‚Ñπ 100 more rows"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#subset-results",
    "href": "slides/06-3-lasso_ridge_tm.html#subset-results",
    "title": "Chapter 6 Part 3",
    "section": "Subset results",
    "text": "Subset results\n\nresults |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 55 √ó 8\n   penalty mixture .metric .estimator  mean     n std_err .config              \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1       0    0.25 rmse    standard    4.26     1      NA Preprocessor1_Model12\n 2       0    0.5  rmse    standard    4.26     1      NA Preprocessor1_Model23\n 3       0    1    rmse    standard    4.26     1      NA Preprocessor1_Model45\n 4       0    0.75 rmse    standard    4.27     1      NA Preprocessor1_Model34\n 5       0    0    rmse    standard    4.30     1      NA Preprocessor1_Model01\n 6      10    0    rmse    standard    4.86     1      NA Preprocessor1_Model02\n 7      20    0    rmse    standard    5.41     1      NA Preprocessor1_Model03\n 8      10    0.25 rmse    standard    5.69     1      NA Preprocessor1_Model13\n 9      30    0    rmse    standard    5.81     1      NA Preprocessor1_Model04\n10      40    0    rmse    standard    6.10     1      NA Preprocessor1_Model05\n# ‚Ñπ 45 more rows\n\n\n\nSince this is a data frame, we can do things like filter and arrange!"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#subset-results-1",
    "href": "slides/06-3-lasso_ridge_tm.html#subset-results-1",
    "title": "Chapter 6 Part 3",
    "section": "Subset results",
    "text": "Subset results\n\nresults |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 55 √ó 8\n   penalty mixture .metric .estimator  mean     n std_err .config              \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1       0    0.25 rmse    standard    4.26     1      NA Preprocessor1_Model12\n 2       0    0.5  rmse    standard    4.26     1      NA Preprocessor1_Model23\n 3       0    1    rmse    standard    4.26     1      NA Preprocessor1_Model45\n 4       0    0.75 rmse    standard    4.27     1      NA Preprocessor1_Model34\n 5       0    0    rmse    standard    4.30     1      NA Preprocessor1_Model01\n 6      10    0    rmse    standard    4.86     1      NA Preprocessor1_Model02\n 7      20    0    rmse    standard    5.41     1      NA Preprocessor1_Model03\n 8      10    0.25 rmse    standard    5.69     1      NA Preprocessor1_Model13\n 9      30    0    rmse    standard    5.81     1      NA Preprocessor1_Model04\n10      40    0    rmse    standard    6.10     1      NA Preprocessor1_Model05\n# ‚Ñπ 45 more rows\n\n\n\nWhich would you choose?"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#section",
    "href": "slides/06-3-lasso_ridge_tm.html#section",
    "title": "Chapter 6 Part 3",
    "section": "",
    "text": "results |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(aes(penalty, mean, color = factor(mixture), group = factor(mixture))) +\n  geom_line() +\n  geom_point() + \n  labs(y = \"RMSE\")"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#application-exercise-3",
    "href": "slides/06-3-lasso_ridge_tm.html#application-exercise-3",
    "title": "Chapter 6 Part 3",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\n\nUsing the Hitters cross validation object and recipe created in the previous exercise, use tune_grid to pick the optimal penalty and mixture values.\nUpdate the code below to create a grid that includes penalties from 0 to 50 by 1 and mixtures from 0 to 1 by 0.5.\nUse this grid in the tune_grid function. Then use collect_metrics and filter to only include the RSME estimates.\nCreate a figure to examine the estimated test RMSE for the grid of penalty and mixture values ‚Äì which should you choose?\n\n\n\n\ngrid &lt;- expand_grid(penalty = seq(0, ----),\n                    mixture = seq(0, 1, by = ----))"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#putting-it-all-together",
    "href": "slides/06-3-lasso_ridge_tm.html#putting-it-all-together",
    "title": "Chapter 6 Part 3",
    "section": "Putting it all together",
    "text": "Putting it all together\n\nOften we can use a combination of all of these tools together\nFirst split our data\nDo cross validation on just the training data to tune the parameters\nUse last_fit() with the selected parameters, specifying the split data so that it is evaluated on the left out test sample"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#putting-it-all-together-1",
    "href": "slides/06-3-lasso_ridge_tm.html#putting-it-all-together-1",
    "title": "Chapter 6 Part 3",
    "section": "Putting it all together",
    "text": "Putting it all together\n\nauto_split &lt;- initial_split(Auto, prop = 0.5)\nauto_train &lt;- training(auto_split)\nauto_cv &lt;- vfold_cv(auto_train, v = 5)\n\nrec &lt;- recipe(mpg ~ horsepower + displacement + weight, data = auto_train) |&gt;\n  step_scale(all_predictors())\n\ntuning &lt;- tune_grid(penalty_spec,\n                     rec,\n                     grid = grid,\n                     resamples = auto_cv)\n\ntuning |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 66 √ó 8\n   penalty mixture .metric .estimator  mean     n std_err .config              \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1       0     1   rmse    standard    3.48     1      NA Preprocessor1_Model56\n 2       0     0.8 rmse    standard    3.48     1      NA Preprocessor1_Model45\n 3       0     0.6 rmse    standard    3.49     1      NA Preprocessor1_Model34\n 4       0     0.4 rmse    standard    3.49     1      NA Preprocessor1_Model23\n 5       0     0.2 rmse    standard    3.49     1      NA Preprocessor1_Model12\n 6       0     0   rmse    standard    3.63     1      NA Preprocessor1_Model01\n 7      10     0   rmse    standard    4.42     1      NA Preprocessor1_Model02\n 8      20     0   rmse    standard    5.02     1      NA Preprocessor1_Model03\n 9      10     0.2 rmse    standard    5.10     1      NA Preprocessor1_Model13\n10      30     0   rmse    standard    5.44     1      NA Preprocessor1_Model04\n# ‚Ñπ 56 more rows"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#putting-it-all-together-2",
    "href": "slides/06-3-lasso_ridge_tm.html#putting-it-all-together-2",
    "title": "Chapter 6 Part 3",
    "section": "Putting it all together",
    "text": "Putting it all together\n\nfinal_spec &lt;- linear_reg(penalty = 0, mixture = 0) |&gt;\n  set_engine(\"glmnet\")\nfit &lt;- last_fit(final_spec, \n                rec,\n                split = auto_split) \nfit |&gt;\n  collect_metrics()\n\n# A tibble: 2 √ó 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       4.45  Preprocessor1_Model1\n2 rsq     standard       0.691 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#extracting-coefficients",
    "href": "slides/06-3-lasso_ridge_tm.html#extracting-coefficients",
    "title": "Chapter 6 Part 3",
    "section": "Extracting coefficients",
    "text": "Extracting coefficients\n\nWe can use workflow() to combine the recipe and the model specification to pass to a fit object.\n\n\n\ntraining_data &lt;- training(auto_split)\n\nworkflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(final_spec) |&gt;\n  fit(data = training_data) |&gt;\n  tidy()\n\n# A tibble: 4 √ó 3\n  term         estimate penalty\n  &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)     42.6        0\n2 horsepower      -1.41       0\n3 displacement    -1.44       0\n4 weight          -3.62       0"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#application-exercise-4",
    "href": "slides/06-3-lasso_ridge_tm.html#application-exercise-4",
    "title": "Chapter 6 Part 3",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\n\nUsing the final model specification, extract the coefficients from the model by creating a workflow\nFilter out any coefficients exactly equal to 0\n\n\n\n\n\n\n\nüîó https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm.html#tune-autoplot",
    "href": "slides/06-3-lasso_ridge_tm.html#tune-autoplot",
    "title": "Chapter 6 Part 3",
    "section": "tune autoplot",
    "text": "tune autoplot\n\nautoplot(results)+ ## ggplot function\n  theme_classic()"
  },
  {
    "objectID": "labs/04-ridge-lasso-elastic.html",
    "href": "labs/04-ridge-lasso-elastic.html",
    "title": "Lab 04 - Ridge Lasso Elastic Net",
    "section": "",
    "text": "Go to our RStudio and create a new R project inside your class folder. Create a .qmd file for your lab.\n\n\nNow, make sure the author is your name, title is appropriate and Render the document."
  },
  {
    "objectID": "labs/04-ridge-lasso-elastic.html#yaml",
    "href": "labs/04-ridge-lasso-elastic.html#yaml",
    "title": "Lab 04 - Ridge Lasso Elastic Net",
    "section": "",
    "text": "Now, make sure the author is your name, title is appropriate and Render the document."
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html",
    "href": "slides/06-3-lasso_ridge_tm_o.html",
    "title": "Chapter 6 Part 3",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(ISLR2)"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#setup",
    "href": "slides/06-3-lasso_ridge_tm_o.html#setup",
    "title": "Chapter 6 Part 3",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(ISLR2)"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#lasso-and-ridge---statquest",
    "href": "slides/06-3-lasso_ridge_tm_o.html#lasso-and-ridge---statquest",
    "title": "Chapter 6 Part 3",
    "section": "Lasso and Ridge - Statquest",
    "text": "Lasso and Ridge - Statquest\nhttps://www.youtube.com/watch?v=Q81RR3yKn30&t=7s\nhttps://www.youtube.com/watch?v=NGf0voTMlcs\nOn your own:\nRidge Vs Lasso https://www.youtube.com/watch?v=Xm2C_gTAl8c\nElastic Net https://www.youtube.com/watch?v=1dKRdX9bfIo"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#ridge-regression",
    "href": "slides/06-3-lasso_ridge_tm_o.html#ridge-regression",
    "title": "Chapter 6 Part 3",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\n\n\nPros\n\nCan be used when \\(p &gt; n\\)\nCan be used to help with multicollinearity\nWill decrease variance (as \\(\\lambda \\rightarrow \\infty\\) )\n\n\n\n\nCons\n\nWill have increased bias (compared to least squares)\nDoes not really help with variable selection (all variables are included in some regard, even if their \\(\\beta\\) coefficients are really small)"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#lasso",
    "href": "slides/06-3-lasso_ridge_tm_o.html#lasso",
    "title": "Chapter 6 Part 3",
    "section": "Lasso",
    "text": "Lasso\n\n\n\nPros\n\nCan be used when \\(p &gt; n\\)\nCan be used to help with multicollinearity\nWill decrease variance (as \\(\\lambda \\rightarrow \\infty\\) )\nCan be used for variable selection, since it will make some \\(\\beta\\) coefficients exactly 0\n\n\n\n\nCons\n\nWill have increased bias (compared to least squares)\nIf \\(p&gt;n\\) the lasso can select at most \\(n\\) variables"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#what-if-we-want-to-do-both",
    "href": "slides/06-3-lasso_ridge_tm_o.html#what-if-we-want-to-do-both",
    "title": "Chapter 6 Part 3",
    "section": "What if we want to do both?",
    "text": "What if we want to do both?\n\nElastic net!\n\\(RSS + \\lambda_1\\sum_{j=1}^p\\beta^2_j+\\lambda_2\\sum_{j=1}^p|\\beta_j|\\)\n\n. . .\n\nWhat is the \\(\\ell_1\\) part of the penalty?\n\n. . .\n\nWhat is the \\(\\ell_2\\) part of the penalty"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#elastic-net",
    "href": "slides/06-3-lasso_ridge_tm_o.html#elastic-net",
    "title": "Chapter 6 Part 3",
    "section": "Elastic net",
    "text": "Elastic net\n\\[RSS + \\lambda_1\\sum_{j=1}^p\\beta^2_j+\\lambda_2\\sum_{j=1}^p|\\beta_j|\\]\n\nWhen will this be equivalent to Ridge Regression?"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#elastic-net-1",
    "href": "slides/06-3-lasso_ridge_tm_o.html#elastic-net-1",
    "title": "Chapter 6 Part 3",
    "section": "Elastic net",
    "text": "Elastic net\n\\[RSS + \\lambda_1\\sum_{j=1}^p\\beta^2_j+\\lambda_2\\sum_{j=1}^p|\\beta_j|\\]\n\nWhen will this be equivalent to Lasso?"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#elastic-net-2",
    "href": "slides/06-3-lasso_ridge_tm_o.html#elastic-net-2",
    "title": "Chapter 6 Part 3",
    "section": "Elastic Net",
    "text": "Elastic Net\n\\[RSS + \\lambda_1\\sum_{j=1}^p\\beta^2_j+\\lambda_2\\sum_{j=1}^p|\\beta_j|\\]\n\nThe \\(\\ell_1\\) part of the penalty will generate a sparse model (shrink some \\(\\beta\\) coefficients to exactly 0)\nThe \\(\\ell_2\\) part of the penalty removes the limitation on the number of variables selected (can be \\(&gt;n\\) now)"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#tidymodels",
    "href": "slides/06-3-lasso_ridge_tm_o.html#tidymodels",
    "title": "Chapter 6 Part 3",
    "section": "tidymodels",
    "text": "tidymodels\n\nlm_spec &lt;- \n  linear_reg() |&gt; # Pick linear regression\n  set_engine(engine = \"lm\") # set engine\nlm_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\nlm_fit &lt;- fit(lm_spec,\n              mpg ~ horsepower,\n              data = Auto)"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#validation-set-approach",
    "href": "slides/06-3-lasso_ridge_tm_o.html#validation-set-approach",
    "title": "Chapter 6 Part 3",
    "section": "Validation set approach",
    "text": "Validation set approach\n\nAuto_split &lt;- initial_split(Auto, prop = 0.5)\nAuto_split\n\n&lt;Training/Testing/Total&gt;\n&lt;196/196/392&gt;\n\n\n. . .\nExtract the training and testing data\n\ntraining(Auto_split)\ntesting(Auto_split)"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#a-faster-way",
    "href": "slides/06-3-lasso_ridge_tm_o.html#a-faster-way",
    "title": "Chapter 6 Part 3",
    "section": "A faster way!",
    "text": "A faster way!\n\nYou can use last_fit() and specify the split\nThis will automatically train the data on the train data from the split\nInstead of specifying which metric to calculate (with rmse as before) you can just use collect_metrics() and it will automatically calculate the metrics on the test data from the split"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#a-faster-way-1",
    "href": "slides/06-3-lasso_ridge_tm_o.html#a-faster-way-1",
    "title": "Chapter 6 Part 3",
    "section": "A faster way!",
    "text": "A faster way!\n\nset.seed(100000)\n\nAuto_split &lt;- initial_split(Auto, prop = 0.5)\nlm_fit &lt;- last_fit(lm_spec,\n                   mpg ~ horsepower,\n                   split = Auto_split) \n\nlm_fit |&gt;\n  collect_metrics() \n\n# A tibble: 2 √ó 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       4.77  Preprocessor1_Model1\n2 rsq     standard       0.634 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#what-about-cross-validation",
    "href": "slides/06-3-lasso_ridge_tm_o.html#what-about-cross-validation",
    "title": "Chapter 6 Part 3",
    "section": "What about cross validation?",
    "text": "What about cross validation?\n\nAuto_cv &lt;- vfold_cv(Auto, v = 5)\nAuto_cv\n\n#  5-fold cross-validation \n# A tibble: 5 √ó 2\n  splits           id   \n  &lt;list&gt;           &lt;chr&gt;\n1 &lt;split [313/79]&gt; Fold1\n2 &lt;split [313/79]&gt; Fold2\n3 &lt;split [314/78]&gt; Fold3\n4 &lt;split [314/78]&gt; Fold4\n5 &lt;split [314/78]&gt; Fold5"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#what-if-we-wanted-to-do-some-preprocessing",
    "href": "slides/06-3-lasso_ridge_tm_o.html#what-if-we-wanted-to-do-some-preprocessing",
    "title": "Chapter 6 Part 3",
    "section": "What if we wanted to do some preprocessing",
    "text": "What if we wanted to do some preprocessing\n\nFor the shrinkage methods we discussed it was important to scale the variables\n\n. . .\n\n\nWhat would happen if we scale before doing cross-validation? Will we get different answers?"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#what-if-we-wanted-to-do-some-preprocessing-1",
    "href": "slides/06-3-lasso_ridge_tm_o.html#what-if-we-wanted-to-do-some-preprocessing-1",
    "title": "Chapter 6 Part 3",
    "section": "What if we wanted to do some preprocessing",
    "text": "What if we wanted to do some preprocessing\n\nAuto_scaled &lt;- Auto |&gt;\n  mutate(horsepower = scale(horsepower))\n\nsd(Auto_scaled$horsepower)\n\n[1] 1\n\n\n\nAuto_cv_scaled &lt;- vfold_cv(Auto_scaled, v = 5)\n\n# Will not actually use:\nmap_dbl(Auto_cv_scaled$splits,\n        function(x) {\n          dat &lt;- as.data.frame(x)$horsepower\n          sd(dat)\n        })\n\n[1] 0.9698961 1.0060047 0.9967524 1.0339029 0.9922419"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#what-if-we-wanted-to-do-some-preprocessing-2",
    "href": "slides/06-3-lasso_ridge_tm_o.html#what-if-we-wanted-to-do-some-preprocessing-2",
    "title": "Chapter 6 Part 3",
    "section": "What if we wanted to do some preprocessing",
    "text": "What if we wanted to do some preprocessing\n\nrecipe()!\nUsing the recipe() function along with step_*() functions, we can specify preprocessing steps and R will automagically apply them to each fold appropriately.\n\n. . .\n\nrec &lt;- recipe(mpg ~ horsepower, data = Auto) |&gt;\n  step_scale(horsepower) \n\n\nYou can find all of the potential preprocessing steps here: https://tidymodels.github.io/recipes/reference/index.html"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#where-do-we-plug-in-this-recipe",
    "href": "slides/06-3-lasso_ridge_tm_o.html#where-do-we-plug-in-this-recipe",
    "title": "Chapter 6 Part 3",
    "section": "Where do we plug in this recipe?",
    "text": "Where do we plug in this recipe?\n\nThe recipe gets plugged into the fit_resamples() function\n\n. . .\n\nAuto_cv &lt;- vfold_cv(Auto, v = 5)\n\nrec &lt;- recipe(mpg ~ horsepower, data = Auto) |&gt;\n  step_scale(horsepower)\n\nresults &lt;- fit_resamples(lm_spec,\n                         preprocessor = rec,\n                         resamples = Auto_cv)\n\nresults |&gt;\n  collect_metrics()\n\n# A tibble: 2 √ó 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4.92      5  0.0744 Preprocessor1_Model1\n2 rsq     standard   0.611     5  0.0158 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#what-if-we-want-to-predict-mpg-with-more-variables",
    "href": "slides/06-3-lasso_ridge_tm_o.html#what-if-we-want-to-predict-mpg-with-more-variables",
    "title": "Chapter 6 Part 3",
    "section": "What if we want to predict mpg with more variables",
    "text": "What if we want to predict mpg with more variables\n\nNow we still want to add a step to scale predictors\nWe could either write out all predictors individually to scale them\nOR we could use the all_predictors() short hand.\n\n. . .\n\nrec &lt;- recipe(mpg ~ horsepower + displacement + weight, data = Auto) |&gt;\n  step_scale(all_predictors())"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#putting-it-together",
    "href": "slides/06-3-lasso_ridge_tm_o.html#putting-it-together",
    "title": "Chapter 6 Part 3",
    "section": "Putting it together",
    "text": "Putting it together\n\nrec &lt;- recipe(mpg ~ horsepower + displacement + weight, data = Auto) |&gt;\n  step_scale(all_predictors())\n\nresults &lt;- fit_resamples(lm_spec,\n                         preprocessor = rec,\n                         resamples = Auto_cv)\n\nresults |&gt;\n  collect_metrics()\n\n# A tibble: 2 √ó 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4.23      5  0.157  Preprocessor1_Model1\n2 rsq     standard   0.704     5  0.0253 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#application-exercise",
    "href": "slides/06-3-lasso_ridge_tm_o.html#application-exercise",
    "title": "Chapter 6 Part 3",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nExamine the Hitters dataset by running ?Hitters in the Console\nWe want to predict a major league player‚Äôs Salary from all of the other 19 variables in this dataset. Create a visualization of Salary.\nCreate a recipe to estimate this model.\nAdd a preprocessing step to your recipe, scaling each of the predictors"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#what-if-we-have-categorical-variables",
    "href": "slides/06-3-lasso_ridge_tm_o.html#what-if-we-have-categorical-variables",
    "title": "Chapter 6 Part 3",
    "section": "What if we have categorical variables?",
    "text": "What if we have categorical variables?\n\nWe can turn the categorical variables into indicator (‚Äúdummy‚Äù) variables in the recipe\n\n. . .\n\nrec &lt;- recipe(mpg ~ horsepower + displacement + weight, data = Auto) |&gt;\n  step_dummy(all_nominal()) |&gt;\n  step_scale(all_predictors())"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#what-if-we-have-missing-data",
    "href": "slides/06-3-lasso_ridge_tm_o.html#what-if-we-have-missing-data",
    "title": "Chapter 6 Part 3",
    "section": "What if we have missing data?",
    "text": "What if we have missing data?\n\nWe can remove any rows with missing data\n\n. . .\n\nrec &lt;- recipe(mpg ~ horsepower + displacement + weight, data = Auto) |&gt;\n  step_dummy(all_nominal()) |&gt;\n  step_naomit(everything()) |&gt;\n  step_scale(all_predictors())"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#what-if-we-have-missing-data-1",
    "href": "slides/06-3-lasso_ridge_tm_o.html#what-if-we-have-missing-data-1",
    "title": "Chapter 6 Part 3",
    "section": "What if we have missing data?",
    "text": "What if we have missing data?\n\nrec &lt;- recipe(mpg ~ horsepower + displacement + weight, data = Auto) |&gt;\n  step_dummy(all_nominal()) |&gt;\n  step_naomit(all_outcomes()) |&gt;\n  step_impute_mean(all_predictors()) |&gt;\n  step_scale(all_predictors())"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#application-exercise-1",
    "href": "slides/06-3-lasso_ridge_tm_o.html#application-exercise-1",
    "title": "Chapter 6 Part 3",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nAdd a preprocessing step to your recipe to convert nominal variables into indicators\nAdd a step to your recipe to remove missing values for the outcome\nAdd a step to your recipe to impute missing values for the predictors using the average for the remaining values NOTE THIS IS NOT THE BEST WAY TO DO THIS!"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#ridge-lasso-and-elastic-net",
    "href": "slides/06-3-lasso_ridge_tm_o.html#ridge-lasso-and-elastic-net",
    "title": "Chapter 6 Part 3",
    "section": "Ridge, Lasso, and Elastic net",
    "text": "Ridge, Lasso, and Elastic net\nWhen specifying your model, you can indicate whether you would like to use ridge, lasso, or elastic net. We can write a general equation to minimize:\n\\[RSS + \\lambda\\left((1-\\alpha)\\sum_{i=1}^p\\beta_j^2+\\alpha\\sum_{i=1}^p|\\beta_j|\\right)\\]\n\nlm_spec &lt;- linear_reg() |&gt;\n  set_engine(\"glmnet\") \n\n\nFirst specify the engine. We‚Äôll use glmnet\nThe linear_reg() function has two additional parameters, penalty and mixture\npenalty is \\(\\lambda\\) from our equation.\nmixture is a number between 0 and 1 representing \\(\\alpha\\)"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#ridge-lasso-and-elastic-net-1",
    "href": "slides/06-3-lasso_ridge_tm_o.html#ridge-lasso-and-elastic-net-1",
    "title": "Chapter 6 Part 3",
    "section": "Ridge, Lasso, and Elastic net",
    "text": "Ridge, Lasso, and Elastic net\n\\[RSS + \\lambda\\left((1-\\alpha)\\sum_{i=1}^p\\beta_j^2+\\alpha\\sum_{i=1}^p|\\beta_j|\\right)\\]\n\nWhat would we set mixture to in order to perform Ridge regression?\n\n. . .\n\nridge_spec &lt;- linear_reg(penalty = 100, mixture = 0) |&gt; \n  set_engine(\"glmnet\")"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#application-exercise-2",
    "href": "slides/06-3-lasso_ridge_tm_o.html#application-exercise-2",
    "title": "Chapter 6 Part 3",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nSet a seed set.seed(1)\nCreate a cross validation object for the Hitters dataset\nUsing the recipe from the previous exercise, fit the model using Ridge regression with a penalty \\(\\lambda\\) = 300\nWhat is the estimate of the test RMSE for this model?"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#ridge-lasso-and-elastic-net-2",
    "href": "slides/06-3-lasso_ridge_tm_o.html#ridge-lasso-and-elastic-net-2",
    "title": "Chapter 6 Part 3",
    "section": "Ridge, Lasso, and Elastic net",
    "text": "Ridge, Lasso, and Elastic net\n\\[RSS + \\lambda\\left((1-\\alpha)\\sum_{i=1}^p\\beta_j^2+\\alpha\\sum_{i=1}^p|\\beta_j|\\right)\\]\n\nridge_spec &lt;- linear_reg(penalty = 100, mixture = 0) |&gt; \n  set_engine(\"glmnet\") \n\n. . .\n\nlasso_spec &lt;- linear_reg(penalty = 5, mixture = 1) |&gt;\n  set_engine(\"glmnet\") \n\n. . .\n\nenet_spec &lt;- linear_reg(penalty = 60, mixture = 0.7) |&gt; \n  set_engine(\"glmnet\")"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#okay-but-we-wanted-to-look-at-3-different-models",
    "href": "slides/06-3-lasso_ridge_tm_o.html#okay-but-we-wanted-to-look-at-3-different-models",
    "title": "Chapter 6 Part 3",
    "section": "Okay, but we wanted to look at 3 different models!",
    "text": "Okay, but we wanted to look at 3 different models!\n\nridge_spec &lt;- linear_reg(penalty = 100, mixture = 0) |&gt;\n  set_engine(\"glmnet\") \n\nresults &lt;- fit_resamples(ridge_spec,\n                         preprocessor = rec,\n                         resamples = Auto_cv)\n\n. . .\n\nlasso_spec &lt;- linear_reg(penalty = 5, mixture = 1) |&gt;\n  set_engine(\"glmnet\") \n\nresults &lt;- fit_resamples(lasso_spec,\n                         preprocessor = rec,\n                         resamples = Auto_cv)\n\n. . .\n\nelastic_spec &lt;- linear_reg(penalty = 40, mixture = 0.1) |&gt;\n  set_engine(\"glmnet\") \n\nresults &lt;- fit_resamples(elastic_spec,\n                         preprocessor = rec,\n                         resamples = Auto_cv)\n\n\nüò± this looks like copy + pasting!"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#tune",
    "href": "slides/06-3-lasso_ridge_tm_o.html#tune",
    "title": "Chapter 6 Part 3",
    "section": "tune üé∂",
    "text": "tune üé∂\n\npenalty_spec &lt;- \n  linear_reg(penalty = tune(), mixture = tune()) |&gt; \n  set_engine(\"glmnet\") \n\n\nNotice the code above has tune() for the the penalty and the mixture. Those are the things we want to vary!"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#tune-1",
    "href": "slides/06-3-lasso_ridge_tm_o.html#tune-1",
    "title": "Chapter 6 Part 3",
    "section": "tune üé∂",
    "text": "tune üé∂\n\nNow we need to create a grid of potential penalties ( \\(\\lambda\\) ) and mixtures ( \\(\\alpha\\) ) that we want to test\nInstead of fit_resamples() we are going to use tune_grid()\n\n. . .\n\ngrid &lt;- expand_grid(penalty = seq(0, 100, by = 10),\n                    mixture = seq(0, 1, by = 0.25))\n\nresults &lt;- tune_grid(penalty_spec,\n                     preprocessor = rec,\n                     grid = grid, \n                     resamples = Auto_cv)"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#tune-autoplot",
    "href": "slides/06-3-lasso_ridge_tm_o.html#tune-autoplot",
    "title": "Chapter 6 Part 3",
    "section": "tune autoplot",
    "text": "tune autoplot\n\nautoplot(results)+ ## ggplot function\n  theme_classic()"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#tune-2",
    "href": "slides/06-3-lasso_ridge_tm_o.html#tune-2",
    "title": "Chapter 6 Part 3",
    "section": "tune üé∂",
    "text": "tune üé∂\n\nresults |&gt;\n  collect_metrics()\n\n# A tibble: 110 √ó 8\n   penalty mixture .metric .estimator  mean     n std_err .config              \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1       0       0 rmse    standard   4.30      1      NA Preprocessor1_Model01\n 2       0       0 rsq     standard   0.699     1      NA Preprocessor1_Model01\n 3      10       0 rmse    standard   4.86      1      NA Preprocessor1_Model02\n 4      10       0 rsq     standard   0.692     1      NA Preprocessor1_Model02\n 5      20       0 rmse    standard   5.41      1      NA Preprocessor1_Model03\n 6      20       0 rsq     standard   0.691     1      NA Preprocessor1_Model03\n 7      30       0 rmse    standard   5.81      1      NA Preprocessor1_Model04\n 8      30       0 rsq     standard   0.691     1      NA Preprocessor1_Model04\n 9      40       0 rmse    standard   6.10      1      NA Preprocessor1_Model05\n10      40       0 rsq     standard   0.691     1      NA Preprocessor1_Model05\n# ‚Ñπ 100 more rows"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#subset-results",
    "href": "slides/06-3-lasso_ridge_tm_o.html#subset-results",
    "title": "Chapter 6 Part 3",
    "section": "Subset results",
    "text": "Subset results\n\nresults |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 55 √ó 8\n   penalty mixture .metric .estimator  mean     n std_err .config              \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1       0    0.25 rmse    standard    4.26     1      NA Preprocessor1_Model12\n 2       0    0.5  rmse    standard    4.26     1      NA Preprocessor1_Model23\n 3       0    1    rmse    standard    4.26     1      NA Preprocessor1_Model45\n 4       0    0.75 rmse    standard    4.27     1      NA Preprocessor1_Model34\n 5       0    0    rmse    standard    4.30     1      NA Preprocessor1_Model01\n 6      10    0    rmse    standard    4.86     1      NA Preprocessor1_Model02\n 7      20    0    rmse    standard    5.41     1      NA Preprocessor1_Model03\n 8      10    0.25 rmse    standard    5.69     1      NA Preprocessor1_Model13\n 9      30    0    rmse    standard    5.81     1      NA Preprocessor1_Model04\n10      40    0    rmse    standard    6.10     1      NA Preprocessor1_Model05\n# ‚Ñπ 45 more rows\n\n\n\nSince this is a data frame, we can do things like filter and arrange!"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#subset-results-1",
    "href": "slides/06-3-lasso_ridge_tm_o.html#subset-results-1",
    "title": "Chapter 6 Part 3",
    "section": "Subset results",
    "text": "Subset results\n\nresults |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 55 √ó 8\n   penalty mixture .metric .estimator  mean     n std_err .config              \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1       0    0.25 rmse    standard    4.26     1      NA Preprocessor1_Model12\n 2       0    0.5  rmse    standard    4.26     1      NA Preprocessor1_Model23\n 3       0    1    rmse    standard    4.26     1      NA Preprocessor1_Model45\n 4       0    0.75 rmse    standard    4.27     1      NA Preprocessor1_Model34\n 5       0    0    rmse    standard    4.30     1      NA Preprocessor1_Model01\n 6      10    0    rmse    standard    4.86     1      NA Preprocessor1_Model02\n 7      20    0    rmse    standard    5.41     1      NA Preprocessor1_Model03\n 8      10    0.25 rmse    standard    5.69     1      NA Preprocessor1_Model13\n 9      30    0    rmse    standard    5.81     1      NA Preprocessor1_Model04\n10      40    0    rmse    standard    6.10     1      NA Preprocessor1_Model05\n# ‚Ñπ 45 more rows\n\n\n\nWhich would you choose?"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#section",
    "href": "slides/06-3-lasso_ridge_tm_o.html#section",
    "title": "Chapter 6 Part 3",
    "section": "",
    "text": "results |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(aes(penalty, mean, color = factor(mixture), group = factor(mixture))) +\n  geom_line() +\n  geom_point() + \n  labs(y = \"RMSE\")"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#application-exercise-3",
    "href": "slides/06-3-lasso_ridge_tm_o.html#application-exercise-3",
    "title": "Chapter 6 Part 3",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\n\nUsing the Hitters cross validation object and recipe created in the previous exercise, use tune_grid to pick the optimal penalty and mixture values.\nUpdate the code below to create a grid that includes penalties from 0 to 50 by 1 and mixtures from 0 to 1 by 0.5.\nUse this grid in the tune_grid function. Then use collect_metrics and filter to only include the RSME estimates.\nCreate a figure to examine the estimated test RMSE for the grid of penalty and mixture values ‚Äì which should you choose?\n\n\n\n\ngrid &lt;- expand_grid(penalty = seq(0, ----),\n                    mixture = seq(0, 1, by = ----))"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#putting-it-all-together",
    "href": "slides/06-3-lasso_ridge_tm_o.html#putting-it-all-together",
    "title": "Chapter 6 Part 3",
    "section": "Putting it all together",
    "text": "Putting it all together\n\nOften we can use a combination of all of these tools together\nFirst split our data\nDo cross validation on just the training data to tune the parameters\nUse last_fit() with the selected parameters, specifying the split data so that it is evaluated on the left out test sample"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#putting-it-all-together-1",
    "href": "slides/06-3-lasso_ridge_tm_o.html#putting-it-all-together-1",
    "title": "Chapter 6 Part 3",
    "section": "Putting it all together",
    "text": "Putting it all together\n\nauto_split &lt;- initial_split(Auto, prop = 0.5)\nauto_train &lt;- training(auto_split)\nauto_cv &lt;- vfold_cv(auto_train, v = 5)\n\nrec &lt;- recipe(mpg ~ horsepower + displacement + weight, data = auto_train) |&gt;\n  step_scale(all_predictors())\n\ntuning &lt;- tune_grid(penalty_spec,\n                     rec,\n                     grid = grid,\n                     resamples = auto_cv)\n\ntuning |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 66 √ó 8\n   penalty mixture .metric .estimator  mean     n std_err .config              \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1       0     1   rmse    standard    3.48     1      NA Preprocessor1_Model56\n 2       0     0.8 rmse    standard    3.48     1      NA Preprocessor1_Model45\n 3       0     0.6 rmse    standard    3.49     1      NA Preprocessor1_Model34\n 4       0     0.4 rmse    standard    3.49     1      NA Preprocessor1_Model23\n 5       0     0.2 rmse    standard    3.49     1      NA Preprocessor1_Model12\n 6       0     0   rmse    standard    3.63     1      NA Preprocessor1_Model01\n 7      10     0   rmse    standard    4.42     1      NA Preprocessor1_Model02\n 8      20     0   rmse    standard    5.02     1      NA Preprocessor1_Model03\n 9      10     0.2 rmse    standard    5.10     1      NA Preprocessor1_Model13\n10      30     0   rmse    standard    5.44     1      NA Preprocessor1_Model04\n# ‚Ñπ 56 more rows"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#putting-it-all-together-2",
    "href": "slides/06-3-lasso_ridge_tm_o.html#putting-it-all-together-2",
    "title": "Chapter 6 Part 3",
    "section": "Putting it all together",
    "text": "Putting it all together\n\nfinal_spec &lt;- linear_reg(penalty = 0, mixture = 0) |&gt;\n  set_engine(\"glmnet\")\nfit &lt;- last_fit(final_spec, \n                rec,\n                split = auto_split) \nfit |&gt;\n  collect_metrics()\n\n# A tibble: 2 √ó 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       4.45  Preprocessor1_Model1\n2 rsq     standard       0.691 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#extracting-coefficients",
    "href": "slides/06-3-lasso_ridge_tm_o.html#extracting-coefficients",
    "title": "Chapter 6 Part 3",
    "section": "Extracting coefficients",
    "text": "Extracting coefficients\n\nWe can use workflow() to combine the recipe and the model specification to pass to a fit object.\n\n. . .\n\ntraining_data &lt;- training(auto_split)\n\nworkflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(final_spec) |&gt;\n  fit(data = training_data) |&gt;\n  tidy()\n\n# A tibble: 4 √ó 3\n  term         estimate penalty\n  &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)     42.6        0\n2 horsepower      -1.41       0\n3 displacement    -1.44       0\n4 weight          -3.62       0"
  },
  {
    "objectID": "slides/06-3-lasso_ridge_tm_o.html#application-exercise-4",
    "href": "slides/06-3-lasso_ridge_tm_o.html#application-exercise-4",
    "title": "Chapter 6 Part 3",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\n\nUsing the final model specification, extract the coefficients from the model by creating a workflow\nFilter out any coefficients exactly equal to 0"
  },
  {
    "objectID": "hw/hw-04-regression_sol.html",
    "href": "hw/hw-04-regression_sol.html",
    "title": "Homework 4 Solutions",
    "section": "",
    "text": "1.\nWe perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain p + 1 models, containing \\(0, 1, 2, . . . ,p\\) predictors.\nExplain your answers:\n(a) Which of the three models with k predictors has the smallest training RSS?\nBest subsets since it considers every possible combination. It is possible both forward and backward stepwise pick the same k predictor model and have the same training RSS.\n\nWhich of the three models with k predictors has the smallest test RSS?\n\nYou cannot determine this to be sure. It depends if the best subset choice on the training set overfit. If it did, one of the other approach models could perform better.\n\nTrue or False:\n\n\nThe predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by forward stepwise selection.\n\nTrue, FSR keeps all k predictors from the previous step and adds an additional variable.\n\nThe predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k + 1)- variable model identified by backward stepwise selection.\n\nTrue, BSR drops 1 variable form a k+1 predictor model and goes down to a k variable model. The k variables must be in the k+1 variable model.\n\nThe predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k + 1)- variable model identified by forward stepwise selection.\n\nFalse, not always true.\n\nThe predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by backward stepwise selection.\n\nFalse, not always true.\n\nThe predictors in the k-variable model identified by best subset are a subset of the predictors in the (k + 1)-variable model identified by best subset selection.\n\nFalse, not always true.\n\n\n8.\nIn this exercise, we will generate simulated data, and will then use this data to perform best subset selection.\n\nUse the rnorm() function to generate a predictor X of length n = 100, as well as a noise vector \\(\\epsilon\\) of length n = 100.\n\n\nX &lt;- rnorm(100, mean = 10, sd = 10) \nep &lt;- rnorm(100,sd = 30)\n\n\nGenerate a response vector \\(Y\\) of length \\(n=100\\) according to the model\n\n\\[Y=\\beta_0 + \\beta_1X+\\beta_2X^2+\\beta_3X^3+\\epsilon,\\]\nwhere \\(\\beta_0,\\beta_1,\\) and \\(\\beta_3\\) are constants of your choice.\n\nY = 5 + 3*X + 10*X^2+ 2*X^3 + ep\n\n\nUse the regsubsets() function to perform best subset selection in order to choose the best model containing the predictors \\(X\\),\\(X^2\\), . . . ,\\(X^{10}\\). What is the best model obtained according to \\(C_p\\), BIC, and adjusted \\(R^2\\)? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the data.frame() function to create a single data set containing both X and Y .\n\n\nsimdata &lt;- data.frame(ysim = Y,xsim = X)\n\nhead(simdata)\n\n        ysim      xsim\n1 1845.03133  8.368388\n2   34.56759 -3.272097\n3   14.39104  1.845040\n4 5054.90702 12.113334\n5  457.36512  4.704802\n6   42.17828  1.571858\n\n\n\ncreate_metrics_table &lt;- function(X){\n  K &lt;- length(X$rsq)\n  metrics_df &lt;- data.frame(num_pred= 1:K, # K different models\n                           Rsq = X$rsq,\n                           rss = X$rss,\n                           adjr2 = X$adjr2,\n                           cp = X$cp,\n                           bic = X$bic) |&gt;\n    tidyr::pivot_longer(cols=Rsq:bic,\n                 names_to = \"metric\",values_to = \"metric_val\")\n  # This pivot puts the metric values in 1 column \n  # and creates another column for the name of \n  # the metric\n  return(metrics_df)\n}\n\n\nlibrary(leaps)\n\nWarning: package 'leaps' was built under R version 4.3.2\n\nbss_fit &lt;- regsubsets(ysim~poly(xsim,10),data = simdata)\nbss_fit\n\nSubset selection object\nCall: regsubsets.formula(ysim ~ poly(xsim, 10), data = simdata)\n10 Variables  (and intercept)\n                 Forced in Forced out\npoly(xsim, 10)1      FALSE      FALSE\npoly(xsim, 10)2      FALSE      FALSE\npoly(xsim, 10)3      FALSE      FALSE\npoly(xsim, 10)4      FALSE      FALSE\npoly(xsim, 10)5      FALSE      FALSE\npoly(xsim, 10)6      FALSE      FALSE\npoly(xsim, 10)7      FALSE      FALSE\npoly(xsim, 10)8      FALSE      FALSE\npoly(xsim, 10)9      FALSE      FALSE\npoly(xsim, 10)10     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: exhaustive\n\nbss_summary &lt;- summary(bss_fit)\n\nbss_summary\n\nSubset selection object\nCall: regsubsets.formula(ysim ~ poly(xsim, 10), data = simdata)\n10 Variables  (and intercept)\n                 Forced in Forced out\npoly(xsim, 10)1      FALSE      FALSE\npoly(xsim, 10)2      FALSE      FALSE\npoly(xsim, 10)3      FALSE      FALSE\npoly(xsim, 10)4      FALSE      FALSE\npoly(xsim, 10)5      FALSE      FALSE\npoly(xsim, 10)6      FALSE      FALSE\npoly(xsim, 10)7      FALSE      FALSE\npoly(xsim, 10)8      FALSE      FALSE\npoly(xsim, 10)9      FALSE      FALSE\npoly(xsim, 10)10     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: exhaustive\n         poly(xsim, 10)1 poly(xsim, 10)2 poly(xsim, 10)3 poly(xsim, 10)4\n1  ( 1 ) \"*\"             \" \"             \" \"             \" \"            \n2  ( 1 ) \"*\"             \"*\"             \" \"             \" \"            \n3  ( 1 ) \"*\"             \"*\"             \"*\"             \" \"            \n4  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n5  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n6  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n7  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n8  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n         poly(xsim, 10)5 poly(xsim, 10)6 poly(xsim, 10)7 poly(xsim, 10)8\n1  ( 1 ) \" \"             \" \"             \" \"             \" \"            \n2  ( 1 ) \" \"             \" \"             \" \"             \" \"            \n3  ( 1 ) \" \"             \" \"             \" \"             \" \"            \n4  ( 1 ) \" \"             \" \"             \" \"             \" \"            \n5  ( 1 ) \"*\"             \" \"             \" \"             \" \"            \n6  ( 1 ) \"*\"             \" \"             \"*\"             \" \"            \n7  ( 1 ) \"*\"             \"*\"             \"*\"             \" \"            \n8  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n         poly(xsim, 10)9 poly(xsim, 10)10\n1  ( 1 ) \" \"             \" \"             \n2  ( 1 ) \" \"             \" \"             \n3  ( 1 ) \" \"             \" \"             \n4  ( 1 ) \" \"             \" \"             \n5  ( 1 ) \" \"             \" \"             \n6  ( 1 ) \" \"             \" \"             \n7  ( 1 ) \" \"             \" \"             \n8  ( 1 ) \" \"             \" \"             \n\n\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\nbss_full_data_metrics &lt;- create_metrics_table(bss_summary)\nhead(bss_full_data_metrics)\n\n# A tibble: 6 √ó 3\n  num_pred metric metric_val\n     &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt;\n1        1 Rsq       6.57e-1\n2        1 rss       5.51e+9\n3        1 adjr2     6.53e-1\n4        1 cp        4.95e+6\n5        1 bic      -9.77e+1\n6        2 Rsq       9.45e-1\n\nbss_full_data_metrics |&gt; \n  ggplot(aes(y=metric_val,x=num_pred))+\n    geom_line() + geom_point()+\n    facet_wrap(~metric,scales = \"free_y\")+\n    labs(title=\"Best Subset Regression\",\n         subtitle = \"On Full Data\")\n\n\n\n\n\n\n\n\nAccording to \\(C_p\\), BIC, and adjusted \\(R^2\\) the best model is the three variable model. On the adjusted \\(R^\\) graph you can see substantial improvements in the value between the 1 and 2 variable and the 2 and 3 variable model, and then little to no improvements beyond 3 variables. With \\(C_p\\) and BIC we see the same behavior with with substantial decreases in the values between 1 and 2 variable models and then again between 2 and 3 variable models. All 3 metrics agree that the best model is a model with 3 variables.\n\nRepeat (c), using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in (c)?\n\n\nfsw_fit &lt;- regsubsets(ysim~poly(xsim,10),data = simdata,method = \"forward\")\nfsw_fit\n\nSubset selection object\nCall: regsubsets.formula(ysim ~ poly(xsim, 10), data = simdata, method = \"forward\")\n10 Variables  (and intercept)\n                 Forced in Forced out\npoly(xsim, 10)1      FALSE      FALSE\npoly(xsim, 10)2      FALSE      FALSE\npoly(xsim, 10)3      FALSE      FALSE\npoly(xsim, 10)4      FALSE      FALSE\npoly(xsim, 10)5      FALSE      FALSE\npoly(xsim, 10)6      FALSE      FALSE\npoly(xsim, 10)7      FALSE      FALSE\npoly(xsim, 10)8      FALSE      FALSE\npoly(xsim, 10)9      FALSE      FALSE\npoly(xsim, 10)10     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: forward\n\nfsw_summary &lt;- summary(fsw_fit)\n\nfsw_summary\n\nSubset selection object\nCall: regsubsets.formula(ysim ~ poly(xsim, 10), data = simdata, method = \"forward\")\n10 Variables  (and intercept)\n                 Forced in Forced out\npoly(xsim, 10)1      FALSE      FALSE\npoly(xsim, 10)2      FALSE      FALSE\npoly(xsim, 10)3      FALSE      FALSE\npoly(xsim, 10)4      FALSE      FALSE\npoly(xsim, 10)5      FALSE      FALSE\npoly(xsim, 10)6      FALSE      FALSE\npoly(xsim, 10)7      FALSE      FALSE\npoly(xsim, 10)8      FALSE      FALSE\npoly(xsim, 10)9      FALSE      FALSE\npoly(xsim, 10)10     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: forward\n         poly(xsim, 10)1 poly(xsim, 10)2 poly(xsim, 10)3 poly(xsim, 10)4\n1  ( 1 ) \"*\"             \" \"             \" \"             \" \"            \n2  ( 1 ) \"*\"             \"*\"             \" \"             \" \"            \n3  ( 1 ) \"*\"             \"*\"             \"*\"             \" \"            \n4  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n5  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n6  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n7  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n8  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n         poly(xsim, 10)5 poly(xsim, 10)6 poly(xsim, 10)7 poly(xsim, 10)8\n1  ( 1 ) \" \"             \" \"             \" \"             \" \"            \n2  ( 1 ) \" \"             \" \"             \" \"             \" \"            \n3  ( 1 ) \" \"             \" \"             \" \"             \" \"            \n4  ( 1 ) \" \"             \" \"             \" \"             \" \"            \n5  ( 1 ) \"*\"             \" \"             \" \"             \" \"            \n6  ( 1 ) \"*\"             \" \"             \"*\"             \" \"            \n7  ( 1 ) \"*\"             \"*\"             \"*\"             \" \"            \n8  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n         poly(xsim, 10)9 poly(xsim, 10)10\n1  ( 1 ) \" \"             \" \"             \n2  ( 1 ) \" \"             \" \"             \n3  ( 1 ) \" \"             \" \"             \n4  ( 1 ) \" \"             \" \"             \n5  ( 1 ) \" \"             \" \"             \n6  ( 1 ) \" \"             \" \"             \n7  ( 1 ) \" \"             \" \"             \n8  ( 1 ) \" \"             \" \"             \n\nfsw_full_data_metrics &lt;- create_metrics_table(fsw_summary)\nhead(fsw_full_data_metrics)\n\n# A tibble: 6 √ó 3\n  num_pred metric metric_val\n     &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt;\n1        1 Rsq       6.57e-1\n2        1 rss       5.51e+9\n3        1 adjr2     6.53e-1\n4        1 cp        4.95e+6\n5        1 bic      -9.77e+1\n6        2 Rsq       9.45e-1\n\nfsw_full_data_metrics |&gt; \n  ggplot(aes(y=metric_val,x=num_pred))+\n    geom_line() + geom_point()+\n    facet_wrap(~metric,scales = \"free_y\")+\n    labs(title=\"Forward Stepwise Regression\",\n         subtitle = \"On Full Data\")\n\n\n\n\n\n\n\n\n\nbsw_fit &lt;- regsubsets(ysim~poly(xsim,10),data = simdata,method = \"backward\")\nfsw_fit\n\nSubset selection object\nCall: regsubsets.formula(ysim ~ poly(xsim, 10), data = simdata, method = \"forward\")\n10 Variables  (and intercept)\n                 Forced in Forced out\npoly(xsim, 10)1      FALSE      FALSE\npoly(xsim, 10)2      FALSE      FALSE\npoly(xsim, 10)3      FALSE      FALSE\npoly(xsim, 10)4      FALSE      FALSE\npoly(xsim, 10)5      FALSE      FALSE\npoly(xsim, 10)6      FALSE      FALSE\npoly(xsim, 10)7      FALSE      FALSE\npoly(xsim, 10)8      FALSE      FALSE\npoly(xsim, 10)9      FALSE      FALSE\npoly(xsim, 10)10     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: forward\n\nbsw_summary &lt;- summary(bsw_fit)\n\nbsw_summary\n\nSubset selection object\nCall: regsubsets.formula(ysim ~ poly(xsim, 10), data = simdata, method = \"backward\")\n10 Variables  (and intercept)\n                 Forced in Forced out\npoly(xsim, 10)1      FALSE      FALSE\npoly(xsim, 10)2      FALSE      FALSE\npoly(xsim, 10)3      FALSE      FALSE\npoly(xsim, 10)4      FALSE      FALSE\npoly(xsim, 10)5      FALSE      FALSE\npoly(xsim, 10)6      FALSE      FALSE\npoly(xsim, 10)7      FALSE      FALSE\npoly(xsim, 10)8      FALSE      FALSE\npoly(xsim, 10)9      FALSE      FALSE\npoly(xsim, 10)10     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: backward\n         poly(xsim, 10)1 poly(xsim, 10)2 poly(xsim, 10)3 poly(xsim, 10)4\n1  ( 1 ) \"*\"             \" \"             \" \"             \" \"            \n2  ( 1 ) \"*\"             \"*\"             \" \"             \" \"            \n3  ( 1 ) \"*\"             \"*\"             \"*\"             \" \"            \n4  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n5  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n6  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n7  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n8  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n         poly(xsim, 10)5 poly(xsim, 10)6 poly(xsim, 10)7 poly(xsim, 10)8\n1  ( 1 ) \" \"             \" \"             \" \"             \" \"            \n2  ( 1 ) \" \"             \" \"             \" \"             \" \"            \n3  ( 1 ) \" \"             \" \"             \" \"             \" \"            \n4  ( 1 ) \" \"             \" \"             \" \"             \" \"            \n5  ( 1 ) \"*\"             \" \"             \" \"             \" \"            \n6  ( 1 ) \"*\"             \" \"             \"*\"             \" \"            \n7  ( 1 ) \"*\"             \"*\"             \"*\"             \" \"            \n8  ( 1 ) \"*\"             \"*\"             \"*\"             \"*\"            \n         poly(xsim, 10)9 poly(xsim, 10)10\n1  ( 1 ) \" \"             \" \"             \n2  ( 1 ) \" \"             \" \"             \n3  ( 1 ) \" \"             \" \"             \n4  ( 1 ) \" \"             \" \"             \n5  ( 1 ) \" \"             \" \"             \n6  ( 1 ) \" \"             \" \"             \n7  ( 1 ) \" \"             \" \"             \n8  ( 1 ) \" \"             \" \"             \n\nbsw_full_data_metrics &lt;- create_metrics_table(bsw_summary)\nhead(bsw_full_data_metrics)\n\n# A tibble: 6 √ó 3\n  num_pred metric metric_val\n     &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt;\n1        1 Rsq       6.57e-1\n2        1 rss       5.51e+9\n3        1 adjr2     6.53e-1\n4        1 cp        4.95e+6\n5        1 bic      -9.77e+1\n6        2 Rsq       9.45e-1\n\nbsw_full_data_metrics |&gt; \n  ggplot(aes(y=metric_val,x=num_pred))+\n    geom_line() + geom_point()+\n    facet_wrap(~metric,scales = \"free_y\")+\n    labs(title=\"Backward Stepwise Regression\",\n         subtitle = \"On Full Data\")\n\n\n\n\n\n\n\n\nWith both forward and backwards stepwise regression we see identical behavior of BIC, \\(C_p\\) and adjusted \\(R^2\\) as with the best subset regression, the best model is with 3 predictors.\n\nNow fit a lasso model to the simulated data, again using \\(X\\),\\(X^2\\), . . . ,\\(X^{10}\\) as predictors. Use cross-validation to select the optimal value of \\(\\lambda\\). Create plots of the cross-validation error as a function of \\(\\lambda\\). Report the resulting coefficient estimates, and discuss the results obtained.\n\n\nlibrary(tidymodels)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels 1.1.1 ‚îÄ‚îÄ\n\n\n‚úî broom        1.0.5          ‚úî rsample      1.2.1     \n‚úî dials        1.2.1          ‚úî tibble       3.2.1     \n‚úî dplyr        1.1.4          ‚úî tidyr        1.3.1     \n‚úî infer        1.0.6          ‚úî tune         1.2.1.9000\n‚úî modeldata    1.3.0          ‚úî workflows    1.1.4     \n‚úî parsnip      1.2.1          ‚úî workflowsets 1.0.1     \n‚úî purrr        1.0.2          ‚úî yardstick    1.3.0     \n‚úî recipes      1.0.10         \n\n\nWarning: package 'broom' was built under R version 4.3.1\n\n\nWarning: package 'dials' was built under R version 4.3.2\n\n\nWarning: package 'scales' was built under R version 4.3.2\n\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\nWarning: package 'infer' was built under R version 4.3.2\n\n\nWarning: package 'modeldata' was built under R version 4.3.2\n\n\nWarning: package 'parsnip' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.1\n\n\nWarning: package 'recipes' was built under R version 4.3.2\n\n\nWarning: package 'rsample' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.2\n\n\nWarning: package 'workflows' was built under R version 4.3.2\n\n\nWarning: package 'workflowsets' was built under R version 4.3.1\n\n\nWarning: package 'yardstick' was built under R version 4.3.2\n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels_conflicts() ‚îÄ‚îÄ\n‚úñ purrr::discard() masks scales::discard()\n‚úñ dplyr::filter()  masks stats::filter()\n‚úñ dplyr::lag()     masks stats::lag()\n‚úñ recipes::step()  masks stats::step()\n‚Ä¢ Learn how to get started at https://www.tidymodels.org/start/\n\nset.seed(434)\n\nsimdat_alt &lt;- simdata|&gt;\n  bind_cols(poly(simdata$xsim,degree=10,simple = T,raw = T))|&gt;\n  select(-xsim)\n\nsim_cv &lt;- vfold_cv(simdat_alt, v = 5)\n\nlasso_spec &lt;- \n  linear_reg(penalty = tune(), mixture = 1) |&gt; \n  set_engine(\"glmnet\") \n\nlam_grid &lt;- expand_grid(penalty = seq(0, 325, by = 10))\n\n\nrec &lt;- recipe(ysim ~ ., data = simdat_alt) |&gt;\n  step_normalize(all_predictors())\n\nresults &lt;- tune_grid(lasso_spec,\n                     preprocessor = rec,\n                     grid = lam_grid, \n                     resamples = sim_cv)\n\nWarning: package 'glmnet' was built under R version 4.3.2\n\n\nWarning: package 'Matrix' was built under R version 4.3.2\n\nmetrics&lt;- results |&gt;\n            collect_metrics()\n\nmetrics\n\n# A tibble: 66 √ó 7\n   penalty .metric .estimator   mean     n     std_err .config              \n     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt; &lt;chr&gt;                \n 1       0 rmse    standard   355.       5 49.1        Preprocessor1_Model01\n 2       0 rsq     standard     1.00     5  0.00000335 Preprocessor1_Model01\n 3      10 rmse    standard   355.       5 49.1        Preprocessor1_Model02\n 4      10 rsq     standard     1.00     5  0.00000335 Preprocessor1_Model02\n 5      20 rmse    standard   355.       5 49.1        Preprocessor1_Model03\n 6      20 rsq     standard     1.00     5  0.00000335 Preprocessor1_Model03\n 7      30 rmse    standard   355.       5 49.1        Preprocessor1_Model04\n 8      30 rsq     standard     1.00     5  0.00000335 Preprocessor1_Model04\n 9      40 rmse    standard   355.       5 49.1        Preprocessor1_Model05\n10      40 rsq     standard     1.00     5  0.00000335 Preprocessor1_Model05\n# ‚Ñπ 56 more rows\n\nmetrics |&gt; \n  filter(.metric ==\"rmse\") |&gt; \n  group_by(penalty)|&gt;\n  summarise(penalty = min(penalty),\n            mean = mean(mean))|&gt;\n  ggplot(aes(x=penalty,y=mean)) +\n    geom_point()+\n    labs(title = \"Lasso Penalty vs RMSE\")\n\n\n\n\n\n\n\nfinal_spec &lt;- linear_reg(penalty = 0, mixture = 1) |&gt;\n  set_engine(\"glmnet\")\n\nfinal_lasso_workflow &lt;- workflow()|&gt;\n  add_model(final_spec) |&gt; \n  add_recipe(rec)\n\nfinal_lasso_fit &lt;- final_lasso_workflow|&gt;\n  fit(data =simdat_alt)\n\nfinal_lasso_fit |&gt; tidy()\n\n# A tibble: 11 √ó 3\n   term        estimate penalty\n   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)    8587.       0\n 2 1                 0        0\n 3 2              1848.       0\n 4 3             10578.       0\n 5 4                 0        0\n 6 5                 0        0\n 7 6                 0        0\n 8 7                 0        0\n 9 8                 0        0\n10 9                 0        0\n11 10                0        0\n\n\nLasso regression keeps only variables \\(X^2\\) and \\(X^3\\) which misses X.\n\nNow generate a response vector Y according to the model\n\n\\[Y=\\beta_0 + \\beta_7X^7 + \\epsilon\\]\nand perform best subset selection and the lasso. Discuss the results obtained.\nNew Sim Data\n\nY2 = 5 + 3*X + 5*X^7 + ep\n\nsimdata_2 &lt;- data.frame(ysim = Y2,xsim = X)\n\n\n## This is to fit the model with the polynomials\nsimdat_alt_2 &lt;- simdata_2|&gt;\n  bind_cols(poly(simdata$xsim,degree=10,simple = T,raw = T))|&gt;\n  select(-xsim)\n\n\n\nBest Subsets\n\nbss_fit_2 &lt;- regsubsets(ysim~.,data = simdat_alt_2)\nbss_fit_2\n\nSubset selection object\nCall: regsubsets.formula(ysim ~ ., data = simdat_alt_2)\n10 Variables  (and intercept)\n     Forced in Forced out\n`1`      FALSE      FALSE\n`2`      FALSE      FALSE\n`3`      FALSE      FALSE\n`4`      FALSE      FALSE\n`5`      FALSE      FALSE\n`6`      FALSE      FALSE\n`7`      FALSE      FALSE\n`8`      FALSE      FALSE\n`9`      FALSE      FALSE\n`10`     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: exhaustive\n\nbss_summary_2 &lt;- summary(bss_fit_2)\n\nbss_summary_2\n\nSubset selection object\nCall: regsubsets.formula(ysim ~ ., data = simdat_alt_2)\n10 Variables  (and intercept)\n     Forced in Forced out\n`1`      FALSE      FALSE\n`2`      FALSE      FALSE\n`3`      FALSE      FALSE\n`4`      FALSE      FALSE\n`5`      FALSE      FALSE\n`6`      FALSE      FALSE\n`7`      FALSE      FALSE\n`8`      FALSE      FALSE\n`9`      FALSE      FALSE\n`10`     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: exhaustive\n         `1` `2` `3` `4` `5` `6` `7` `8` `9` `10`\n1  ( 1 ) \" \" \" \" \" \" \" \" \" \" \" \" \"*\" \" \" \" \" \" \" \n2  ( 1 ) \"*\" \" \" \" \" \" \" \" \" \" \" \"*\" \" \" \" \" \" \" \n3  ( 1 ) \" \" \" \" \"*\" \"*\" \" \" \" \" \"*\" \" \" \" \" \" \" \n4  ( 1 ) \"*\" \" \" \" \" \" \" \"*\" \"*\" \"*\" \" \" \" \" \" \" \n5  ( 1 ) \"*\" \"*\" \" \" \" \" \"*\" \"*\" \"*\" \" \" \" \" \" \" \n6  ( 1 ) \"*\" \" \" \" \" \"*\" \"*\" \" \" \"*\" \" \" \"*\" \"*\" \n7  ( 1 ) \"*\" \"*\" \" \" \" \" \" \" \"*\" \"*\" \"*\" \"*\" \"*\" \n8  ( 1 ) \"*\" \"*\" \" \" \"*\" \" \" \"*\" \"*\" \"*\" \"*\" \"*\" \n\nbss_full_data_metrics_2 &lt;- create_metrics_table(bss_summary_2)\n\nhead(bss_full_data_metrics_2)\n\n# A tibble: 6 √ó 3\n  num_pred metric metric_val\n     &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt;\n1        1 Rsq           1  \n2        1 rss      158450. \n3        1 adjr2         1  \n4        1 cp           46.2\n5        1 bic       -4050. \n6        2 Rsq           1  \n\nbss_full_data_metrics_2 |&gt; \n  ggplot(aes(y=metric_val,x=num_pred))+\n    geom_line() + geom_point()+\n    facet_wrap(~metric,scales = \"free_y\")+\n    labs(title=\"Best Subset Regression\",\n         subtitle = \"On Full Data\")\n\n\n\n\n\n\n\n\nBest subsets indicates that either two or eight coefficients should be kept. If we choose 2, the model would be X and \\(X^7\\), the correct model. If we choose predictors then we would keep all but \\(X^4\\), \\(X^5\\) and \\(X^6\\).\n\n\nLasso\n\nlibrary(tidymodels)\nset.seed(434)\n\nsim_cv &lt;- vfold_cv(simdat_alt_2, v = 5)\n\nlasso_spec &lt;- \n  linear_reg(penalty = tune(), mixture = 1) |&gt; \n  set_engine(\"glmnet\") \n\nlam_grid &lt;- expand_grid(penalty = seq(0, 100, by = 10))\n\n\nrec &lt;- recipe(ysim ~ ., data = simdat_alt_2) |&gt;\n  step_scale(all_predictors())\n\nresults &lt;- tune_grid(lasso_spec,\n                     preprocessor = rec,\n                     grid = lam_grid, \n                     resamples = sim_cv)\n\nmetrics&lt;- results |&gt;\n            collect_metrics()\n\nmetrics\n\n# A tibble: 22 √ó 7\n   penalty .metric .estimator         mean     n std_err .config              \n     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;             &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1       0 rmse    standard   510011538.       5 3.43e+8 Preprocessor1_Model01\n 2       0 rsq     standard           1.00     5 1.22e-6 Preprocessor1_Model01\n 3      10 rmse    standard   510011538.       5 3.43e+8 Preprocessor1_Model02\n 4      10 rsq     standard           1.00     5 1.22e-6 Preprocessor1_Model02\n 5      20 rmse    standard   510011538.       5 3.43e+8 Preprocessor1_Model03\n 6      20 rsq     standard           1.00     5 1.22e-6 Preprocessor1_Model03\n 7      30 rmse    standard   510011538.       5 3.43e+8 Preprocessor1_Model04\n 8      30 rsq     standard           1.00     5 1.22e-6 Preprocessor1_Model04\n 9      40 rmse    standard   510011538.       5 3.43e+8 Preprocessor1_Model05\n10      40 rsq     standard           1.00     5 1.22e-6 Preprocessor1_Model05\n# ‚Ñπ 12 more rows\n\nmetrics |&gt; \n  filter(.metric ==\"rmse\") |&gt; \n  group_by(penalty)|&gt;\n  summarise(penalty = min(penalty),\n            mean = mean(mean))|&gt;\n  ggplot(aes(x=penalty,y=mean)) +\n    geom_line()+\n    labs(title = \"Lasso Penalty vs RMSE\")\n\n\n\n\n\n\n\nmetrics |&gt; filter(.metric==\"rmse\") |&gt;\n  arrange(mean)\n\n# A tibble: 11 √ó 7\n   penalty .metric .estimator       mean     n    std_err .config              \n     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;                \n 1       0 rmse    standard   510011538.     5 342846372. Preprocessor1_Model01\n 2      10 rmse    standard   510011538.     5 342846372. Preprocessor1_Model02\n 3      20 rmse    standard   510011538.     5 342846372. Preprocessor1_Model03\n 4      30 rmse    standard   510011538.     5 342846372. Preprocessor1_Model04\n 5      40 rmse    standard   510011538.     5 342846372. Preprocessor1_Model05\n 6      50 rmse    standard   510011538.     5 342846372. Preprocessor1_Model06\n 7      60 rmse    standard   510011538.     5 342846372. Preprocessor1_Model07\n 8      70 rmse    standard   510011538.     5 342846372. Preprocessor1_Model08\n 9      80 rmse    standard   510011538.     5 342846372. Preprocessor1_Model09\n10      90 rmse    standard   510011538.     5 342846372. Preprocessor1_Model10\n11     100 rmse    standard   510011538.     5 342846372. Preprocessor1_Model11\n\n# Choose lambda = 81\n\nfinal_spec &lt;- linear_reg(penalty = 0, mixture = 1) |&gt;\n  set_engine(\"glmnet\")\n\nfinal_lasso_workflow &lt;- workflow()|&gt;\n  add_model(final_spec) |&gt; \n  add_recipe(rec)\n\nfinal_lasso_fit &lt;- final_lasso_workflow|&gt;\n  fit(data =simdat_alt_2)\n\nfinal_lasso_fit |&gt; tidy()\n\n# A tibble: 11 √ó 3\n   term            estimate penalty\n   &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)   130518046.       0\n 2 1                     0        0\n 3 2                     0        0\n 4 3                     0        0\n 5 4                     0        0\n 6 5                     0        0\n 7 6             415202305.       0\n 8 7           24905659724.       0\n 9 8              20982661.       0\n10 9                     0        0\n11 10                    0        0\n\n\nIn this case, lasso keeps the intercept and coefficients for \\(X^6\\), \\(X^7\\) and \\(X^8\\). Best subsets did a better job."
  },
  {
    "objectID": "slides/06-4-dim-red.html#setup",
    "href": "slides/06-4-dim-red.html#setup",
    "title": "Chapter 6 Part 4",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ISLR2)\n#install.packages('pak')\n#pak::pak(\"mixOmics\") #Very large!"
  },
  {
    "objectID": "slides/06-4-dim-red.html#dimension-reduction",
    "href": "slides/06-4-dim-red.html#dimension-reduction",
    "title": "Chapter 6 Part 4",
    "section": "Dimension Reduction",
    "text": "Dimension Reduction\n\nLasso does dimension reduction when fitting\nThe methods that we have discussed so far in this chapter have involved fitting linear regression models, via least squares or a shrunken approach, using the original predictors, \\(X_1\\),\\(X_2\\),‚Ä¶,\\(X_p\\).\nNext we cover a few approaches that transform the predictors and then fit a least squares model using the transformed variables. We will refer to these techniques as dimension reduction methods."
  },
  {
    "objectID": "slides/06-4-dim-red.html#the-transformations",
    "href": "slides/06-4-dim-red.html#the-transformations",
    "title": "Chapter 6 Part 4",
    "section": "The Transformations",
    "text": "The Transformations\n\nLet \\(Z_1,...,Z_m\\) represent \\(M&lt;p\\) linear combinations of our original \\(p\\) predictors. Or, \\[Z_m = \\sum_{j=1}^p\\phi_{mj}X_j\\] for some constants \\(\\phi_{m1},...,\\phi_{mp}\\).\nWe can then fit linear regression model, \\[y_i = \\theta_0 + \\sum{m=1}^M\\theta_mz_{im}+\\epsilon_i\\text{, }i=1,...,n,\\] using ordinary least squares."
  },
  {
    "objectID": "slides/06-4-dim-red.html#the-coefficients",
    "href": "slides/06-4-dim-red.html#the-coefficients",
    "title": "Chapter 6 Part 4",
    "section": "The Coefficients",
    "text": "The Coefficients\n\nNote that in our transformed model, the regression coefficients are given by \\(\\theta_0...\\theta_M\\).\nIf the constants \\(\\phi_{m1},...,\\phi_{mp}\\) are chosen well, then this dimension reduction can often outperform OLS regression.\nAlso note, that \\(\\beta_j = \\sum_{m=1}^M\\theta_m\\phi_{mf}\\), and thus this new transformed model is a special case of OLS\nThis effectively constrains the \\(\\beta_j\\)‚Äôs."
  },
  {
    "objectID": "slides/06-4-dim-red.html#principal-components-regression",
    "href": "slides/06-4-dim-red.html#principal-components-regression",
    "title": "Chapter 6 Part 4",
    "section": "Principal Components Regression",
    "text": "Principal Components Regression\n\nHere we apply principal components analysis (PCA) (discussed in Chapter 10 of the text) to define the linear combinations of the predictors, for use in our regression.\nThe first principal component is that (normalized) linear combination of the variables with the largest variance.\nThe second principal component has largest variance, subject to being uncorrelated with the first. And so on.\nHence with many correlated original variables, we replace them with a small set of principal components that capture their joint variation."
  },
  {
    "objectID": "slides/06-4-dim-red.html#prc---how-does-it-work",
    "href": "slides/06-4-dim-red.html#prc---how-does-it-work",
    "title": "Chapter 6 Part 4",
    "section": "PRC - How does it work?",
    "text": "PRC - How does it work?\nVideo\nhttps://www.youtube.com/watch?v=SWfucxnOF8c"
  },
  {
    "objectID": "slides/06-4-dim-red.html#partial-least-squares",
    "href": "slides/06-4-dim-red.html#partial-least-squares",
    "title": "Chapter 6 Part 4",
    "section": "Partial Least Squares",
    "text": "Partial Least Squares\n\nPCR identifies linear combinations, or directions, that best represent the predictors \\(X_1,...X_p\\)\nThese directions are identified in an unsupervised way, since the response Y is not used to help determine the principal component directions.\nThat is, the response does not supervise the identification of the principal components.\nConsequently, PCR suffers from a potentially serious drawback: there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response."
  },
  {
    "objectID": "slides/06-4-dim-red.html#more-plr",
    "href": "slides/06-4-dim-red.html#more-plr",
    "title": "Chapter 6 Part 4",
    "section": "More PLR",
    "text": "More PLR\n\nLike PCR, PLS is a dimension reduction method, which first identifies a new set of features \\(Z_1,...,Z_m\\) that are linear combinations of the original features, and then its a linear model via OLS using these \\(M\\) new features.\nBut unlike PCR, PLS identifies these new features in a supervised way, that is, it makes use of the response Y in order to identify new features that not only approximate the old features well, but also that are related to the response.\nRoughly speaking, the PLS approach attempts to find directions that help explain both the response and the predictors."
  },
  {
    "objectID": "slides/06-4-dim-red.html#even-more-plr",
    "href": "slides/06-4-dim-red.html#even-more-plr",
    "title": "Chapter 6 Part 4",
    "section": "Even More PLR",
    "text": "Even More PLR\nVideo\nhttps://www.youtube.com/watch?v=Vf7doatc2rA"
  },
  {
    "objectID": "slides/06-4-dim-red.html#pcr-in-tidymodels",
    "href": "slides/06-4-dim-red.html#pcr-in-tidymodels",
    "title": "Chapter 6 Part 4",
    "section": "PCR in tidymodels",
    "text": "PCR in tidymodels\n\nUse step_normalize and then step_pca\nFit a linear regression model\n\n\ndata(iris)\n\nlm_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\niris_rec_pcr &lt;- recipe(Sepal.Width ~ .,data = iris)|&gt;\n    step_dummy(all_nominal_predictors())|&gt;\n    step_normalize(all_predictors()) |&gt;\n    step_pca(all_numeric_predictors())          #New\n\npcr_wf &lt;- workflow() |&gt;\n  add_model(lm_spec)|&gt;\n  add_recipe(iris_rec_pcr)"
  },
  {
    "objectID": "slides/06-4-dim-red.html#pcr-in-tidymodels-1",
    "href": "slides/06-4-dim-red.html#pcr-in-tidymodels-1",
    "title": "Chapter 6 Part 4",
    "section": "PCR in tidymodels",
    "text": "PCR in tidymodels\n\niris_pcr_fit &lt;- pcr_wf |&gt; \n  fit(data = iris)\n\ntidy_pcr_fit&lt;- iris_pcr_fit |&gt; tidy()\n\ntidy_pcr_fit\n\n# A tibble: 6 √ó 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   3.06      0.0219    140.   1.08e-155\n2 PC1          -0.0682    0.0119     -5.73 5.61e-  8\n3 PC2          -0.157     0.0191     -8.23 1.01e- 13\n4 PC3           0.436     0.0456      9.56 4.58e- 17\n5 PC4          -0.910     0.122      -7.46 7.40e- 12\n6 PC5           0.356     0.204       1.75 8.29e-  2"
  },
  {
    "objectID": "slides/06-4-dim-red.html#pcr-in-tidymodels-2",
    "href": "slides/06-4-dim-red.html#pcr-in-tidymodels-2",
    "title": "Chapter 6 Part 4",
    "section": "PCR in tidymodels",
    "text": "PCR in tidymodels\n\nlibrary(learntidymodels)\n\nplot_top_loadings(iris_pcr_fit)"
  },
  {
    "objectID": "slides/06-4-dim-red.html#pcr",
    "href": "slides/06-4-dim-red.html#pcr",
    "title": "Chapter 6 Part 4",
    "section": "PCR",
    "text": "PCR\n\nFor now we are going to let it choose our number of components\nOnce we cover PCA formally, we can do more."
  },
  {
    "objectID": "slides/06-4-dim-red.html#pls-in-tidymodels",
    "href": "slides/06-4-dim-red.html#pls-in-tidymodels",
    "title": "Chapter 6 Part 4",
    "section": "PLS in tidymodels",
    "text": "PLS in tidymodels\n\niris_rec_pls &lt;- recipe(Sepal.Width ~ .,data = iris)|&gt;\n    step_dummy(all_nominal_predictors())|&gt;\n    step_normalize(all_predictors()) |&gt;\n    step_pls(all_numeric_predictors(),outcome = \"Sepal.Width\")          #New\n\npls_wf &lt;- workflow() |&gt;\n  add_model(lm_spec)|&gt;\n  add_recipe(iris_rec_pls)\n\niris_pls_fit &lt;- pls_wf |&gt; \n  fit(data = iris)\n\ntidy_pls_fit&lt;- iris_pls_fit |&gt; tidy()\n\ntidy_pls_fit\n\n# A tibble: 3 √ó 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    3.06     0.0283    108.   7.30e-142\n2 PLS1           0.146    0.0190      7.72 1.68e- 12\n3 PLS2           0.132    0.0247      5.34 3.49e-  7"
  },
  {
    "objectID": "slides/06-4-dim-red.html#pls-in-tidymodels-1",
    "href": "slides/06-4-dim-red.html#pls-in-tidymodels-1",
    "title": "Chapter 6 Part 4",
    "section": "PLS in tidymodels",
    "text": "PLS in tidymodels\n\nplot_top_loadings(iris_pls_fit,type = 'pls')\n\n\n\n\n\n\n\n\n\n\n\n\nüîó https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "slides/06-4-dim-red.html#pcr---how-does-it-work",
    "href": "slides/06-4-dim-red.html#pcr---how-does-it-work",
    "title": "Chapter 6 Part 4",
    "section": "PCR - How does it work?",
    "text": "PCR - How does it work?\nVideo\nhttps://www.youtube.com/watch?v=SWfucxnOF8c"
  },
  {
    "objectID": "slides/06-4-dim-red.html#more-pls",
    "href": "slides/06-4-dim-red.html#more-pls",
    "title": "Chapter 6 Part 4",
    "section": "More PLS",
    "text": "More PLS\n\nLike PCR, PLS is a dimension reduction method, which first identifies a new set of features \\(Z_1,...,Z_m\\) that are linear combinations of the original features, and then its a linear model via OLS using these \\(M\\) new features.\nBut unlike PCR, PLS identifies these new features in a supervised way, that is, it makes use of the response Y in order to identify new features that not only approximate the old features well, but also that are related to the response.\nRoughly speaking, the PLS approach attempts to find directions that help explain both the response and the predictors."
  },
  {
    "objectID": "slides/06-4-dim-red.html#even-more-pls",
    "href": "slides/06-4-dim-red.html#even-more-pls",
    "title": "Chapter 6 Part 4",
    "section": "Even More PLS",
    "text": "Even More PLS\nVideo\nhttps://www.youtube.com/watch?v=Vf7doatc2rA"
  },
  {
    "objectID": "slides/06-4-dim_red_o.html",
    "href": "slides/06-4-dim_red_o.html",
    "title": "Chapter 6 Part 4",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(ISLR2)\n#install.packages('pak')\n#pak::pak(\"mixOmics\") #Very large!"
  },
  {
    "objectID": "slides/06-4-dim_red_o.html#setup",
    "href": "slides/06-4-dim_red_o.html#setup",
    "title": "Chapter 6 Part 4",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(ISLR2)\n#install.packages('pak')\n#pak::pak(\"mixOmics\") #Very large!"
  },
  {
    "objectID": "slides/06-4-dim_red_o.html#dimension-reduction",
    "href": "slides/06-4-dim_red_o.html#dimension-reduction",
    "title": "Chapter 6 Part 4",
    "section": "Dimension Reduction",
    "text": "Dimension Reduction\n\nLasso does dimension reduction when fitting\nThe methods that we have discussed so far in this chapter have involved fitting linear regression models, via least squares or a shrunken approach, using the original predictors, \\(X_1\\),\\(X_2\\),‚Ä¶,\\(X_p\\).\nNext we cover a few approaches that transform the predictors and then fit a least squares model using the transformed variables. We will refer to these techniques as dimension reduction methods."
  },
  {
    "objectID": "slides/06-4-dim_red_o.html#the-transformations",
    "href": "slides/06-4-dim_red_o.html#the-transformations",
    "title": "Chapter 6 Part 4",
    "section": "The Transformations",
    "text": "The Transformations\n\nLet \\(Z_1,...,Z_m\\) represent \\(M&lt;p\\) linear combinations of our original \\(p\\) predictors. Or, \\[Z_m = \\sum_{j=1}^p\\phi_{mj}X_j\\] for some constants \\(\\phi_{m1},...,\\phi_{mp}\\).\nWe can then fit linear regression model, \\[y_i = \\theta_0 + \\sum{m=1}^M\\theta_mz_{im}+\\epsilon_i\\text{, }i=1,...,n,\\] using ordinary least squares."
  },
  {
    "objectID": "slides/06-4-dim_red_o.html#the-coefficients",
    "href": "slides/06-4-dim_red_o.html#the-coefficients",
    "title": "Chapter 6 Part 4",
    "section": "The Coefficients",
    "text": "The Coefficients\n\nNote that in our transformed model, the regression coefficients are given by \\(\\theta_0...\\theta_M\\).\nIf the constants \\(\\phi_{m1},...,\\phi_{mp}\\) are chosen well, then this dimension reduction can often outperform OLS regression.\nAlso note, that \\(\\beta_j = \\sum_{m=1}^M\\theta_m\\phi_{mf}\\), and thus this new transformed model is a special case of OLS\nThis effectively constrains the \\(\\beta_j\\)‚Äôs."
  },
  {
    "objectID": "slides/06-4-dim_red_o.html#principal-components-regression",
    "href": "slides/06-4-dim_red_o.html#principal-components-regression",
    "title": "Chapter 6 Part 4",
    "section": "Principal Components Regression",
    "text": "Principal Components Regression\n\nHere we apply principal components analysis (PCA) (discussed in Chapter 10 of the text) to define the linear combinations of the predictors, for use in our regression.\nThe first principal component is that (normalized) linear combination of the variables with the largest variance.\nThe second principal component has largest variance, subject to being uncorrelated with the first. And so on.\nHence with many correlated original variables, we replace them with a small set of principal components that capture their joint variation."
  },
  {
    "objectID": "slides/06-4-dim_red_o.html#pcr---how-does-it-work",
    "href": "slides/06-4-dim_red_o.html#pcr---how-does-it-work",
    "title": "Chapter 6 Part 4",
    "section": "PCR - How does it work?",
    "text": "PCR - How does it work?\nVideo\nhttps://www.youtube.com/watch?v=SWfucxnOF8c"
  },
  {
    "objectID": "slides/06-4-dim_red_o.html#partial-least-squares",
    "href": "slides/06-4-dim_red_o.html#partial-least-squares",
    "title": "Chapter 6 Part 4",
    "section": "Partial Least Squares",
    "text": "Partial Least Squares\n\nPCR identifies linear combinations, or directions, that best represent the predictors \\(X_1,...X_p\\)\nThese directions are identified in an unsupervised way, since the response Y is not used to help determine the principal component directions.\nThat is, the response does not supervise the identification of the principal components.\nConsequently, PCR suffers from a potentially serious drawback: there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response."
  },
  {
    "objectID": "slides/06-4-dim_red_o.html#more-pls",
    "href": "slides/06-4-dim_red_o.html#more-pls",
    "title": "Chapter 6 Part 4",
    "section": "More PLS",
    "text": "More PLS\n\nLike PCR, PLS is a dimension reduction method, which first identifies a new set of features \\(Z_1,...,Z_m\\) that are linear combinations of the original features, and then its a linear model via OLS using these \\(M\\) new features.\nBut unlike PCR, PLS identifies these new features in a supervised way, that is, it makes use of the response Y in order to identify new features that not only approximate the old features well, but also that are related to the response.\nRoughly speaking, the PLS approach attempts to find directions that help explain both the response and the predictors."
  },
  {
    "objectID": "slides/06-4-dim_red_o.html#even-more-pls",
    "href": "slides/06-4-dim_red_o.html#even-more-pls",
    "title": "Chapter 6 Part 4",
    "section": "Even More PLS",
    "text": "Even More PLS\nVideo\nhttps://www.youtube.com/watch?v=Vf7doatc2rA"
  },
  {
    "objectID": "slides/06-4-dim_red_o.html#pcr-in-tidymodels",
    "href": "slides/06-4-dim_red_o.html#pcr-in-tidymodels",
    "title": "Chapter 6 Part 4",
    "section": "PCR in tidymodels",
    "text": "PCR in tidymodels\n\nUse step_normalize and then step_pca\nFit a linear regression model\n\n\ndata(iris)\n\nlm_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\niris_rec_pcr &lt;- recipe(Sepal.Width ~ .,data = iris)|&gt;\n    step_dummy(all_nominal_predictors())|&gt;\n    step_normalize(all_predictors()) |&gt;\n    step_pca(all_numeric_predictors())          #New\n\npcr_wf &lt;- workflow() |&gt;\n  add_model(lm_spec)|&gt;\n  add_recipe(iris_rec_pcr)"
  },
  {
    "objectID": "slides/06-4-dim_red_o.html#pcr-in-tidymodels-1",
    "href": "slides/06-4-dim_red_o.html#pcr-in-tidymodels-1",
    "title": "Chapter 6 Part 4",
    "section": "PCR in tidymodels",
    "text": "PCR in tidymodels\n\niris_pcr_fit &lt;- pcr_wf |&gt; \n  fit(data = iris)\n\ntidy_pcr_fit&lt;- iris_pcr_fit |&gt; tidy()\n\ntidy_pcr_fit\n\n# A tibble: 6 √ó 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   3.06      0.0219    140.   1.08e-155\n2 PC1          -0.0682    0.0119     -5.73 5.61e-  8\n3 PC2          -0.157     0.0191     -8.23 1.01e- 13\n4 PC3           0.436     0.0456      9.56 4.58e- 17\n5 PC4          -0.910     0.122      -7.46 7.40e- 12\n6 PC5           0.356     0.204       1.75 8.29e-  2"
  },
  {
    "objectID": "slides/06-4-dim_red_o.html#pcr-in-tidymodels-2",
    "href": "slides/06-4-dim_red_o.html#pcr-in-tidymodels-2",
    "title": "Chapter 6 Part 4",
    "section": "PCR in tidymodels",
    "text": "PCR in tidymodels\n\nlibrary(learntidymodels)\n\nplot_top_loadings(iris_pcr_fit)"
  },
  {
    "objectID": "slides/06-4-dim_red_o.html#pcr",
    "href": "slides/06-4-dim_red_o.html#pcr",
    "title": "Chapter 6 Part 4",
    "section": "PCR",
    "text": "PCR\n\nFor now we are going to let it choose our number of components\nOnce we cover PCA formally, we can do more."
  },
  {
    "objectID": "slides/06-4-dim_red_o.html#pls-in-tidymodels",
    "href": "slides/06-4-dim_red_o.html#pls-in-tidymodels",
    "title": "Chapter 6 Part 4",
    "section": "PLS in tidymodels",
    "text": "PLS in tidymodels\n\niris_rec_pls &lt;- recipe(Sepal.Width ~ .,data = iris)|&gt;\n    step_dummy(all_nominal_predictors())|&gt;\n    step_normalize(all_predictors()) |&gt;\n    step_pls(all_numeric_predictors(),outcome = \"Sepal.Width\")          #New\n\npls_wf &lt;- workflow() |&gt;\n  add_model(lm_spec)|&gt;\n  add_recipe(iris_rec_pls)\n\niris_pls_fit &lt;- pls_wf |&gt; \n  fit(data = iris)\n\ntidy_pls_fit&lt;- iris_pls_fit |&gt; tidy()\n\ntidy_pls_fit\n\n# A tibble: 3 √ó 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    3.06     0.0283    108.   7.30e-142\n2 PLS1           0.146    0.0190      7.72 1.68e- 12\n3 PLS2           0.132    0.0247      5.34 3.49e-  7"
  },
  {
    "objectID": "slides/06-4-dim_red_o.html#pls-in-tidymodels-1",
    "href": "slides/06-4-dim_red_o.html#pls-in-tidymodels-1",
    "title": "Chapter 6 Part 4",
    "section": "PLS in tidymodels",
    "text": "PLS in tidymodels\n\nplot_top_loadings(iris_pls_fit,type = 'pls')"
  },
  {
    "objectID": "slides/06-4-dim-red_o.html",
    "href": "slides/06-4-dim-red_o.html",
    "title": "Chapter 6 Part 4",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(ISLR2)\n#install.packages('pak')\n#pak::pak(\"mixOmics\") #Very large!"
  },
  {
    "objectID": "slides/06-4-dim-red_o.html#setup",
    "href": "slides/06-4-dim-red_o.html#setup",
    "title": "Chapter 6 Part 4",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(ISLR2)\n#install.packages('pak')\n#pak::pak(\"mixOmics\") #Very large!"
  },
  {
    "objectID": "slides/06-4-dim-red_o.html#dimension-reduction",
    "href": "slides/06-4-dim-red_o.html#dimension-reduction",
    "title": "Chapter 6 Part 4",
    "section": "Dimension Reduction",
    "text": "Dimension Reduction\n\nLasso does dimension reduction when fitting\nThe methods that we have discussed so far in this chapter have involved fitting linear regression models, via least squares or a shrunken approach, using the original predictors, \\(X_1\\),\\(X_2\\),‚Ä¶,\\(X_p\\).\nNext we cover a few approaches that transform the predictors and then fit a least squares model using the transformed variables. We will refer to these techniques as dimension reduction methods."
  },
  {
    "objectID": "slides/06-4-dim-red_o.html#the-transformations",
    "href": "slides/06-4-dim-red_o.html#the-transformations",
    "title": "Chapter 6 Part 4",
    "section": "The Transformations",
    "text": "The Transformations\n\nLet \\(Z_1,...,Z_m\\) represent \\(M&lt;p\\) linear combinations of our original \\(p\\) predictors. Or, \\[Z_m = \\sum_{j=1}^p\\phi_{mj}X_j\\] for some constants \\(\\phi_{m1},...,\\phi_{mp}\\).\nWe can then fit linear regression model, \\[y_i = \\theta_0 + \\sum{m=1}^M\\theta_mz_{im}+\\epsilon_i\\text{, }i=1,...,n,\\] using ordinary least squares."
  },
  {
    "objectID": "slides/06-4-dim-red_o.html#the-coefficients",
    "href": "slides/06-4-dim-red_o.html#the-coefficients",
    "title": "Chapter 6 Part 4",
    "section": "The Coefficients",
    "text": "The Coefficients\n\nNote that in our transformed model, the regression coefficients are given by \\(\\theta_0...\\theta_M\\).\nIf the constants \\(\\phi_{m1},...,\\phi_{mp}\\) are chosen well, then this dimension reduction can often outperform OLS regression.\nAlso note, that \\(\\beta_j = \\sum_{m=1}^M\\theta_m\\phi_{mf}\\), and thus this new transformed model is a special case of OLS\nThis effectively constrains the \\(\\beta_j\\)‚Äôs."
  },
  {
    "objectID": "slides/06-4-dim-red_o.html#principal-components-regression",
    "href": "slides/06-4-dim-red_o.html#principal-components-regression",
    "title": "Chapter 6 Part 4",
    "section": "Principal Components Regression",
    "text": "Principal Components Regression\n\nHere we apply principal components analysis (PCA) (discussed in Chapter 10 of the text) to define the linear combinations of the predictors, for use in our regression.\nThe first principal component is that (normalized) linear combination of the variables with the largest variance.\nThe second principal component has largest variance, subject to being uncorrelated with the first. And so on.\nHence with many correlated original variables, we replace them with a small set of principal components that capture their joint variation."
  },
  {
    "objectID": "slides/06-4-dim-red_o.html#pcr---how-does-it-work",
    "href": "slides/06-4-dim-red_o.html#pcr---how-does-it-work",
    "title": "Chapter 6 Part 4",
    "section": "PCR - How does it work?",
    "text": "PCR - How does it work?\nVideo\nhttps://www.youtube.com/watch?v=SWfucxnOF8c"
  },
  {
    "objectID": "slides/06-4-dim-red_o.html#partial-least-squares",
    "href": "slides/06-4-dim-red_o.html#partial-least-squares",
    "title": "Chapter 6 Part 4",
    "section": "Partial Least Squares",
    "text": "Partial Least Squares\n\nPCR identifies linear combinations, or directions, that best represent the predictors \\(X_1,...X_p\\)\nThese directions are identified in an unsupervised way, since the response Y is not used to help determine the principal component directions.\nThat is, the response does not supervise the identification of the principal components.\nConsequently, PCR suffers from a potentially serious drawback: there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response."
  },
  {
    "objectID": "slides/06-4-dim-red_o.html#more-pls",
    "href": "slides/06-4-dim-red_o.html#more-pls",
    "title": "Chapter 6 Part 4",
    "section": "More PLS",
    "text": "More PLS\n\nLike PCR, PLS is a dimension reduction method, which first identifies a new set of features \\(Z_1,...,Z_m\\) that are linear combinations of the original features, and then its a linear model via OLS using these \\(M\\) new features.\nBut unlike PCR, PLS identifies these new features in a supervised way, that is, it makes use of the response Y in order to identify new features that not only approximate the old features well, but also that are related to the response.\nRoughly speaking, the PLS approach attempts to find directions that help explain both the response and the predictors."
  },
  {
    "objectID": "slides/06-4-dim-red_o.html#even-more-pls",
    "href": "slides/06-4-dim-red_o.html#even-more-pls",
    "title": "Chapter 6 Part 4",
    "section": "Even More PLS",
    "text": "Even More PLS\nVideo\nhttps://www.youtube.com/watch?v=Vf7doatc2rA"
  },
  {
    "objectID": "slides/06-4-dim-red_o.html#pcr-in-tidymodels",
    "href": "slides/06-4-dim-red_o.html#pcr-in-tidymodels",
    "title": "Chapter 6 Part 4",
    "section": "PCR in tidymodels",
    "text": "PCR in tidymodels\n\nUse step_normalize and then step_pca\nFit a linear regression model\n\n\ndata(iris)\n\nlm_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\niris_rec_pcr &lt;- recipe(Sepal.Width ~ .,data = iris)|&gt;\n    step_dummy(all_nominal_predictors())|&gt;\n    step_normalize(all_predictors()) |&gt;\n    step_pca(all_numeric_predictors())          #New\n\npcr_wf &lt;- workflow() |&gt;\n  add_model(lm_spec)|&gt;\n  add_recipe(iris_rec_pcr)"
  },
  {
    "objectID": "slides/06-4-dim-red_o.html#pcr-in-tidymodels-1",
    "href": "slides/06-4-dim-red_o.html#pcr-in-tidymodels-1",
    "title": "Chapter 6 Part 4",
    "section": "PCR in tidymodels",
    "text": "PCR in tidymodels\n\niris_pcr_fit &lt;- pcr_wf |&gt; \n  fit(data = iris)\n\ntidy_pcr_fit&lt;- iris_pcr_fit |&gt; tidy()\n\ntidy_pcr_fit\n\n# A tibble: 6 √ó 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   3.06      0.0219    140.   1.08e-155\n2 PC1          -0.0682    0.0119     -5.73 5.61e-  8\n3 PC2          -0.157     0.0191     -8.23 1.01e- 13\n4 PC3           0.436     0.0456      9.56 4.58e- 17\n5 PC4          -0.910     0.122      -7.46 7.40e- 12\n6 PC5           0.356     0.204       1.75 8.29e-  2"
  },
  {
    "objectID": "slides/06-4-dim-red_o.html#pcr-in-tidymodels-2",
    "href": "slides/06-4-dim-red_o.html#pcr-in-tidymodels-2",
    "title": "Chapter 6 Part 4",
    "section": "PCR in tidymodels",
    "text": "PCR in tidymodels\n\nlibrary(learntidymodels)\n\nplot_top_loadings(iris_pcr_fit)"
  },
  {
    "objectID": "slides/06-4-dim-red_o.html#pcr",
    "href": "slides/06-4-dim-red_o.html#pcr",
    "title": "Chapter 6 Part 4",
    "section": "PCR",
    "text": "PCR\n\nFor now we are going to let it choose our number of components\nOnce we cover PCA formally, we can do more."
  },
  {
    "objectID": "slides/06-4-dim-red_o.html#pls-in-tidymodels",
    "href": "slides/06-4-dim-red_o.html#pls-in-tidymodels",
    "title": "Chapter 6 Part 4",
    "section": "PLS in tidymodels",
    "text": "PLS in tidymodels\n\niris_rec_pls &lt;- recipe(Sepal.Width ~ .,data = iris)|&gt;\n    step_dummy(all_nominal_predictors())|&gt;\n    step_normalize(all_predictors()) |&gt;\n    step_pls(all_numeric_predictors(),outcome = \"Sepal.Width\")          #New\n\npls_wf &lt;- workflow() |&gt;\n  add_model(lm_spec)|&gt;\n  add_recipe(iris_rec_pls)\n\niris_pls_fit &lt;- pls_wf |&gt; \n  fit(data = iris)\n\ntidy_pls_fit&lt;- iris_pls_fit |&gt; tidy()\n\ntidy_pls_fit\n\n# A tibble: 3 √ó 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    3.06     0.0283    108.   7.30e-142\n2 PLS1           0.146    0.0190      7.72 1.68e- 12\n3 PLS2           0.132    0.0247      5.34 3.49e-  7"
  },
  {
    "objectID": "slides/06-4-dim-red_o.html#pls-in-tidymodels-1",
    "href": "slides/06-4-dim-red_o.html#pls-in-tidymodels-1",
    "title": "Chapter 6 Part 4",
    "section": "PLS in tidymodels",
    "text": "PLS in tidymodels\n\nplot_top_loadings(iris_pls_fit,type = 'pls')"
  }
]