[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Introduction to Statistical Learning",
    "section": "",
    "text": "Instructor\nInstructor Dr. Tyler George\n   Cornell College, West 311\n    tgeorge@cornellcollege.edu \n\n\n\nClass Meetings\nApril 15th - May 8th\n   9am-11am & 1pm-2pm\n   West 201\n   Course Calendar\n\n\n\nOffice Hours\n   MWTh 3:05pm-4:05pm and by appt.\n   West 311\n   Optional Appointment\n\n\n\nI am available far beyond these times listed. Please email me and we can set up a time to chat about class material or whatever you prefer! I will generally announce changes to office hours in class but I still suggest checking the Course Calendar to verify availability.\n\nYou Are A Priority\nMy goal this block is to help you learn the material. I want to first and foremost recognize that you are an individual and thus are unique and may learn uniquely. Additionally, your health and wellbeing are priority one. Learning cannot happen effectively if you don’t meet your other personal needs. That all being said, I have structured the class in a way that I, from experience teaching and learning myself, think will be most beneficial for the majority of students. I promise you that I will do my best to create an inclusive and engaging learning environment. I ask that you keep an open line of communication between us for when you may need help and/or flexibility. You and your learning are why I am here.\n\n\nCourse Description\nThis course will introduce students to relatively new and powerful statistical techniques used to analyze data. The course will begin with a review of linear regression and an introduction to computer-based variable and model selection methods. Other topics will include classification methods, resampling methods for model-building, non-linear models, and tree-based methods. The computer software program R will be used throughout.\n\n\nLearning Objectives\nAt the end of this course I would like you conceptually understand, be able to use apply, and interpret the results of\n\nThe variance/bias trade-off\nMeasuring quality of fits\nLinear model selection and regularization\nClassification including K nearest neighbors\nCross validation\nDimension reduction\nTree based methods\nUnsupervised learning\n\n\n\nPrerequisite\nTo be successful in this class, you should have completed STA 201, STA 202, and DSC 223.\n\n\nOpen Access Books – Free!\nAll of materials for this class are free.\nThe main text book is: An Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani – it is freely available online.\nThe secondary text is R resource: Tidy Modeling with R by Max Kuhn and Julia Silge – it is freely available online.\n\n\nCourse Site and Moodle\nOur course will run from a combination of Moodle and the course website at https://sta362-sb8-24.github.io/STA362StatLearning/.\n\n\nSoftware – No need to install\nWe will use a combination of technologies in this course including R, and RStudio (server). Luckily for you I have put lots of effort into setting all of this on a machine we have on campus that we will all access with a web browser! You don’t need to install any – in fact for a while I prefer you don’t. More on this in class. If you are an off campus student, please let me know right away, as you may need to checkout a laptop (free) from IT to work on homework from home.\nIf you have any technical problems you should contact IT as soon as possible. Submit a Work Order!\n\n\nGroup Work\nIn this class, I would like you to work in groups for a variety of reasons. A large part of this class is communicating analysis – not just completing analysis. At the beginning of the block, groups will be formed. You should expect to work with this group every day. When we work in groups in class we will decide on roles, specifically who is controlling the one screen will rotate). Group members will rotate roles between tasks to help make sure everybody is sharing work. You won’t be working in a group for everything; any quizzes, and exams may be individual.\n\n\nEvaluations and grades\n\nGrade Category Descriptions\n\nHomework:\nHomeworks will be graded for correctness. The goal is the practice the application of the method and then be able to interpret the result.\n\n\nParticipation\nThis will be measured by attending class and working on the work given including labs and class examples.\n\n\nProject\nThis will entail multiple stages of submission with details accessed through “Project” on the left side of the course website (once available). Some class time will be given for discussing projects with me but not enough to complete the project during class times. I do not anticipate we will start these until week 3.\n\n\nExams\nThere will be a Midterm exam (4/26) and a final exam (morning of 5/8).\n\n\n\n\n\n\n\n\nAssignment\nPoints\n\n\n\n\nHomework\n200\n\n\nParticipation\n100\n\n\nProject\n300\n\n\nExams, two 200pts exams\n400\n\n\nTotal\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\nA\n93-100%\nC\n73-76%\n\n\nA-\n90–92%\nC-\n70-72%\n\n\nB+\n87–89%\nD+\n67-69%\n\n\nB\n83-86%\nD\n63-66%\n\n\nB-\n80-82%\nD-\n60-62%\n\n\nC+\n77-79%\nF\n&lt;60%\n\n\n\n\n\n\n\n\n\nUse of AI\nI expect you to generate your own work in this class. When you submit any kind of work (including projects, exams, homeworks), you are asserting that you have generated and written the text, and code, unless you indicate otherwise by the use of quotation marks and proper attribution for the source. Submitting content as your own that has been generated by someone other than you, or was created or assisted by a computer application or tool, including artificial intelligence (AI) tools such as ChatGPT is cheating and constitutes a violation of our Academic Honesty policy. You may use simple word processing tools to update spelling and grammar in your assignments, but unless given permission otherwise, you may not use AI tools to draft your work, even if you edit, revise, or paraphrase it. There may be opportunities for you to use AI tools in this class. Where they exist, I will clearly specify when and in what capacity it is permissible for you to use these tools.\n\n\nDISABILITIES AND ACCOMODATIONS POLICY\nCornell College makes reasonable accommodations for persons with disabilities. Students should notify the Office of Academic Support and Advising and their course instructor of any disability related accommodations within the first three days of the term for which the accommodations are required, due to the fast pace of the block format. For more information on the documentation required to establish the need for accommodations and the process of requesting the accommodations.\n\n\nACADEMIC HONESTY POLICY\nCornell College expects all members of the Cornell community to act with academic integrity. An important aspect of academic integrity is respecting the work of others. A student is expected to explicitly acknowledge ideas, claims, observations, or data of others, unless generally known. When a piece of work is submitted for credit, a student is asserting that the submission is her or his work unless there is a citation of a specific source. If there is no appropriate acknowledgment of sources, whether intended or not, this may constitute a violation of the College’s requirement for honesty in academic work and may be treated as a case of academic dishonesty. The procedures regarding how the College deals with cases of academic dishonesty appear in The Catalog, under the heading “Academic Honesty.”\n\n\nIllness Policy\nIf you are experiencing COVID-19 symptoms, do not attend class. Perform a home test or contact Director of Student Health Services Lynn O’Brien at student_health@cornellcollege.edu immediately to arrange a COVID-19 test at the Health Center. If you need to isolate due to COVID-19, or if you become unable to attend class for any other health reason, contact me as soon as possible to determine if you are able to continue in the class. A Withdrawal for Health Reasons may be required.\n\n\nMandatory Reporter Reminder\nIt is my goal that you feel supported and able to share information related to your life experiences during classroom discussions, in your written work, and in any one-on-one meetings with me. You should also know that all Cornell College faculty and staff are mandatory reporters. This means that I will keep information you share with me private to the greatest extent possible. However, I am required to share information regarding sexual assault, abuse, criminal behavior, or about a student who may be a danger to themselves or to others. If you wish to speak to someone confidentially who is not a mandatory reporter, you can schedule an appointment with one of the counselors in the Ebersole Health and Wellbeing Center or contact the College Chaplain, Rev. Melea White, at mwhite@cornelllcollege.edu.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "slides/04-logistic.html#recap",
    "href": "slides/04-logistic.html#recap",
    "title": "Chapter 4 Part 1",
    "section": "Recap",
    "text": "Recap\n\nWe had a linear regression refresher\nLinear regression is a great tool when we have a continuous outcome\nWe are going to learn some fancy ways to do even better in the future\n\nSetup:\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR)"
  },
  {
    "objectID": "slides/04-logistic.html#classification-1",
    "href": "slides/04-logistic.html#classification-1",
    "title": "Chapter 4 Part 1",
    "section": "Classification",
    "text": "Classification\n\nWhat are some examples of classification problems?\n\n\nQualitative response variable in an unordered set, \\(\\mathcal{C}\\)\neye color \\(\\in\\) {blue, brown, green}\nemail \\(\\in\\) {spam, not spam}\nResponse, \\(Y\\) takes on values in \\(\\mathcal{C}\\)\nPredictors are a vector, \\(X\\)\nThe task: build a function \\(C(X)\\) that takes \\(X\\) and predicts \\(Y\\), \\(C(X)\\in\\mathcal{C}\\)\nMany times we are actually more interested in the probabilities that \\(X\\) belongs to each category in \\(\\mathcal{C}\\)"
  },
  {
    "objectID": "slides/04-logistic.html#example-credit-card-default",
    "href": "slides/04-logistic.html#example-credit-card-default",
    "title": "Chapter 4 Part 1",
    "section": "Example: Credit card default",
    "text": "Example: Credit card default\n\n\nCode\nset.seed(1)\nDefault |&gt;\n  sample_frac(size = 0.25) |&gt;\n  ggplot(aes(balance, income, color = default)) +\n  geom_point(pch = 4) +\n  scale_color_manual(values = c(\"cornflower blue\", \"red\")) +\n  theme_classic() +\n  theme(legend.position = \"top\") -&gt; p1\n\np2 &lt;- ggplot(Default, aes(x = default, y = balance, fill = default)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"cornflower blue\", \"red\")) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\np3 &lt;- ggplot(Default, aes(x = default, y = income, fill = default)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"cornflower blue\", \"red\")) +\n  theme_classic() +\n  theme(legend.position = \"none\")\ngrid.arrange(p1, p2, p3, ncol = 3, widths = c(2, 1, 1))"
  },
  {
    "objectID": "slides/04-logistic.html#can-we-use-linear-regression",
    "href": "slides/04-logistic.html#can-we-use-linear-regression",
    "title": "Chapter 4 Part 1",
    "section": "Can we use linear regression?",
    "text": "Can we use linear regression?\nWe can code Default as\n\\[Y = \\begin{cases} 0 & \\textrm{if }\\texttt{No}\\\\ 1&\\textrm{if }\\texttt{Yes}\\end{cases}\\] Can we fit a linear regression of \\(Y\\) on \\(X\\) and classify as Yes if \\(\\hat{Y}&gt; 0.5\\)?\n\nIn this case of a binary outcome, linear regression is okay (it is equivalent to linear discriminant analysis, you can read more about that in your book!)\n\\(E[Y|X=x] = P(Y=1|X=x)\\), so it seems like this is a pretty good idea!\nThe problem: Linear regression can produce probabilities less than 0 or greater than 1 😱"
  },
  {
    "objectID": "slides/04-logistic.html#can-we-use-linear-regression-1",
    "href": "slides/04-logistic.html#can-we-use-linear-regression-1",
    "title": "Chapter 4 Part 1",
    "section": "Can we use linear regression?",
    "text": "Can we use linear regression?\nWe can code Default as\n\\[Y = \\begin{cases} 0 & \\textrm{if }\\texttt{No}\\\\ 1&\\textrm{if }\\texttt{Yes}\\end{cases}\\] Can we fit a linear regression of \\(Y\\) on \\(X\\) and classify as Yes if \\(\\hat{Y}&gt; 0.5\\)?\n\nWhat may do a better job?\n\n\nLogistic regression!"
  },
  {
    "objectID": "slides/04-logistic.html#linear-versus-logistic-regression",
    "href": "slides/04-logistic.html#linear-versus-logistic-regression",
    "title": "Chapter 4 Part 1",
    "section": "Linear versus logistic regression",
    "text": "Linear versus logistic regression\n\n\nCode\nDefault &lt;- Default |&gt;\n  mutate(\n  p = glm(default ~ balance, data = Default, family = \"binomial\") |&gt;\n  predict(type = \"response\"),\n  p2 = lm(I(default == \"Yes\") ~ balance, data = Default) |&gt; predict(),\n  def = ifelse(default == \"Yes\", 1, 0)\n)\n\n\nDefault |&gt;\n  sample_frac(0.25) |&gt;\nggplot(aes(balance, p2)) +\n  geom_hline(yintercept = c(0, 1), lty = 2, size = 0.2) +\n  geom_line(color = \"cornflower blue\") +\n  geom_point(aes(balance, def), shape = \"|\", color = \"orange\") +\n  theme_classic() +\n  labs(y = \"probability of default\") -&gt; p1\n\nDefault |&gt;\n  sample_frac(0.25) |&gt;\nggplot(aes(balance, p)) +\n  geom_hline(yintercept = c(0, 1), lty = 2, size = 0.2) +\n  geom_line(color = \"cornflower blue\") +\n  geom_point(aes(balance, def), shape = \"|\", color = \"orange\") +\n  theme_classic() +\n  labs(y = \"probability of default\") -&gt; p2\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\nWhich does a better job at predicting the probability of default?\n\n\nThe orange marks represent the response \\(Y\\in\\{0,1\\}\\)"
  },
  {
    "objectID": "slides/04-logistic.html#linear-regression",
    "href": "slides/04-logistic.html#linear-regression",
    "title": "Chapter 4 Part 1",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhat if we have \\(&gt;2\\) possible outcomes? For example, someone comes to the emergency room and we need to classify them according to their symptoms\n\\[\n\\begin{align}\nY = \\begin{cases} 1& \\textrm{if }\\texttt{stroke}\\\\2&\\textrm{if }\\texttt{drug overdose}\\\\3&\\textrm{if }\\texttt{epileptic seizure}\\end{cases}\n\\end{align}\n\\]\n\nWhat could go wrong here?\n\n\nThe coding implies an ordering\nThe coding implies equal spacing (that is the difference between stroke and drug overdose is the same as drug overdose and epileptic seizure)"
  },
  {
    "objectID": "slides/04-logistic.html#linear-regression-1",
    "href": "slides/04-logistic.html#linear-regression-1",
    "title": "Chapter 4 Part 1",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhat if we have \\(&gt;2\\) possible outcomes? For example, someone comes to the emergency room and we need to classify them according to their symptoms\n\\[\n\\begin{align}\nY = \\begin{cases} 1& \\textrm{if }\\texttt{stroke}\\\\2&\\textrm{if }\\texttt{drug overdose}\\\\3&\\textrm{if }\\texttt{epileptic seizure}\\end{cases}\n\\end{align}\n\\]\n\nLinear regression is not appropriate here\nMutliclass logistic regression or discriminant analysis are more appropriate"
  },
  {
    "objectID": "slides/04-logistic.html#logistic-regression",
    "href": "slides/04-logistic.html#logistic-regression",
    "title": "Chapter 4 Part 1",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\\[\np(X) = \\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}\n\\]\n\nNote: \\(p(X)\\) is shorthand for \\(P(Y=1|X)\\)\nNo matter what values \\(\\beta_0\\), \\(\\beta_1\\), or \\(X\\) take \\(p(X)\\) will always be between 0 and 1"
  },
  {
    "objectID": "slides/04-logistic.html#logistic-regression-1",
    "href": "slides/04-logistic.html#logistic-regression-1",
    "title": "Chapter 4 Part 1",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\\[\np(X) = \\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}\n\\]\nWe can rearrange this into the following form:\n\\[\n\\log\\left(\\frac{p(X)}{1-p(X)}\\right) = \\beta_0 + \\beta_1 X\n\\]\n\nWhat is this transformation called?\n\n\nThis is a log odds or logit transformation of \\(p(X)\\)"
  },
  {
    "objectID": "slides/04-logistic.html#linear-versus-logistic-regression-1",
    "href": "slides/04-logistic.html#linear-versus-logistic-regression-1",
    "title": "Chapter 4 Part 1",
    "section": "Linear versus logistic regression",
    "text": "Linear versus logistic regression\n\nLogistic regression ensures that our estimates for \\(p(X)\\) are between 0 and 1 🎉"
  },
  {
    "objectID": "slides/04-logistic.html#maximum-likelihood",
    "href": "slides/04-logistic.html#maximum-likelihood",
    "title": "Chapter 4 Part 1",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\nRefresher: How did we estimate \\(\\hat\\beta\\) in linear regression?"
  },
  {
    "objectID": "slides/04-logistic.html#maximum-likelihood-1",
    "href": "slides/04-logistic.html#maximum-likelihood-1",
    "title": "Chapter 4 Part 1",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\nRefresher: How did we estimate \\(\\hat\\beta\\) in linear regression?\n\nIn logistic regression, we use maximum likelihood to estimate the parameters\n\\[\\mathcal{l}(\\beta_0,\\beta_1)=\\prod_{i:y_i=1}p(x_i)\\prod_{i:y_i=0}(1-p(x_i))\\]\n\nThis likelihood give the probability of the observed ones and zeros in the data\nWe pick \\(\\beta_0\\) and \\(\\beta_1\\) to maximize the likelihood\nWe’ll let R do the heavy lifting here"
  },
  {
    "objectID": "slides/04-logistic.html#lets-see-it-in-r",
    "href": "slides/04-logistic.html#lets-see-it-in-r",
    "title": "Chapter 4 Part 1",
    "section": "Let’s see it in R",
    "text": "Let’s see it in R\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(default ~ balance, \n      data = Default) |&gt;\n  tidy()\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic   p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) -10.7      0.361        -29.5 3.62e-191\n2 balance       0.00550  0.000220      25.0 1.98e-137\n\n\n\nUse the logistic_reg() function in R with the glm engine"
  },
  {
    "objectID": "slides/04-logistic.html#making-predictions",
    "href": "slides/04-logistic.html#making-predictions",
    "title": "Chapter 4 Part 1",
    "section": "Making predictions",
    "text": "Making predictions\n\nWhat is our estimated probability of default for someone with a balance of $1000?\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.6513306\n0.3611574\n-29.49221\n0\n\n\nbalance\n0.0054989\n0.0002204\n24.95309\n0\n\n\n\n\n\n\n\n\n\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}{1+e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}=\\frac{e^{-10.65+0.0055\\times 1000}}{1+e^{-10.65+0.0055\\times 1000}}=0.006\n\\]"
  },
  {
    "objectID": "slides/04-logistic.html#making-predictions-1",
    "href": "slides/04-logistic.html#making-predictions-1",
    "title": "Chapter 4 Part 1",
    "section": "Making predictions",
    "text": "Making predictions\n\nWhat is our estimated probability of default for someone with a balance of $2000?\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.6513306\n0.3611574\n-29.49221\n0\n\n\nbalance\n0.0054989\n0.0002204\n24.95309\n0\n\n\n\n\n\n\n\n\n\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}{1+e^{\\hat{\\beta}_0+\\hat{\\beta}_1X}}=\\frac{e^{-10.65+0.0055\\times 2000}}{1+e^{-10.65+0.0055\\times 2000}}=0.586\n\\]"
  },
  {
    "objectID": "slides/04-logistic.html#logistic-regression-example",
    "href": "slides/04-logistic.html#logistic-regression-example",
    "title": "Chapter 4 Part 1",
    "section": "Logistic regression example",
    "text": "Logistic regression example\nLet’s refit the model to predict the probability of default given the customer is a student\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-3.5041278\n0.0707130\n-49.554219\n0.0000000\n\n\nstudentYes\n0.4048871\n0.1150188\n3.520181\n0.0004313\n\n\n\n\n\n\n\n\n\\[P(\\texttt{default = Yes}|\\texttt{student = Yes}) = \\frac{e^{-3.5041+0.4049\\times1}}{1+e^{-3.5041+0.4049\\times1}}=0.0431\\]\n\n\nHow will this change if student = No?\n\n\n\n\\[P(\\texttt{default = Yes}|\\texttt{student = No}) = \\frac{e^{-3.5041+0.4049\\times0}}{1+e^{-3.5041+0.4049\\times0}}=0.0292\\]"
  },
  {
    "objectID": "slides/04-logistic.html#multiple-logistic-regression",
    "href": "slides/04-logistic.html#multiple-logistic-regression",
    "title": "Chapter 4 Part 1",
    "section": "Multiple logistic regression",
    "text": "Multiple logistic regression\n\\[\\log\\left(\\frac{p(X)}{1-p(X)}\\right)=\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p\\] \\[p(X) = \\frac{e^{\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p}}{1+e^{\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p}}\\]\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.8690452\n0.4922555\n-22.080088\n0.0000000\n\n\nbalance\n0.0057365\n0.0002319\n24.737563\n0.0000000\n\n\nincome\n0.0000030\n0.0000082\n0.369815\n0.7115203\n\n\nstudentYes\n-0.6467758\n0.2362525\n-2.737646\n0.0061881\n\n\n\n\n\n\n\n\n\nWhy is the coefficient for student negative now when it was positive before?"
  },
  {
    "objectID": "slides/04-logistic.html#confounding",
    "href": "slides/04-logistic.html#confounding",
    "title": "Chapter 4 Part 1",
    "section": "Confounding",
    "text": "Confounding\n\n\nStudents tend to have higher balances than non-students\nTheir marginal default rate is higher\nFor each level of balance, students default less\nTheir conditional default rate is lower"
  },
  {
    "objectID": "slides/04-logistic.html#confounding-1",
    "href": "slides/04-logistic.html#confounding-1",
    "title": "Chapter 4 Part 1",
    "section": "Confounding",
    "text": "Confounding\n\n\nStudents tend to have higher balances than non-students\nTheir marginal default rate is higher\nFor each level of balance, students default less\nTheir conditional default rate is lower"
  },
  {
    "objectID": "slides/04-logistic.html#logistic-regression-for-more-than-two-classes",
    "href": "slides/04-logistic.html#logistic-regression-for-more-than-two-classes",
    "title": "Chapter 4 Part 1",
    "section": "Logistic regression for more than two classes",
    "text": "Logistic regression for more than two classes\n\\[P(Y=k|X) = \\frac{e ^{\\beta_{0k}+\\beta_{1k}X_1+\\dots+\\beta_{pk}X_p}}{\\sum_{l=1}^Ke^{\\beta_{0l}+\\beta_{1l}X_1+\\dots+\\beta_{pl}X_p}}\\]\n\nSo far we’ve discussed binary outcome data\nWe can generalize this to situations with multiple classes\nHere we have a linear function for each of the \\(K\\) classes\nThis is known as multinomial logistic regression"
  },
  {
    "objectID": "slides/04-logistic.html#a-bit-about-odds",
    "href": "slides/04-logistic.html#a-bit-about-odds",
    "title": "Chapter 4 Part 1",
    "section": "A bit about “odds”",
    "text": "A bit about “odds”\n\nThe “odds” tell you how likely an event is\n🌂 Let’s say there is a 60% chance of rain today * What is the probability that it will rain?\n\\(p = 0.6\\)\nWhat is the probability that it won’t rain?\n\\(1-p = 0.4\\)\nWhat are the odds that it will rain?\n3 to 2, 3:2, \\(\\frac{0.6}{0.4} = 1.5\\)"
  },
  {
    "objectID": "slides/04-logistic.html#transforming-logs",
    "href": "slides/04-logistic.html#transforming-logs",
    "title": "Chapter 4 Part 1",
    "section": "Transforming logs",
    "text": "Transforming logs\n\nHow do you “undo” a \\(\\log\\) base \\(e\\)?\nUse \\(e\\)! For example:\n\\(e^{\\log(10)} = 10\\)\n\\(e^{\\log(1283)} = 1283\\)\n\\(e^{\\log(x)} = x\\)"
  },
  {
    "objectID": "slides/04-logistic.html#transforming-logs-1",
    "href": "slides/04-logistic.html#transforming-logs-1",
    "title": "Chapter 4 Part 1",
    "section": "Transforming logs",
    "text": "Transforming logs\n\nHow would you get the odds from the log(odds)?\n\n\n\nHow do you “undo” a \\(\\log\\) base \\(e\\)?\nUse \\(e\\)! For example:\n\\(e^{\\log(10)} = 10\\)\n\\(e^{\\log(1283)} = 1283\\)\n\\(e^{\\log(x)} = x\\)\n\n\n\n\\(e^{\\log(odds)}\\) = odds"
  },
  {
    "objectID": "slides/04-logistic.html#transforming-odds",
    "href": "slides/04-logistic.html#transforming-odds",
    "title": "Chapter 4 Part 1",
    "section": "Transforming odds",
    "text": "Transforming odds\n\nodds = \\(\\frac{\\pi}{1-\\pi}\\)\nSolving for \\(\\pi\\)\n\\(\\pi = \\frac{\\textrm{odds}}{1+\\textrm{odds}}\\)\nPlugging in \\(e^{\\log(odds)}\\) = odds\n\\(\\pi = \\frac{e^{\\log(odds)}}{1+e^{\\log(odds)}}\\)\nPlugging in \\(\\log(odds) = \\beta_0 + \\beta_1x\\)\n\\(\\pi = \\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}\\)"
  },
  {
    "objectID": "slides/04-logistic.html#the-logistic-model",
    "href": "slides/04-logistic.html#the-logistic-model",
    "title": "Chapter 4 Part 1",
    "section": "The logistic model",
    "text": "The logistic model\n\n✌️ forms\n\n\n\n\n\n\n\n\nForm\nModel\n\n\n\n\nLogit form\n\\(\\log\\left(\\frac{\\pi}{1-\\pi}\\right) = \\beta_0 + \\beta_1x\\)\n\n\nProbability form\n\\(\\Large\\pi = \\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}\\)"
  },
  {
    "objectID": "slides/04-logistic.html#the-logistic-model-1",
    "href": "slides/04-logistic.html#the-logistic-model-1",
    "title": "Chapter 4 Part 1",
    "section": "The logistic model",
    "text": "The logistic model\n\n\n\nprobability\nodds\nlog(odds)\n\n\n\n\n\\(\\pi\\)\n\\(\\frac{\\pi}{1-\\pi}\\)\n\\(\\log\\left(\\frac{\\pi}{1-\\pi}\\right)=l\\)\n\n\n\n⬅️\n\n\n\nlog(odds)\nodds\nprobability\n\n\n\n\n\\(l\\)\n\\(e^l\\)\n\\(\\frac{e^l}{1+e^l} = \\pi\\)"
  },
  {
    "objectID": "slides/04-logistic.html#the-logistic-model-2",
    "href": "slides/04-logistic.html#the-logistic-model-2",
    "title": "Chapter 4 Part 1",
    "section": "The logistic model",
    "text": "The logistic model\n\n✌️ forms\nlog(odds): \\(l = \\beta_0 + \\beta_1x\\)\nP(Outcome = Yes): \\(\\Large\\pi =\\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}\\)"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios",
    "href": "slides/04-logistic.html#odds-ratios",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nA study investigated whether a handheld device that sends a magnetic pulse into a person’s head might be an effective treatment for migraine headaches.\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later. (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-1",
    "href": "slides/04-logistic.html#odds-ratios-1",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat is the explanatory variable?\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person’s head might be an effective treatment for migraine headaches.\n\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-2",
    "href": "slides/04-logistic.html#odds-ratios-2",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat type of variable is this?\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person’s head might be an effective treatment for migraine headaches.\n\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-3",
    "href": "slides/04-logistic.html#odds-ratios-3",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat is the outcome variable?\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person’s head might be an effective treatment for migraine headaches.\n\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-4",
    "href": "slides/04-logistic.html#odds-ratios-4",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat type of variable is this?\n\nA study investigated whether a handheld device that sends a magnetic pulse into a person’s head might be an effective treatment for migraine headaches.\n\n\nResearchers recruited 200 subjects who suffered from migraines\nrandomly assigned them to receive either the TMS (transcranial magnetic stimulation) treatment or a placebo treatment\nSubjects were instructed to apply the device at the onset of migraine symptoms and then assess how they felt two hours later (either Pain-free or Not pain-free)"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-5",
    "href": "slides/04-logistic.html#odds-ratios-5",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\n\n\nTreatment\nTMS\nPlacebo\nTotal\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\nWe can compare the results using odds\nWhat are the odds of being pain-free for the placebo group?\n\\((22/100)/(78/100) = 22/78 = 0.282\\)\nWhat are the odds of being pain-free for the treatment group?\n\\(39/61 = 0.639\\)\nComparing the odds what can we conclude?\nTMS increases the likelihood of success"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-6",
    "href": "slides/04-logistic.html#odds-ratios-6",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\n\n\nTreatment\nTMS\nPlacebo\nTotal\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\nWe can summarize this relationship with an odds ratio: the ratio of the two odds\n\\(\\Large OR = \\frac{39/61}{22/78} = \\frac{0.639}{0.282} = 2.27\\)\n“the odds of being pain free were 2.27 times higher with TMS than with the placebo”"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-7",
    "href": "slides/04-logistic.html#odds-ratios-7",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat if we wanted to calculate this in terms of Not pain-free (with pain-free) as the referent?\n\n\n\n\nTreatment\nTMS\nPlacebo\nTotal\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\n\\(\\Large OR = \\frac{61/39}{78/22} = \\frac{1.564}{3.545} = 0.441\\)\nthe odds for still being in pain for the TMS group are 0.441 times the odds of being in pain for the placebo group"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-8",
    "href": "slides/04-logistic.html#odds-ratios-8",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat changed here?\n\n\n\n\nTreatment\nTMS\nPlacebo\nTotal\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\n\\(\\Large OR = \\frac{78/22}{61/39} = \\frac{3.545}{1.564} = 2.27\\)\nthe odds for still being in pain for the placebo group are 2.27 times the odds of being in pain for the TMS group"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-9",
    "href": "slides/04-logistic.html#odds-ratios-9",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nIn general, it’s more natural to interpret odds ratios &gt; 1, you can flip the referent to do so\n\n\n\nTreatment\nTMS\nPlacebo\nTotal\n\n\n\n\nPain-free two hours later\n39\n22\n61\n\n\nNot pain-free two hours later\n61\n78\n139\n\n\nTotal\n100\n100\n200\n\n\n\n\\(\\Large OR = \\frac{78/22}{61/39} = \\frac{3.545}{1.564} = 2.27\\)\nthe odds for still being in pain for the placebo group are 2.27 times the odds of being in pain for the TMS group"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-10",
    "href": "slides/04-logistic.html#odds-ratios-10",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nLet’s look at some Titanic data. We are interested in whether the passenger reported being female is related to whether they survived.\n\n\n\n\nFemale\nMale\nTotal\n\n\n\n\nSurvived\n308\n142\n450\n\n\nDied\n154\n709\n863\n\n\nTotal\n462\n851\n1313"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-11",
    "href": "slides/04-logistic.html#odds-ratios-11",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat are the odds of surviving for females versus males?\n\n\n\n\n\nFemale\nMale\nTotal\n\n\n\n\nSurvived\n308\n142\n450\n\n\nDied\n154\n709\n863\n\n\nTotal\n462\n851\n1313\n\n\n\n\\[\\Large OR = \\frac{308/154}{142/709} = \\frac{2}{0.2} = 9.99\\]"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-12",
    "href": "slides/04-logistic.html#odds-ratios-12",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nHow do you interpret this?\n\n\n\n\n\nFemale\nMale\nTotal\n\n\n\n\nSurvived\n308\n142\n450\n\n\nDied\n154\n709\n863\n\n\nTotal\n462\n851\n1313\n\n\n\n\\[\\Large OR = \\frac{308/154}{142/709} = \\frac{2}{0.2} = 9.99\\] the odds of surviving for the female passengers was 9.99 times the odds of surviving for the male passengers"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-13",
    "href": "slides/04-logistic.html#odds-ratios-13",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nWhat if we wanted to fit a model? What would the equation be?\n\n\n\n\n\nFemale\nMale\nTotal\n\n\n\n\nSurvived\n308\n142\n450\n\n\nDied\n154\n709\n863\n\n\nTotal\n462\n851\n1313\n\n\n\n\n\\[\\Large \\log(\\textrm{odds of survival}) = \\beta_0 + \\beta_1 \\textrm{Female}\\]"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-14",
    "href": "slides/04-logistic.html#odds-ratios-14",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\\[\\Large \\log(\\textrm{odds of survival}) = \\beta_0 + \\beta_1 \\textrm{Female}\\]\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Survived ~ Sex, data = Titanic) |&gt;\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -1.61    0.0919     -17.5 1.70e-68\n2 Sexfemale       2.30    0.135       17.1 2.91e-65"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-15",
    "href": "slides/04-logistic.html#odds-ratios-15",
    "title": "Chapter 4 Part 1",
    "section": "Odds Ratios",
    "text": "Odds Ratios\n\nHow do you interpret this result?\n\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Survived ~ Sex, data = Titanic) |&gt;\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -1.61    0.0919     -17.5 1.70e-68\n2 Sexfemale       2.30    0.135       17.1 2.91e-65"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-16",
    "href": "slides/04-logistic.html#odds-ratios-16",
    "title": "Chapter 4 Part 1",
    "section": "Odds Ratios",
    "text": "Odds Ratios\n\nHow do you interpret this result?\n\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Survived ~ Sex, data = Titanic) |&gt;\n  tidy(exponentiate = TRUE) \n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.200    0.0919     -17.5 1.70e-68\n2 Sexfemale      9.99     0.135       17.1 2.91e-65\n\nexp(2.301176)\n\n[1] 9.99"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-17",
    "href": "slides/04-logistic.html#odds-ratios-17",
    "title": "Chapter 4 Part 1",
    "section": "Odds Ratios",
    "text": "Odds Ratios\n\nHow do you interpret this result?\n\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Survived ~ Sex, data = Titanic) |&gt;\n  tidy(exponentiate = TRUE) \n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.200    0.0919     -17.5 1.70e-68\n2 Sexfemale      9.99     0.135       17.1 2.91e-65\n\nexp(2.301176)\n\n[1] 9.99\n\n\nthe odds of surviving for the female passengers was 9.99 times the odds of surviving for the male passengers"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-18",
    "href": "slides/04-logistic.html#odds-ratios-18",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nWhat if the explanatory variable is continuous?\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Acceptance ~ GPA, data = MedGPA) |&gt;\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -19.2       5.63     -3.41 0.000644\n2 GPA             5.45      1.58      3.45 0.000553\n\n\nA one unit increase in GPA yields a 5.45 increase in the log odds of acceptance"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-19",
    "href": "slides/04-logistic.html#odds-ratios-19",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\nWhat if the explanatory variable is continuous?\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Acceptance ~ GPA, data = MedGPA) |&gt;\n  tidy(exponentiate = TRUE) \n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  4.56e-9      5.63     -3.41 0.000644\n2 GPA          2.34e+2      1.58      3.45 0.000553\n\n\nA one unit increase in GPA yields a 234-fold increase in the odds of acceptance\n\n😱 that seems huge! Remember: the interpretation of these coefficients depends on your units (the same as in ordinary linear regression)."
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-20",
    "href": "slides/04-logistic.html#odds-ratios-20",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nHow could we get the odds associated with increasing GPA by 0.1?\n\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Acceptance ~ GPA, data = MedGPA) |&gt;\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -19.2       5.63     -3.41 0.000644\n2 GPA             5.45      1.58      3.45 0.000553\n\n\n\nexp(5.454) ## a one unit increase in GPA\n\n[1] 234\n\nexp(5.454 * 0.1) ## a 0.1 increase in GPA\n\n[1] 1.73\n\n\nA one-tenth unit increase in GPA yields a 1.73-fold increase in the odds of acceptance"
  },
  {
    "objectID": "slides/04-logistic.html#odds-ratios-21",
    "href": "slides/04-logistic.html#odds-ratios-21",
    "title": "Chapter 4 Part 1",
    "section": "Odds ratios",
    "text": "Odds ratios\n\nHow could we get the odds associated with increasing GPA by 0.1?\n\n\nMedGPA &lt;- MedGPA |&gt;\n  mutate(GPA_10 = GPA * 10)\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(Acceptance ~ GPA_10, data = MedGPA) |&gt;\n  tidy(exponentiate = TRUE)\n\n# A tibble: 2 × 5\n  term             estimate std.error statistic  p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) 0.00000000456     5.63      -3.41 0.000644\n2 GPA_10      1.73              0.158      3.45 0.000553\n\n\nA one-tenth unit increase in GPA yields a 1.73-fold increase in the odds of acceptance"
  },
  {
    "objectID": "slides/04-logistic.html#application-exercise",
    "href": "slides/04-logistic.html#application-exercise",
    "title": "Chapter 4 Part 1",
    "section": " Application Exercise",
    "text": "Application Exercise\nUsing the Default data from the ISLR package. Fit two logistic regression models that predict whether a customer defaults\n\nOne model with student as a predictor. Interpret the coefficient of studentYes.\nAnother model with balance as a predictor. Interpret the coefficient of balance.\n\nHere is some code to get you started:\n\nlibrary(ISLR)\ndata(\"Default\")\n\n\n\n\n\n🔗 https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section",
    "href": "slides/01-02-welcome_to_sl.html#section",
    "title": "Chapter 1 and 2",
    "section": "👋",
    "text": "👋\nTyler George\n   tgeorge@cornellcollege.edu\n   MWTh 3:05pm-4:05pm and by appt."
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#course-website",
    "href": "slides/01-02-welcome_to_sl.html#course-website",
    "title": "Chapter 1 and 2",
    "section": "Course Website",
    "text": "Course Website\nhttps://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#intros",
    "href": "slides/01-02-welcome_to_sl.html#intros",
    "title": "Chapter 1 and 2",
    "section": "Intros",
    "text": "Intros\n\nName\nMajor\nFun OR boring fact"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#statistical-learning-problems",
    "href": "slides/01-02-welcome_to_sl.html#statistical-learning-problems",
    "title": "Chapter 1 and 2",
    "section": "Statistical Learning Problems",
    "text": "Statistical Learning Problems\n\n\n\nIdentify risk factors for breast cancer\n\n\n\n\n\n\nDr. Tyler George adapted from slides by Hastie & Tibshirani"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#statistical-learning-problems-1",
    "href": "slides/01-02-welcome_to_sl.html#statistical-learning-problems-1",
    "title": "Chapter 1 and 2",
    "section": "Statistical Learning Problems",
    "text": "Statistical Learning Problems\n\n\n\nCustomize an email spam detection system\n\n\n\nData: 4601 labeled emails sent to George who works at HP Labs\nInput features: frequencies of words and punctuation\n\n\n\n\n\n\n—\ngeorge\nyou\nhp\nfree\n!\nedu\nremove\n\n\n\n\nspam\n0.00\n2.26\n0.02\n0.52\n0.51\n0.01\n0.28\n\n\nemail\n2.27\n1.27\n0.90\n0.07\n0.11\n0.29\n0.01\n\n\n\n\nDr. Tyler George adapted from slides by Hastie & Tibshirani"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#statistical-learning-problems-2",
    "href": "slides/01-02-welcome_to_sl.html#statistical-learning-problems-2",
    "title": "Chapter 1 and 2",
    "section": "Statistical Learning Problems",
    "text": "Statistical Learning Problems\n\n\n\nIdentify numbers in handwritten zip code\n\n\n\n\n\n\n\nDr. Tyler George adapted from slides by Hastie & Tibshirani"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#statistical-learning-problems-3",
    "href": "slides/01-02-welcome_to_sl.html#statistical-learning-problems-3",
    "title": "Chapter 1 and 2",
    "section": "Statistical Learning Problems",
    "text": "Statistical Learning Problems\n\n\nEstablish the relationship between variables in population survey data\n\nIncome survey data for males from the central Atlantic region of US, 2009\n\n\n\n\nDr. Tyler George adapted from slides by Hastie & Tibshirani"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#types-of-statistical-learning",
    "href": "slides/01-02-welcome_to_sl.html#types-of-statistical-learning",
    "title": "Chapter 1 and 2",
    "section": "✌️ types of statistical learning",
    "text": "✌️ types of statistical learning\n\nSupervised Learning\nUnsupervised Learning"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#supervised-learning",
    "href": "slides/01-02-welcome_to_sl.html#supervised-learning",
    "title": "Chapter 1 and 2",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nDr. Tyler George adapted from slides by Hastie & Tibshirani\n\n\noutcome variable: \\(Y\\), (dependent variable, response, target)\npredictors: vector of \\(p\\) predictors, \\(X\\), (inputs, regressors, covariates, features, independent variables)\nIn the regression problem, \\(Y\\) is quantitative (e.g price, blood pressure)\nIn the classification problem, \\(Y\\) takes values in a finite, unordered set (survived/died, digit 0-9, cancer class of tissue sample)\nWe have training data \\((x_1, y_1), \\dots, (x_N, y_N)\\). These are observations (examples, instances) of these measurements"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#supervised-learning-1",
    "href": "slides/01-02-welcome_to_sl.html#supervised-learning-1",
    "title": "Chapter 1 and 2",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nWhat do you think are some objectives here?\n\nObjectives\n\nAccurately predict unseen test cases\nUnderstand which inputs affect the outcome, and how\nAssess the quality of our predictions and inferences\n\n\nDr. Tyler George adapted from slides by Hastie & Tibshirani"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#unsupervised-learning",
    "href": "slides/01-02-welcome_to_sl.html#unsupervised-learning",
    "title": "Chapter 1 and 2",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nDr. Tyler George adapted from slides by Hastie & Tibshirani\n\n\nNo outcome variable, just a set of predictors (features) measured on a set of samples\nobjective is more fuzzy – find groups of samples that behave similarly, find features that behave similarly, find linear combinations of features with the most variation\ndifficult to know how well your are doing\ndifferent from supervised learning, but can be useful as a pre-processing step for supervised learning"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#lets-take-a-tour---class-website",
    "href": "slides/01-02-welcome_to_sl.html#lets-take-a-tour---class-website",
    "title": "Chapter 1 and 2",
    "section": "Let’s take a tour - class website",
    "text": "Let’s take a tour - class website\n\n\n\n\n\nConcepts introduced:\n\nHow to find slides\nHow to find assignments\nHow to find RStudio\nHow to get help\nHow to find policies"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-and-classification",
    "href": "slides/01-02-welcome_to_sl.html#regression-and-classification",
    "title": "Chapter 1 and 2",
    "section": "Regression and Classification",
    "text": "Regression and Classification\n\nRegression: quantitative response\nClassification: qualitative (categorical) response"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-and-classification-1",
    "href": "slides/01-02-welcome_to_sl.html#regression-and-classification-1",
    "title": "Chapter 1 and 2",
    "section": "Regression and Classification",
    "text": "Regression and Classification\n\nWhat would be an example of a regression problem?\n\n\nRegression: quantitative response\nClassification: qualitative (categorical) response"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-and-classification-2",
    "href": "slides/01-02-welcome_to_sl.html#regression-and-classification-2",
    "title": "Chapter 1 and 2",
    "section": "Regression and Classification",
    "text": "Regression and Classification\n\nWhat would be an example of a classification problem?\n\n\nRegression: quantitative response\nClassification: qualitative (categorical) response"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#auto-data",
    "href": "slides/01-02-welcome_to_sl.html#auto-data",
    "title": "Chapter 1 and 2",
    "section": "Auto data",
    "text": "Auto data\n\n\n\nAbove are mpg vs horsepower, weight, and acceleration, with a blue linear-regression line fit separately to each. Can we predict mpg using these three?\n\nMaybe we can do better using a model:\n\\[\\texttt{mpg} \\approx f(\\texttt{horsepower}, \\texttt{weight}, \\texttt{acceleration})\\]"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#notation",
    "href": "slides/01-02-welcome_to_sl.html#notation",
    "title": "Chapter 1 and 2",
    "section": "Notation",
    "text": "Notation\n\nmpg is the response variable, the outcome variable, we refer to this as \\(Y\\)\nhorsepower is a feature, input, predictor, we refer to this as \\(X_1\\)\nweight is \\(X_2\\)\nacceleration is \\(X_3\\)\nOur input vector is:\n\n\\(X = \\begin{bmatrix} X_1 \\\\X_2 \\\\X_3\\end{bmatrix}\\)\n\nOur model is\n\n\\(Y = f(X) + \\varepsilon\\)\n\n\\(\\varepsilon\\) is our error"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#why-do-we-care-about-fx",
    "href": "slides/01-02-welcome_to_sl.html#why-do-we-care-about-fx",
    "title": "Chapter 1 and 2",
    "section": "Why do we care about \\(f(X)\\)?",
    "text": "Why do we care about \\(f(X)\\)?\n\nWe can use \\(f(X)\\) to make predictions of \\(Y\\) for new values of \\(X = x\\)\nWe can gain a better understanding of which components of \\(X = (X_1, X_2, \\dots, X_p)\\) are important for explaining \\(Y\\)\nDepending on how complex \\(f\\) is, maybe we can understand how each component ( \\(X_j\\) ) of \\(X\\) affects \\(Y\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx",
    "href": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx",
    "title": "Chapter 1 and 2",
    "section": "How do we choose \\(f(X)\\)?",
    "text": "How do we choose \\(f(X)\\)?\nWhat is a good value for \\(f(X)\\) at any selected value of \\(X\\), say \\(X = 100\\)? There can be many \\(Y\\) values at \\(X = 100\\)."
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx-1",
    "href": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx-1",
    "title": "Chapter 1 and 2",
    "section": "How do we choose \\(f(X)\\)?",
    "text": "How do we choose \\(f(X)\\)?\nWhat is a good value for \\(f(X)\\) at any selected value of \\(X\\), say \\(X = 100\\)? There can be many \\(Y\\) values at \\(X = 100\\)."
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx-2",
    "href": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx-2",
    "title": "Chapter 1 and 2",
    "section": "How do we choose \\(f(X)\\)?",
    "text": "How do we choose \\(f(X)\\)?\nWhat is a good value for \\(f(X)\\) at any selected value of \\(X\\), say \\(X = 100\\)? There can be many \\(Y\\) values at \\(X = 100\\).\n\nThere are 17 points here, what value should I choose for f(100). What do you think the blue dot represents?"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx-3",
    "href": "slides/01-02-welcome_to_sl.html#how-do-we-choose-fx-3",
    "title": "Chapter 1 and 2",
    "section": "How do we choose \\(f(X)\\)?",
    "text": "How do we choose \\(f(X)\\)?\nA good value is\n\\[f(100) = E(Y|X = 100)\\]\n\n\\(E(Y|X = 100)\\) means expected value (average) of \\(Y\\) given \\(X = 100\\)\n\n\nThis ideal \\(f(x) = E(Y | X = x)\\) is called the regression function"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-function-fx",
    "href": "slides/01-02-welcome_to_sl.html#regression-function-fx",
    "title": "Chapter 1 and 2",
    "section": "Regression function, \\(f(X)\\)",
    "text": "Regression function, \\(f(X)\\)\n\nAlso works or a vector, \\(X\\), for example,\n\n\\[f(x) = f(x_1, x_2, x_3) = E[Y | X_1 = x_1, X_2 = x_2, X_3 = x_3]\\]\n\nThis is the optimal predictor of \\(Y\\) in terms of mean-squared prediction error"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-function-fx-1",
    "href": "slides/01-02-welcome_to_sl.html#regression-function-fx-1",
    "title": "Chapter 1 and 2",
    "section": "Regression function, \\(f(X)\\)",
    "text": "Regression function, \\(f(X)\\)\n\n\\(f(x) = E(Y|X = x)\\) is the function that minimizes \\(E[(Y - g(X))^2 |X = x]\\) over all functions \\(g\\) at all points \\(X = x\\)\n\n\n\\(\\varepsilon = Y - f(x)\\) is the irreducible error\neven if we knew \\(f(x)\\), we would still make errors in prediction, since at each \\(X = x\\) there is typically a distribution of possible \\(Y\\) values"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-function-fx-2",
    "href": "slides/01-02-welcome_to_sl.html#regression-function-fx-2",
    "title": "Chapter 1 and 2",
    "section": "Regression function, \\(f(X)\\)",
    "text": "Regression function, \\(f(X)\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-function-fx-3",
    "href": "slides/01-02-welcome_to_sl.html#regression-function-fx-3",
    "title": "Chapter 1 and 2",
    "section": "Regression function, \\(f(X)\\)",
    "text": "Regression function, \\(f(X)\\)\n\nUsing these points, how would I calculate the regression function?\n\n\n\nTake the average! \\(f(100) = E[\\texttt{mpg}|\\texttt{horsepower} = 100] = 19.6\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#regression-function-fx-4",
    "href": "slides/01-02-welcome_to_sl.html#regression-function-fx-4",
    "title": "Chapter 1 and 2",
    "section": "Regression function, \\(f(X)\\)",
    "text": "Regression function, \\(f(X)\\)\n\nThis point has a \\(Y\\) value of 32.9. What is \\(\\hat\\varepsilon\\)?\n\n\n\\(\\hat\\varepsilon = Y - \\hat{f}(X) = 32.9 - 19.6 = \\color{red}{13.3}\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#the-error",
    "href": "slides/01-02-welcome_to_sl.html#the-error",
    "title": "Chapter 1 and 2",
    "section": "The error",
    "text": "The error\nFor any estimate, \\(\\hat{f}(x)\\), of \\(f(x)\\), we have\n\\[E[(Y - \\hat{f}(x))^2 | X = x] = \\underbrace{[f(x) - \\hat{f}(x)]^2}_{\\textrm{reducible error}} + \\underbrace{Var(\\varepsilon)}_{\\textrm{irreducible error}}\\]\n\nAssume for a moment that both \\(\\hat{f}\\) and X are fixed.\n\\(E(Y − \\hat{Y})^2\\) represents the average, or expected value, of the squared difference between the predicted and actual value of Y, and Var( \\(\\varepsilon\\) ) represents the variance associated with the error term\nThe focus of this class is on techniques for estimating f with the aim of minimizing the reducible error.\nthe irreducible error will always provide an upper bound on the accuracy of our prediction for Y\nThis bound is almost always unknown in practice"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#estimating-f",
    "href": "slides/01-02-welcome_to_sl.html#estimating-f",
    "title": "Chapter 1 and 2",
    "section": "Estimating \\(f\\)",
    "text": "Estimating \\(f\\)\n\nTypically we have very few (if any!) data points at \\(X=x\\) exactly, so we cannot compute \\(E[Y|X=x]\\)\nFor example, what if we were interested in estimating miles per gallon when horsepower was 104.\n\n\n\n\n\n💡 We can relax the definition and let\n\\[\\hat{f}(x) = E[Y | X\\in \\mathcal{N}(x)]\\]\n\nWhere \\(\\mathcal{N}(x)\\) is some neighborhood of \\(x\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#notation-pause",
    "href": "slides/01-02-welcome_to_sl.html#notation-pause",
    "title": "Chapter 1 and 2",
    "section": "Notation pause!",
    "text": "Notation pause!\n\n\\[\\hat{f}(x) = \\underbrace{E}_{\\textrm{The expectation}}[\\underbrace{Y}_{\\textrm{of Y}} \\underbrace{|}_{\\textrm{given}} \\underbrace{X\\in \\mathcal{N}(x)}_{\\textrm{X is in the neighborhood of x}}]\\]\n\n\n🚨 If you need a notation pause at any point during this class, please let me know!"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#estimating-f-1",
    "href": "slides/01-02-welcome_to_sl.html#estimating-f-1",
    "title": "Chapter 1 and 2",
    "section": "Estimating \\(f\\)",
    "text": "Estimating \\(f\\)\n💡 We can relax the definition and let\n\\[\\hat{f}(x) = E[Y | X\\in \\mathcal{N}(x)]\\]\n\nNearest neighbor averaging does pretty well with small \\(p\\) ( \\(p\\leq 4\\) ) and large \\(n\\)\nNearest neighbor is not great when \\(p\\) is large because of the curse of dimensionality (because nearest neighbors tend to be far away in high dimensions)\n\n\n\nWhat do I mean by \\(p\\)? What do I mean by \\(n\\)?"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#parametric-models",
    "href": "slides/01-02-welcome_to_sl.html#parametric-models",
    "title": "Chapter 1 and 2",
    "section": "Parametric models",
    "text": "Parametric models\nA common parametric model is a linear model\n\\[f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p\\]\n\nA linear model has \\(p + 1\\) parameters ( \\(\\beta_0,\\dots,\\beta_p\\) )\nWe estimate these parameters by fitting a model to training data\nAlthough this model is almost never correct it can often be a good interpretable approximation to the unknown true function, \\(f(X)\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-1",
    "href": "slides/01-02-welcome_to_sl.html#section-1",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "The  red  points are simulated values for income from the model:\n\n\\[\\texttt{income} = f(\\texttt{education, senority}) + \\varepsilon\\]\n\n\\(f\\) is the  blue  surface"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-2",
    "href": "slides/01-02-welcome_to_sl.html#section-2",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Linear regression model fit to the simulated data\n\\[\\hat{f}_L(\\texttt{education, senority}) = \\hat{\\beta}_0 + \\hat{\\beta}_1\\texttt{education}+\\hat{\\beta}_2\\texttt{senority}\\]"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-3",
    "href": "slides/01-02-welcome_to_sl.html#section-3",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "More flexible regression model \\(\\hat{f}_S(\\texttt{education, seniority})\\) fit to the simulated data.\nHere we use a technique called a thin-plate spline to fit a flexible surface"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-4",
    "href": "slides/01-02-welcome_to_sl.html#section-4",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "And even MORE flexible 😱 model \\(\\hat{f}(\\texttt{education, seniority})\\).\n\nHere we’ve basically drawn the surface to hit every point, minimizing the error, but completely overfitting"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#finding-balance",
    "href": "slides/01-02-welcome_to_sl.html#finding-balance",
    "title": "Chapter 1 and 2",
    "section": "🤹 Finding balance",
    "text": "🤹 Finding balance\n\nPrediction accuracy versus interpretability\nLinear models are easy to interpret, thin-plate splines are not\nGood fit versus overfit or underfit\nHow do we know when the fit is just right?\nParsimony versus black-box\nWe often prefer a simpler model involving fewer variables over a black-box predictor involving them all"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy",
    "href": "slides/01-02-welcome_to_sl.html#accuracy",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\nWe’ve fit a model \\(\\hat{f}(x)\\) to some training data.\nWe can measure accuracy as the average squared prediction error over that train data\n\n\\[MSE_{\\texttt{train}} = \\textrm{Ave}_{train}[y_i-\\hat{f}(x_i)]^2\\]\n\n\nWhat can go wrong here?\n\n\nThis may be biased towards overfit models"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy-1",
    "href": "slides/01-02-welcome_to_sl.html#accuracy-1",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\nI have some train data, plotted above. What \\(\\hat{f}(x)\\) would minimize the \\(MSE_{\\texttt{train}}\\)?\n\n\\[MSE_{\\texttt{train}} = \\textrm{Ave}_{train}[y_i-\\hat{f}(x_i)]^2\\]"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy-2",
    "href": "slides/01-02-welcome_to_sl.html#accuracy-2",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\nI have some train data, plotted above. What \\(\\hat{f}(x)\\) would minimize the \\(MSE_{\\texttt{train}}\\)?\n\n\\[MSE_{train} = \\textrm{Ave}_{i\\in\\texttt{train}}[y_i-\\hat{f}(x_i)]^2\\]"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy-3",
    "href": "slides/01-02-welcome_to_sl.html#accuracy-3",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\nWhat is wrong with this?\n\n\nIt’s overfit!"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy-4",
    "href": "slides/01-02-welcome_to_sl.html#accuracy-4",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\nIf we get a new sample, that overfit model is probably going to be terrible!"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy-5",
    "href": "slides/01-02-welcome_to_sl.html#accuracy-5",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\nWe’ve fit a model \\(\\hat{f}(x)\\) to some training data.\nInstead of measuring accuracy as the average squared prediction error over that train data, we can compute it using fresh test data.\n\n\\[MSE_{\\texttt{test}} = \\textrm{Ave}_{test}[y_i-\\hat{f}(x_i)]^2\\]"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-6",
    "href": "slides/01-02-welcome_to_sl.html#section-6",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Black curve is the “truth” on the left.  Red  curve on right is \\(MSE_{\\texttt{test}}\\), grey curve is \\(MSE_{\\texttt{train}}\\). Orange, blue and green curves/squares correspond to fis of different flexibility."
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-7",
    "href": "slides/01-02-welcome_to_sl.html#section-7",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Here the truth is smoother, so the smoother fit and linear model do really well"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-8",
    "href": "slides/01-02-welcome_to_sl.html#section-8",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Here the truth is wiggly and the noise is low, so the more flexible fits do the best"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#bias-variance-trade-off",
    "href": "slides/01-02-welcome_to_sl.html#bias-variance-trade-off",
    "title": "Chapter 1 and 2",
    "section": "Bias-variance trade-off",
    "text": "Bias-variance trade-off\n\nWe’ve fit a model, \\(\\hat{f}(x)\\), to some training data\nLet’s pull a test observation from this population ( \\(x_0, y_0\\) )\nThe true model is \\(Y = f(x) + \\varepsilon\\)\n\\(f(x) = E[Y|X=x]\\)\n\n\n\\[E(y_0 - \\hat{f}(x_0))^2 = \\textrm{Var}(\\hat{f}(x_0)) + [\\textrm{Bias}(\\hat{f}(x_0))]^2 + \\textrm{Var}(\\varepsilon)\\]\n\n\nThe expectation averages over the variability of \\(y_0\\) as well as the variability of the training data. \\(\\textrm{Bias}(\\hat{f}(x_0)) =E[\\hat{f}(x_0)]-f(x_0)\\)\n\nAs flexibility of \\(\\hat{f}\\) \\(\\uparrow\\), its variance \\(\\uparrow\\) and its bias \\(\\downarrow\\)\nchoosing the flexibility based on average test error amounts to a bias-variance trade-off\n\n\n\nThat U-shape we see for the test MSE curves is due to this bias-variance trade-off\nThe expected test MSE for a given \\(x_0\\) can be decomposed into three components: the variance of \\(\\hat{f}(x_o)\\), the squared bias of \\(\\hat{f}(x_o)\\) and t4he variance of the error term \\(\\varepsilon\\)\nHere the notation \\(E[y_0 − \\hat{f}(x_0)]^2\\) defines the expected test MSE, and refers to the average test MSE that we would obtain if we repeatedly estimated \\(f\\) using a large number of training sets, and tested each at \\(x_0\\)\nThe overall expected test MSE can be computed by averaging \\(E[y_0 − \\hat{f}(x_0)]^2\\) over all possible values of \\(x_0\\) in the test set.\nSO we want to minimize the expected test error, so to do that we need to pick a statistical learning method to simultenously acheive low bias and low variance.\nSince both of these quantities are non-negative, the expected test MSE can never fall below Var( \\(\\varepsilon\\) )"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#bias-variance-trade-off-1",
    "href": "slides/01-02-welcome_to_sl.html#bias-variance-trade-off-1",
    "title": "Chapter 1 and 2",
    "section": "Bias-variance trade-off",
    "text": "Bias-variance trade-off"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#conceptual-idea",
    "href": "slides/01-02-welcome_to_sl.html#conceptual-idea",
    "title": "Chapter 1 and 2",
    "section": "Conceptual Idea",
    "text": "Conceptual Idea\nWatch StatQuest video: Machine Learning Fundamentals: Bias and Variance"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#notation-1",
    "href": "slides/01-02-welcome_to_sl.html#notation-1",
    "title": "Chapter 1 and 2",
    "section": "Notation",
    "text": "Notation\n\n\\(Y\\) is the response variable. It is qualitative\n\\(\\mathcal{C}(X)\\) is the classifier that assigns a class \\(\\mathcal{C}\\) to some future unlabeled observation, \\(X\\)\nExamples:\nEmail can be classified as \\(\\mathcal{C}=(\\texttt{spam, not spam})\\)\nWritten number is one of \\(\\mathcal{C}=\\{0, 1, 2, \\dots, 9\\}\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#classification-problem",
    "href": "slides/01-02-welcome_to_sl.html#classification-problem",
    "title": "Chapter 1 and 2",
    "section": "Classification Problem",
    "text": "Classification Problem\n\nWhat is the goal?\n\n\nBuild a classifier \\(\\mathcal{C}(X)\\) that assigns a class label from \\(\\mathcal{C}\\) to a future unlabeled observation \\(X\\)\n\nAssess the uncertainty in each classification\n\nUnderstand the roles of the different predictors among \\(X = (X_1, X_2, \\dots, X_p)\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-9",
    "href": "slides/01-02-welcome_to_sl.html#section-9",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Suppose there are \\(K\\) elements in \\(\\mathcal{C}\\), numbered \\(1, 2, \\dots, K\\)\n\\[p_k(x) = P(Y = k|X=x), k = 1, 2, \\dots, K\\] These are conditional class probabilities at \\(x\\)\n\n\nHow do you think we could calculate this?\n\n\n\n\nIn the plot, you could examine the mini-barplot at \\(x = 5\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-10",
    "href": "slides/01-02-welcome_to_sl.html#section-10",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "Suppose there are \\(K\\) elements in \\(\\mathcal{C}\\), numbered \\(1, 2, \\dots, K\\)\n\\[p_k(x) = P(Y = k|X=x), k = 1, 2, \\dots, K\\] These are conditional class probabilities at \\(x\\)\n\nThe Bayes optimal classifier at \\(x\\) is\n\n\\[\\mathcal{C}(x) = j \\textrm{ if } p_j(x) = \\textrm{max}\\{p_1(x), p_2(x), \\dots, p_K(x)\\}\\]\n\n\nNotice that probability is a conditional probability\nIt is the probability that Y equals k given the observed preditor vector, \\(x\\)\nLet’s say we were using a Bayes Classifier for a two class problem, Y is 1 or 2. We would predict that the class is one if \\(P(Y=1|X=x_0)&gt;0.5\\) and 2 otherwise"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#section-11",
    "href": "slides/01-02-welcome_to_sl.html#section-11",
    "title": "Chapter 1 and 2",
    "section": "",
    "text": "What if this was our data and there were no points at exactly \\(x = 5\\)? Then how could we calculate this?\n\n\nNearest neighbor like before!\nThis does break down as the dimensions grow, but the impact of \\(\\mathcal{\\hat{C}}(x)\\) is less than on \\(\\hat{p}_k(x), k = 1,2,\\dots,K\\)"
  },
  {
    "objectID": "slides/01-02-welcome_to_sl.html#accuracy-6",
    "href": "slides/01-02-welcome_to_sl.html#accuracy-6",
    "title": "Chapter 1 and 2",
    "section": "Accuracy",
    "text": "Accuracy\n\nMisclassification error rate\n\n\\[Err_{\\texttt{test}} = \\frac{\\#correct predictions}{total predictions} = \\textrm{Ave}_{test}I[y_i\\neq \\mathcal{\\hat{C}}(x_i)]\\] &gt; * \\(I(\\cdot)\\) is an indicator function and will only be eitehr 0 or 1.\n\nThe Bayes Classifier using the true \\(p_k(x)\\) has the smallest error\nSome of the methods we (may) learn build structured models for \\(\\mathcal{C}(x)\\) (support vector machines, for example)\nSome build structured models for \\(p_k(x)\\) (logistic regression, for example)\n\n\n\nthe test error rate \\(\\textrm{Ave}_{i\\in\\texttt{test}}I[y_i\\neq \\mathcal{\\hat{C}}(x_i)]\\) is minimized on average by very simple classifier that assigns each observation to the most likely class, given its predictor values (that’s the Bayes classifier)\n\n\n\n\n\n\n🔗 https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "labs/01-cv-tm.html",
    "href": "labs/01-cv-tm.html",
    "title": "Lab 01",
    "section": "",
    "text": "Go to our RStudio and create a new R project inside your class folder."
  },
  {
    "objectID": "labs/01-cv-tm.html#yaml",
    "href": "labs/01-cv-tm.html#yaml",
    "title": "Lab 01",
    "section": "YAML:",
    "text": "YAML:\nOpen the .qmd file in your project, make sure the author is your name, and Render the document."
  },
  {
    "objectID": "labs/01-cv-tm.html#conceptual-questions",
    "href": "labs/01-cv-tm.html#conceptual-questions",
    "title": "Lab 01",
    "section": "Conceptual questions",
    "text": "Conceptual questions\n\nExplain how k-fold Cross Validation is implemented.\nWhat are the advantages / disadvantages of k-fold Cross Validation compared to the Validation Set approach? What are the advantages / disadvantages of k-fold Cross Validation compared to Leave-one-out Cross Validation?"
  },
  {
    "objectID": "labs/01-cv-tm.html#data-exploration",
    "href": "labs/01-cv-tm.html#data-exploration",
    "title": "Lab 01",
    "section": "Data exploration",
    "text": "Data exploration\n\nFor this analysis, we are using the Auto dataset from the ISLR package. How many rows are in this dataset? How many columns? Is there any missing data?\nOur outcome of interest is miles per gallon: mpg. Create a publication-ready figure examining the distribution of this variable.\nOur main predictor of interest is horsepower. Create a publication-ready figure looking at the relationship between miles per gallon and horsepower."
  },
  {
    "objectID": "labs/01-cv-tm.html#k-fold-cross-validation",
    "href": "labs/01-cv-tm.html#k-fold-cross-validation",
    "title": "Lab 01",
    "section": "K-fold cross validation",
    "text": "K-fold cross validation\nWe are trying to decide between three models of varying flexibility:\n\nModel 1: \\(\\texttt{mpg} = \\beta_0 + \\beta_1 \\texttt{horsepower} + \\epsilon\\)\nModel 2: \\(\\texttt{mpg} = \\beta_0 + \\beta_1 \\texttt{horsepower} + \\beta_2 \\texttt{horsepower}^2 + \\epsilon\\)\nModel 3: \\(\\texttt{mpg} = \\beta_0 + \\beta_1 \\texttt{horsepower} + \\beta_2 \\texttt{horsepower}^2 + \\beta_3 \\texttt{horsepower}^3 + \\epsilon\\)\n\n\nUsing the Auto data, split the data into two groups a training data set, saved as Auto_train and a testing data set, saved as Auto_test. Be sure to set a seed to ensure that you get the same result each time you Render your document.\n\n\n\nYou can use the poly() function to fit a model with a polynomial term. For example, to fit the model \\(y = \\beta_0 + \\beta_1 \\texttt{x} + \\beta_2 \\texttt{x}^2 + \\beta_3 \\texttt{x}^3 + \\epsilon\\), you would run fit(lm_spec, y ~ poly(x, 3), data = data)\n\nFit the three models outlined above on the training data. Using the model created on the training data, predict mpg in the test data set for each model. What is the test RMSE for the three models? Which model would you choose?\nFit the same three models, but instead of the validation set approach, perform 5-fold cross validation. Make sure to set a seed so you get the same answer each time you run the analysis. Calculate the overall 5-fold cross validation error for each of the three models. Which model would you chose?\nThe tidymodels package allows you to do this faster! Instead of having a fit 3 (or more!) different models to determine the best flexibility, you can (1) create a recipe to specify how you would like to fit a model and then (2) tune this model to determine the best output. Copy the code below. What do you think the line step_poly(horsepower, degree = tune()) does? Hint: you can run ?step_poly in the Console to learn more about this function.\n\n\nauto_prep &lt;- Auto |&gt;\n  recipe(mpg ~ horsepower) |&gt;\n  step_poly(horsepower, degree = tune())\n\n\nTo tune this model, you will replace fit_resamples with tune_grid. The pseudo code to do this is below - you may need to update some names to match what you have named objects in your document. Add the code to tune your model based on the code below.\n\n\nauto_tune &lt;- tune_grid(lm_spec,\n          auto_prep,\n          resamples = auto_cv)\n\n\nUsing the collect_metrics function, look at the RMSE for auto_tune. Which degree is preferable?\nYou can plot the output from Exercise 11 to make it a bit easier to determine. First, save your output from Exercise 11 as auto_metrics. Then filter this data frame to only include rows where .metric == \"rmse\". Save this filtered data frame as auto_rmse. Edit the code below to plot the degree on the x-axis and mean on the y-axis. Describe what this plot shows.\n\n\nggplot(auto_rmse, aes(x = ----, y = ----)) + \n  geom_line() +\n  geom_pointrange(aes(ymin = mean - std_err, ymax = mean + std_err)) + \n  labs(x = \"Degree\",\n       y = \"Cross validation error\",\n       title = ---)"
  },
  {
    "objectID": "hw/hw-01-intro-review.html",
    "href": "hw/hw-01-intro-review.html",
    "title": "Homework 1",
    "section": "",
    "text": "R is the name of the programming language itself and RStudio is a convenient interface.\nThe main goal of this homework is to re-introduce you to R and RStudio, which we will be using throughout the course both to learn the statistical concepts discussed in the course and to analyze real data and come to informed conclusions.\nAs the homework’s progress, you are encouraged to explore beyond what the homework dictates; a willingness to experiment will make you a much better programmer. Before we get to that stage, however, you need to build some basic fluency in R. Today we begin with the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands."
  },
  {
    "objectID": "hw/hw-01-intro-review.html#this-one-time",
    "href": "hw/hw-01-intro-review.html#this-one-time",
    "title": "Homework 1",
    "section": "This One Time",
    "text": "This One Time\n\nGo to our RStudio Server at http://turing.cornellcollege.edu:8787/\nClick File tab on the bottom right and then click the work Home.\nCreate a new folder using the little folder icon with the green plus on it. Use STA 362 in the folder name."
  },
  {
    "objectID": "hw/hw-01-intro-review.html#every-homeworklabactivity",
    "href": "hw/hw-01-intro-review.html#every-homeworklabactivity",
    "title": "Homework 1",
    "section": "Every Homework/lab/activity",
    "text": "Every Homework/lab/activity\nEach of your assignments will begin with the following steps.\n\nFinding the instructions on our website: https://sta362-sb8-24.github.io/STA362StatLearning/\nGoing to our RStudio Server at http://turing.cornellcollege.edu:8787/\nCreating a new project. and giving it a sensible name such as homework_1 and having that project in the course folder you created.\nCreate a new quarto document and give it a sensible name such as hw1.\nIn the YAML add the following (add what you don’t have). The embed-resources component will make your final rendered html self-contained.\n\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\n    embed-resources: true\n---"
  },
  {
    "objectID": "hw/hw-01-intro-review.html#yaml",
    "href": "hw/hw-01-intro-review.html#yaml",
    "title": "Homework 1",
    "section": "YAML",
    "text": "YAML\nIn your Quarto (qmd) file in your project, change the author name to your name, and render the document. Make sure that you also have added the extra YAML clode above."
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "Introduction to Statistical Learning",
    "section": "",
    "text": "Course Description\nThis course will introduce students to relatively new and powerful statistical techniques used to analyze data. The course will begin with a review of linear regression and an introduction to computer-based variable and model selection methods. Other topics will include classification methods, resampling methods for model-building, non-linear models, and tree-based methods. The computer software program R will be used throughout.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "course-instructor.html",
    "href": "course-instructor.html",
    "title": "Instructor",
    "section": "",
    "text": "Dr. Tyler George (he/him) is a Assistant Professor of Statistics at Cornell College. He received his PhD in Statistics and Analytics from Central Michigan University. During his PhD he also studied mathematics and statistics education. His dissertation work involved creating a new lack of fit test for linear regression models. His interests are broadly in statistics, data science and best pedagogy to teach them.\n\n\n\nOffice hours\nLocation\n\n\n\n\nMonday - Thursday 3:05pm - 4:05pm\nWest 311\n\n\nOther Times by Appointment\nWest 311\n\n\n\nOffice hours are for STUDENTS. Please take advantage of them to get help with class, advising, and/or getting to know your professor! If you miss class, check out course calendar to verify there have been no changes.",
    "crumbs": [
      "Course information",
      "Instructor"
    ]
  },
  {
    "objectID": "course-instructor.html#instructor",
    "href": "course-instructor.html#instructor",
    "title": "Instructor",
    "section": "",
    "text": "Dr. Tyler George (he/him) is a Assistant Professor of Statistics at Cornell College. He received his PhD in Statistics and Analytics from Central Michigan University. During his PhD he also studied mathematics and statistics education. His dissertation work involved creating a new lack of fit test for linear regression models. His interests are broadly in statistics, data science and best pedagogy to teach them.\n\n\n\nOffice hours\nLocation\n\n\n\n\nMonday - Thursday 3:05pm - 4:05pm\nWest 311\n\n\nOther Times by Appointment\nWest 311\n\n\n\nOffice hours are for STUDENTS. Please take advantage of them to get help with class, advising, and/or getting to know your professor! If you miss class, check out course calendar to verify there have been no changes.",
    "crumbs": [
      "Course information",
      "Instructor"
    ]
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "To access computing resources the course, Introduction to Data Science, offered by the Cornell College Department of Mathematics and Statistics, go to the RStudio Server while on campus and connected to campus internet.\nYour account will be pre-created before the class begins and will use your Cornell College username. The default password will be shared in class and you will need to change it.",
    "crumbs": [
      "Course information",
      "R/RStudio Access"
    ]
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful links",
    "section": "",
    "text": "RStudio Server\n🔗 on Cornell College Cluster\n\n\nCourse GitHub organization\n🔗 on GitHub\n\n\nGradebook\n🔗 on Moodle",
    "crumbs": [
      "Course Contents",
      "Useful links"
    ]
  },
  {
    "objectID": "course-links.html#course-links",
    "href": "course-links.html#course-links",
    "title": "Useful links",
    "section": "",
    "text": "RStudio Server\n🔗 on Cornell College Cluster\n\n\nCourse GitHub organization\n🔗 on GitHub\n\n\nGradebook\n🔗 on Moodle",
    "crumbs": [
      "Course Contents",
      "Useful links"
    ]
  },
  {
    "objectID": "course-links.html#other-useful-links",
    "href": "course-links.html#other-useful-links",
    "title": "Useful links",
    "section": "Other Useful Links",
    "text": "Other Useful Links\n\nData Wrangling and Viz Interactive Tutorials\nRStudio Cheatsheets\nIntroduction to dplyr\nR Date Examples\nNY Times Cornell College Sign-up\nR for Data Science 2nd Ed\nQuarto Documentation\nData visualization\n\nggplot2 Reference\nggplot2: Elegant Graphics for Data Analysis\nData Visualization: A Practice Introduction\nPatchwork R Package",
    "crumbs": [
      "Course Contents",
      "Useful links"
    ]
  },
  {
    "objectID": "course-links.html#data-links",
    "href": "course-links.html#data-links",
    "title": "Useful links",
    "section": "Data Links",
    "text": "Data Links\n\nTidyTuesday\nR Data Sources for Regression Analysis\nFiveThirtyEight data\nAmazon Registry of Open Data\nOpen data StackExchange\nMicrosoft R Application Window\nData.gov\nUS Census\nNew York City data\nGeorge Mason University Data Link List\nToward Data Science list of Data Sources\nNHS Scotland Open Data\nEdinburgh Open Data\nOpen access to Scotland’s official statistics\nBikeshare data portal\nUK Gov Data\nKaggle datasets\nOpenIntro datasets\nAwesome public datasets\nYouth Risk Behavior Surveillance System (YRBSS)\nPRISM Data Archive Project\nHarvard Dataverse\nAndrew G. Reiter Poly Scie Datasets\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies\nU.S. Data\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nState of Iowa Open Geospatial Data\nIf you know of others, let me know!",
    "crumbs": [
      "Course Contents",
      "Useful links"
    ]
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you can identify when that is without getting too frustrated and feel comfortable seeking help.\nYou can ask anonymous course questions by adding them at https://docs.google.com/spreadsheets/d/1R33yYyRdFoRMDtSnD-CzLOqIEwYNrC3kepMm4ZN_PWM/edit?usp=sharing.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#lectures-and-labs",
    "href": "course-support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#office-hours",
    "href": "course-support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nYou are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage each and every one of you to take advantage of this resource! Make a pledge to stop by office hours at least once during the first few days of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of your professors office hours here.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#academic-support",
    "href": "course-support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\n\nQuantitative Reasoning Studio (QRS)\nThere are times you may need help outside of class or office hours. Or, maybe you need something explained in a different way. In those instances, I encourage you to visit the Quantitative Reasoning Studio in Cole Library room 322. The Quantitative Reasoning Studio (QRS) offers free tutoring to all students at Cornell College. There will be at least 1 peer tutor that has taken this course and will be able to help you, if you arrive at a time they are working. Feel free to email Jessica Johanningmeier at QRS@cornellcollege.edu to ask when the tutor for this class will be available. They often will have a schedule posted on the wall in the studio.\n\n\nQRS Hours\n\n\n\nDay(s)\nTimes\n\n\n\n\nMonday-Thursday\n8 a.m. - 5 p.m. and 7 p.m. - 10 p.m.\n\n\nFriday\n8 a.m. - 5 p.m.\n\n\nSunday\n3 p.m. - 5 p.m. and 7 p.m. - 10 p.m.\n\n\n\n\n\nDungy Writing Studio\nFor help with your writing, visit the Dungy Writing Studio. You can make online appointments individual or groups to get help with items such as your group project. If you have any questions about the studio, email Dungy Writing Studio Director and Director of Fellowships and Scholarships, Laura Farmer, at lfarmer@cornellcollege.edu.\n\n\nWriting Studio Hours\n\n\n\nDay(s)\nTimes\n\n\n\n\nMonday-Thursday\n8 a.m. - 5 p.m. and 7 p.m. - 10 p.m.\n\n\nFriday\n8 a.m. - 5 p.m.\n\n\nSunday\n1 p.m. - 5 p.m.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#student-success-center",
    "href": "course-support.html#student-success-center",
    "title": "Course support",
    "section": "Student Success Center",
    "text": "Student Success Center\nThe Student Success Center is a resource for all students. Their staff serves as student success coaches for all students and welcome students to visit us to talk about academic concerns, study plans, finding their place at Cornell, or any questions you have and aren’t sure where to start! You can walk in to chat or contact a staff member directly to set up an appointment! See the website for more information.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#professor-email",
    "href": "course-support.html#professor-email",
    "title": "Course support",
    "section": "Professor Email",
    "text": "Professor Email\nIf you are not available during office hours times or have a questions later in the evening or other times outside of class, email your professor at tgeorge@cornellcollege.edu. If your question involves code - it is very likely you will need to meet with him to get help. Please reach out with any concerns you have during the course!",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#ebersole-health-and-wellbeing-center",
    "href": "course-support.html#ebersole-health-and-wellbeing-center",
    "title": "Course support",
    "section": "Ebersole Health and Wellbeing Center",
    "text": "Ebersole Health and Wellbeing Center\nThe mission of Cornell College Student Health Services complements the mission of the college by promoting the optimal well-being of students. We do this by:\n\nproviding and coordinating quality health care services\nadvocating for students in their pursuit of health and wellness\npreparing students to be their own health advocates and informed consumers of appropriate health care services\nproviding health education to promote the development of healthy lifestyles\n\nThe Student Health Center is located in the Ebersole Building, directly south of the Thomas Commons. Appointments are preferred. You can schedule an appointment online or by phone at 319-895-4292. Walk-ins will be accommodated as time permits. Appointments with the nurse are free.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#technology-support",
    "href": "course-support.html#technology-support",
    "title": "Course support",
    "section": "Technology Support",
    "text": "Technology Support\nIf you have issues with your computer during the block, IT may be able to help. Please submit a ticket.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#course-materials-costs",
    "href": "course-support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.).",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Statistical Learning Schedule",
    "section": "",
    "text": "Note: The timeline of topics and assignments might be updated throughout the semester.\n\n\n\n\n\n\n\n\n\nDay\nDate\nTopic\nNotes\nLab\nHomework\n\n\n\n\n1\n15 April\nCourse Intro, Ch 1 - Stat Learning Examples, Ch 2 - SL, Bias-Variance Tradeoff\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\n16 April\nCh 5 - Cross Validation, Tidy Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\n17 April\nCh 5 - Cross Validation, Tidy Models, Ch 3 - LR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4\n18 April\nCh 4 - Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5\n19 April\nCh 4 - LDA, QDA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6\n22 April\nCh 4 - Naive Bayes, KMeans Classification\n\n\n\n\n\n\n\n\n\n\n\n\n\n7\n23 April\n\n\n\n\n\n\n\n\n\n\n\n\n8\n24 April\n\n\n\n\n\n\n\n\n\n\n\n\n9\n25 April\n\n\n\n\n\n\n\n\n\n\n\n\n10\n26 April\nExam\n\n\n\n\n\n\n\n\n\n\n\n11\n29 April\n\n\n\n\n\n\n\n\n\n\n\n\n12\n30 April\n\n\n\n\n\n\n\n\n\n\n\n\n13\n1 May\n\n\n\n\n\n\n\n\n\n\n\n\n14\n2 May\n\n\n\n\n\n\n\n\n\n\n\n\n15\n3 May\n\n\n\n\n\n\n\n\n\n\n\n\n16\n6 May\n\n\n\n\n\n\n\n\n\n\n\n\n17\n7 May\n\n\n\n\n\n\n\n\n\n\n\n\n18\n8 May\nExam",
    "crumbs": [
      "Course Contents",
      "Schedule & Assignments"
    ]
  },
  {
    "objectID": "labs/02-logistic.html",
    "href": "labs/02-logistic.html",
    "title": "Lab 02",
    "section": "",
    "text": "Go to our RStudio and create a new R project inside your class folder.\n\n\nCreate a .qmd file for your lab, make sure the author is your name, and Render the document."
  },
  {
    "objectID": "labs/02-logistic.html#yaml",
    "href": "labs/02-logistic.html#yaml",
    "title": "Lab 02",
    "section": "",
    "text": "Create a .qmd file for your lab, make sure the author is your name, and Render the document."
  },
  {
    "objectID": "labs/02-logistic.html#conceptual-questions",
    "href": "labs/02-logistic.html#conceptual-questions",
    "title": "Lab 02",
    "section": "Conceptual questions",
    "text": "Conceptual questions\n\nSuppose that an individual has a 23% chance of defaulting on their credit card payment. What are the odds that they will default?"
  },
  {
    "objectID": "labs/02-logistic.html#logistic-regression",
    "href": "labs/02-logistic.html#logistic-regression",
    "title": "Lab 02",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nFor this lab we are using the Smarket data. Examine this data set - how many observations are there? How many columns? What are the variables?\nLet’s look at the correlation between all of the variables. Add the code below to your .qmd file. What can you learn from this visualization? Which pair of variables have the highest correlation?\n\n\nggpairs(Smarket, \n        lower = list(combo = wrap(ggally_facethist, binwidth = 0.5)), \n        progress = FALSE)\n\n\nInference Fit a logistic regression model to predict Direction using Lag1, Lag2, Lag3, Lag4, Lag5, and Volume. Show a table that contains the coefficients and p-values along with the confidence intervals for each of the 6 predictors. Which predictor has the smallest p-value? Interpret the coefficient, confidence interval, and p-value for this predictor.\nInference Exponentiate the results from Exercise 4. Interpret the odds ratio for the same predictor you selected in Exercise 4.\nPrediction Using 5-fold cross validation, fit the same logistic regression model as Exercise 4. What is the test Accuracy for this model? Interpret this result.\nInference Fit a logistic regression model to predict Direction using only Lag1 and Lag2. Show a table that contains the coefficients and p-values along with the confidence intervals for each of the 2 predictors. Which predictor has the smallest p-value? Interpret the coefficient, confidence interval, and p-value for this predictor.\nInference Exponentiate the results from Exercise 7. Interpret the odds ratio for the same predictor you selected in Exercise 7.\nPrediction Using 5-fold cross validation, fit the same logistic regression model as Exercise 7. What is the test Accuracy for this model? Interpret this result.\nIf you had to choose between the model fit in Exercise 4 and the one fit in Exercise 7, which would you choose? Why?"
  },
  {
    "objectID": "slides/03-linear-regression.html#application-exercise",
    "href": "slides/03-linear-regression.html#application-exercise",
    "title": "Chapter 3 - Linear Regression",
    "section": " Application Exercise",
    "text": "Application Exercise\n\nCreate a new quarto file for this homework in your exercises R project."
  },
  {
    "objectID": "slides/03-linear-regression.html#lets-look-at-an-example",
    "href": "slides/03-linear-regression.html#lets-look-at-an-example",
    "title": "Chapter 3 - Linear Regression",
    "section": "Let’s look at an example",
    "text": "Let’s look at an example\nLet’s look at a sample of 116 sparrows from Kent Island. We are interested in the relationship between Weight and Wing Length\n\n\nthe standard error of \\(\\hat{\\beta_1}\\) ( \\(SE_{\\hat{\\beta}_1}\\) ) is how much we expect the sample slope to vary from one random sample to another."
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows",
    "href": "slides/03-linear-regression.html#sparrows",
    "title": "Chapter 3 - Linear Regression",
    "section": "Sparrows",
    "text": "Sparrows\n\nHow can we quantify how much we’d expect the slope to differ from one random sample to another?\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Weight ~ WingLength, data = Sparrows) |&gt;\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1\n2 WingLength     0.467    0.0347     13.5  2.62e-25"
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-1",
    "href": "slides/03-linear-regression.html#sparrows-1",
    "title": "Chapter 3 - Linear Regression",
    "section": "Sparrows",
    "text": "Sparrows\n\nHow do we interpret this?\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Weight ~ WingLength, data = Sparrows) |&gt;\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1\n2 WingLength     0.467    0.0347     13.5  2.62e-25\n\n\n\n“the sample slope is more than 13 standard errors above a slope of zero”"
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-2",
    "href": "slides/03-linear-regression.html#sparrows-2",
    "title": "Chapter 3 - Linear Regression",
    "section": "Sparrows",
    "text": "Sparrows\n\nHow do we know what values of this statistic are worth paying attention to?\n\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Weight ~ WingLength, data = Sparrows) |&gt;\n  tidy(conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1   -0.531     3.26 \n2 WingLength     0.467    0.0347     13.5  2.62e-25    0.399     0.536\n\n\n\nconfidence intervals\np-values"
  },
  {
    "objectID": "slides/03-linear-regression.html#application-exercise-1",
    "href": "slides/03-linear-regression.html#application-exercise-1",
    "title": "Chapter 3 - Linear Regression",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nFit a linear model using the mtcars data frame predicting miles per gallon (mpg) from weight and horsepower (wt and hp).\nPull out the coefficients and confidence intervals using the tidy() function demonstrated. How do you interpret these?"
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-3",
    "href": "slides/03-linear-regression.html#sparrows-3",
    "title": "Chapter 3 - Linear Regression",
    "section": "Sparrows",
    "text": "Sparrows\n\nHow are these statistics distributed under the null hypothesis?\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Weight ~ WingLength, data = Sparrows) |&gt;\n  tidy() \n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1\n2 WingLength     0.467    0.0347     13.5  2.62e-25"
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-4",
    "href": "slides/03-linear-regression.html#sparrows-4",
    "title": "Chapter 3 - Linear Regression",
    "section": "Sparrows",
    "text": "Sparrows\n\n\nI’ve generated some data under a null hypothesis where \\(n = 20\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-5",
    "href": "slides/03-linear-regression.html#sparrows-5",
    "title": "Chapter 3 - Linear Regression",
    "section": "Sparrows",
    "text": "Sparrows\n\n\nthis is a t-distribution with n-p-1 degrees of freedom."
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-6",
    "href": "slides/03-linear-regression.html#sparrows-6",
    "title": "Chapter 3 - Linear Regression",
    "section": "Sparrows",
    "text": "Sparrows\nThe distribution of test statistics we would expect given the null hypothesis is true, \\(\\beta_1 = 0\\), is t-distribution with n-2 degrees of freedom."
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-7",
    "href": "slides/03-linear-regression.html#sparrows-7",
    "title": "Chapter 3 - Linear Regression",
    "section": "Sparrows",
    "text": "Sparrows"
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-8",
    "href": "slides/03-linear-regression.html#sparrows-8",
    "title": "Chapter 3 - Linear Regression",
    "section": "Sparrows",
    "text": "Sparrows\n\nHow can we compare this line to the distribution under the null?\n\n\n\np-value"
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-9",
    "href": "slides/03-linear-regression.html#sparrows-9",
    "title": "Chapter 3 - Linear Regression",
    "section": "Sparrows",
    "text": "Sparrows\n\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Weight ~ WingLength, data = Sparrows) |&gt;\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1\n2 WingLength     0.467    0.0347     13.5  2.62e-25"
  },
  {
    "objectID": "slides/03-linear-regression.html#return-to-generated-data-n-20",
    "href": "slides/03-linear-regression.html#return-to-generated-data-n-20",
    "title": "Chapter 3 - Linear Regression",
    "section": "Return to generated data, n = 20",
    "text": "Return to generated data, n = 20\n\n\nLet’s say we get a statistic of 1.5 in a sample"
  },
  {
    "objectID": "slides/03-linear-regression.html#lets-do-it-in-r",
    "href": "slides/03-linear-regression.html#lets-do-it-in-r",
    "title": "Chapter 3 - Linear Regression",
    "section": "Let’s do it in R!",
    "text": "Let’s do it in R!\nThe proportion of area less than 1.5\n\n\npt(1.5, df = 18)\n\n[1] 0.9245248"
  },
  {
    "objectID": "slides/03-linear-regression.html#lets-do-it-in-r-1",
    "href": "slides/03-linear-regression.html#lets-do-it-in-r-1",
    "title": "Chapter 3 - Linear Regression",
    "section": "Let’s do it in R!",
    "text": "Let’s do it in R!\nThe proportion of area greater than 1.5\n\n\npt(1.5, df = 18, lower.tail = FALSE)\n\n[1] 0.07547523"
  },
  {
    "objectID": "slides/03-linear-regression.html#lets-do-it-in-r-2",
    "href": "slides/03-linear-regression.html#lets-do-it-in-r-2",
    "title": "Chapter 3 - Linear Regression",
    "section": "Let’s do it in R!",
    "text": "Let’s do it in R!\nThe proportion of area greater than 1.5 or less than -1.5.\n\n\n\npt(1.5, df = 18, lower.tail = FALSE) * 2\n\n[1] 0.1509505"
  },
  {
    "objectID": "slides/03-linear-regression.html#hypothesis-test",
    "href": "slides/03-linear-regression.html#hypothesis-test",
    "title": "Chapter 3 - Linear Regression",
    "section": "Hypothesis test",
    "text": "Hypothesis test\n\nnull hypothesis \\(H_0: \\beta_1 = 0\\)\nalternative hypothesis \\(H_A: \\beta_1 \\ne 0\\)\np-value: 0.15\nOften, we have an \\(\\alpha\\)-level cutoff to compare this to, for example 0.05. Since this is greater than 0.05, we fail to reject the null hypothesis"
  },
  {
    "objectID": "slides/03-linear-regression.html#application-exercise-2",
    "href": "slides/03-linear-regression.html#application-exercise-2",
    "title": "Chapter 3 - Linear Regression",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nUsing the linear model you fit previously (mpg from wt and hp) - calculate the p-value for the coefficient for weight\nInterpret this value. What is the null hypothesis? What is the alternative hypothesis? Do you reject the null?"
  },
  {
    "objectID": "slides/03-linear-regression.html#lets-do-it-in-r-3",
    "href": "slides/03-linear-regression.html#lets-do-it-in-r-3",
    "title": "Chapter 3 - Linear Regression",
    "section": "Let’s do it in R!",
    "text": "Let’s do it in R!\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Weight ~ WingLength, data = Sparrows) |&gt;\n  tidy(conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1   -0.531     3.26 \n2 WingLength     0.467    0.0347     13.5  2.62e-25    0.399     0.536\n\n\n\n\\(t^* = t_{n-p-1} = t_{114} = 1.98\\)\n\\(LB = 0.47 - 1.98\\times 0.0347 = 0.399\\)\n\\(UB = 0.47+1.98 \\times 0.0347 = 0.536\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#linear-regression-questions",
    "href": "slides/03-linear-regression.html#linear-regression-questions",
    "title": "Chapter 3 - Linear Regression",
    "section": "Linear Regression Questions",
    "text": "Linear Regression Questions\n\n✔️ Is there a relationship between a response variable and predictors?\n✔️ How strong is the relationship?\n✔️ What is the uncertainty?\nHow accurately can we predict a future outcome?"
  },
  {
    "objectID": "slides/03-linear-regression.html#sparrows-10",
    "href": "slides/03-linear-regression.html#sparrows-10",
    "title": "Chapter 3 - Linear Regression",
    "section": "Sparrows",
    "text": "Sparrows\n\nUsing the information here, how could I predict a new sparrow’s weight if I knew the wing length was 30?\n\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Weight ~ WingLength, data = Sparrows) |&gt;\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    1.37     0.957       1.43 1.56e- 1\n2 WingLength     0.467    0.0347     13.5  2.62e-25\n\n\n\n\\(1.37 + 0.467 \\times 30 = 15.38\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#linear-regression-accuracy",
    "href": "slides/03-linear-regression.html#linear-regression-accuracy",
    "title": "Chapter 3 - Linear Regression",
    "section": "Linear Regression Accuracy",
    "text": "Linear Regression Accuracy\n\nWhat is the residual sum of squares again?\n\n\nNote: In previous classes, this may have been referred to as SSE (sum of squares error), the book uses RSS, so we will stick with that!\n\n\n\\[RSS = \\sum(y_i - \\hat{y}_i)^2\\]"
  },
  {
    "objectID": "slides/03-linear-regression.html#linear-regression-accuracy-1",
    "href": "slides/03-linear-regression.html#linear-regression-accuracy-1",
    "title": "Chapter 3 - Linear Regression",
    "section": "Linear Regression Accuracy",
    "text": "Linear Regression Accuracy\n\n\nThe total sum of squares represents the variability of the outcome, it is equivalent to the variability described by the model plus the remaining residual sum of squares\n\n\\[TSS = \\sum(y_i - \\bar{y})^2\\]"
  },
  {
    "objectID": "slides/03-linear-regression.html#linear-regression-accuracy-2",
    "href": "slides/03-linear-regression.html#linear-regression-accuracy-2",
    "title": "Chapter 3 - Linear Regression",
    "section": "Linear Regression Accuracy",
    "text": "Linear Regression Accuracy\n\nThere are many ways “model fit” can be assessed. Two common ones are:\n\nResidual Standard Error (RSE)\n\\(R^2\\) - the fraction of the variance explained\n\n\\(RSE = \\sqrt{\\frac{1}{n-p-1}RSS}\\)\n\\(R^2 = 1 - \\frac{RSS}{TSS}\\)"
  },
  {
    "objectID": "slides/03-linear-regression.html#lets-do-it-in-r-4",
    "href": "slides/03-linear-regression.html#lets-do-it-in-r-4",
    "title": "Chapter 3 - Linear Regression",
    "section": "Let’s do it in R!",
    "text": "Let’s do it in R!\n\nlm_fit &lt;- linear_reg() |&gt; \n  set_engine(\"lm\") |&gt;\n  fit(Weight ~ WingLength, data = Sparrows)\n\nlm_fit |&gt;\n  predict(new_data = Sparrows) |&gt;\n  bind_cols(Sparrows) |&gt;\n  rsq(truth = Weight, estimate = .pred) \n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.614\n\n\n\n\nIs this testing \\(R^2\\) or training \\(R^2\\)?"
  },
  {
    "objectID": "slides/03-linear-regression.html#application-exercise-3",
    "href": "slides/03-linear-regression.html#application-exercise-3",
    "title": "Chapter 3 - Linear Regression",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nFit a linear model using the mtcars data frame predicting miles per gallon (mpg) from weight and horsepower (wt and hp), using polynomials with 4 degrees of freedom for both.\nEstimate the training \\(R^2\\) using the rsq function.\nInterpret this values."
  },
  {
    "objectID": "slides/03-linear-regression.html#application-exercise-4",
    "href": "slides/03-linear-regression.html#application-exercise-4",
    "title": "Chapter 3 - Linear Regression",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nCreate a cross validation object to do 5 fold cross validation using the mtcars data\nRefit the model on this object (using fit_resamples)\nUse collect_metrics to estimate the test \\(R^2\\) - how does this compare to the training \\(R^2\\) calculated in the previous exercise?"
  },
  {
    "objectID": "slides/03-linear-regression.html#additional-linear-regression-topics",
    "href": "slides/03-linear-regression.html#additional-linear-regression-topics",
    "title": "Chapter 3 - Linear Regression",
    "section": "Additional Linear Regression Topics",
    "text": "Additional Linear Regression Topics\n\nPolynomial terms\nInteractions\nOutliers\nNon-constant variance of error terms\nHigh leverage points\nCollinearity\n\nRefer to Chapter 3 for more details on these topics if you need a refresher.\n\n\n\n\n🔗 https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#setup",
    "href": "slides/05-cv-tidymodels.html#setup",
    "title": "Chapter 5 and tidymodels",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(ISLR)\nlibrary(tidymodels)\nlibrary(gridExtra)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#cross-validation",
    "href": "slides/05-cv-tidymodels.html#cross-validation",
    "title": "Chapter 5 and tidymodels",
    "section": "Cross validation",
    "text": "Cross validation\n💡 Big idea\n\n\nWe have determined that it is sensible to use a test set to calculate metrics like prediction error"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#cross-validation-1",
    "href": "slides/05-cv-tidymodels.html#cross-validation-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Cross validation",
    "text": "Cross validation\n💡 Big idea\n\n\nWe have determined that it is sensible to use a test set to calculate metrics like prediction error\n\n\n\nWhy?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#cross-validation-2",
    "href": "slides/05-cv-tidymodels.html#cross-validation-2",
    "title": "Chapter 5 and tidymodels",
    "section": "Cross validation",
    "text": "Cross validation\n💡 Big idea\n\n\nWe have determined that it is sensible to use a test set to calculate metrics like prediction error\n\n\n\nHow could we do this?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#cross-validation-3",
    "href": "slides/05-cv-tidymodels.html#cross-validation-3",
    "title": "Chapter 5 and tidymodels",
    "section": "Cross validation",
    "text": "Cross validation\n💡 Big idea\n\n\nWe have determined that it is sensible to use a test set to calculate metrics like prediction error\nWhat if we don’t have a separate data set to test our model on?\n🎉 We can use resampling methods to estimate the test-set prediction error"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#training-error-versus-test-error",
    "href": "slides/05-cv-tidymodels.html#training-error-versus-test-error",
    "title": "Chapter 5 and tidymodels",
    "section": "Training error versus test error",
    "text": "Training error versus test error\n\nWhat is the difference? Which is typically larger?\n\n\nThe training error is calculated by using the same observations used to fit the statistical learning model\nThe test error is calculated by using a statistical learning method to predict the response of new observations\nThe training error rate typically underestimates the true prediction error rate"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#estimating-prediction-error",
    "href": "slides/05-cv-tidymodels.html#estimating-prediction-error",
    "title": "Chapter 5 and tidymodels",
    "section": "Estimating prediction error",
    "text": "Estimating prediction error\n\nBest case scenario: We have a large data set to test our model on\nThis is not always the case!\n\n\n💡 Let’s instead find a way to estimate the test error by holding out a subset of the training observations from the model fitting process, and then applying the statistical learning method to those held out observations"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set",
    "text": "Approach #1: Validation set\n\nRandomly divide the available set up samples into two parts: a training set and a validation set\nFit the model on the training set, calculate the prediction error on the validation set\n\n\n\nIf we have a quantitative predictor what metric would we use to calculate this test error?\n\n\nOften we use Mean Squared Error (MSE)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set-1",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set",
    "text": "Approach #1: Validation set\n\n\nRandomly divide the available set up samples into two parts: a training set and a validation set\nFit the model on the training set, calculate the prediction error on the validation set\n\n\n\nIf we have a qualitative predictor what metric would we use to calculate this test error?\n\n\nOften we use misclassification rate"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set-2",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set-2",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set",
    "text": "Approach #1: Validation set\n\n\n\\[\\Large\\color{orange}{MSE_{\\texttt{test-split}} = \\textrm{Ave}_{i\\in\\texttt{test-split}}[y_i-\\hat{f}(x_i)]^2}\\]\n\n\n\\[\\Large\\color{orange}{Err_{\\texttt{test-split}} = \\textrm{Ave}_{i\\in\\texttt{test-split}}I[y_i\\neq \\mathcal{\\hat{C}}(x_i)]}\\]"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set-3",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set-3",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set",
    "text": "Approach #1: Validation set\nAuto example:\n\n\nWe have 392 observations.\nTrying to predict mpg from horsepower.\nWe can split the data in half and use 196 to fit the model and 196 to test"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set-4",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set-4",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set",
    "text": "Approach #1: Validation set\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split}}}\\)\n\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split}}}\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split}}}\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split}}}\\)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set-5",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set-5",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set",
    "text": "Approach #1: Validation set\nAuto example:\n\n\nWe have 392 observations.\nTrying to predict mpg from horsepower.\nWe can split the data in half and use 196 to fit the model and 196 to test - what if we did this many times?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-1-validation-set-drawbacks",
    "href": "slides/05-cv-tidymodels.html#approach-1-validation-set-drawbacks",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #1: Validation set (Drawbacks)",
    "text": "Approach #1: Validation set (Drawbacks)\n\nthe validation estimate of the test error can be highly variable, depending on which observations are included in the training set and which observations are included in the validation set\nIn the validation approach, only a subset of the observations (those that are included in the training set rather than in the validation set) are used to fit the model\nTherefore, the validation set error may tend to overestimate the test error for the model fit on the entire data set"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-2-k-fold-cross-validation",
    "href": "slides/05-cv-tidymodels.html#approach-2-k-fold-cross-validation",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #2: K-fold cross validation",
    "text": "Approach #2: K-fold cross validation\n💡 The idea is to do the following:\n\nRandomly divide the data into \\(K\\) equal-sized parts\nLeave out part \\(k\\), fit the model to the other \\(K - 1\\) parts (combined)\nObtain predictions for the left-out \\(k\\)th part\nDo this for each part \\(k = 1, 2,\\dots K\\), and then combine the result"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#k-fold-cross-validation",
    "href": "slides/05-cv-tidymodels.html#k-fold-cross-validation",
    "title": "Chapter 5 and tidymodels",
    "section": "K-fold cross validation",
    "text": "K-fold cross validation\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split-1}}}\\)\n\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split-2}}}\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split-3}}}\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(\\color{orange}{MSE_{\\texttt{test-split-4}}}\\)\nTake the mean of the \\(k\\) MSE values"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise",
    "href": "slides/05-cv-tidymodels.html#application-exercise",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\nCreate a new R project, then a new quarto file with cv in its name in that project. Answer the questions in that file.\nIf we use 10 folds:\n\n\nWhat percentage of the training data is used in each analysis for each fold?\nWhat percentage of the training data is used in the assessment for each fold?\n\n\n\ncountdown::countdown(2)\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#estimating-prediction-error-quantitative-outcome",
    "href": "slides/05-cv-tidymodels.html#estimating-prediction-error-quantitative-outcome",
    "title": "Chapter 5 and tidymodels",
    "section": "Estimating prediction error (quantitative outcome)",
    "text": "Estimating prediction error (quantitative outcome)\n\nSplit the data into K parts, where \\(C_1, C_2, \\dots, C_k\\) indicate the indices of observations in part \\(k\\)\n\\(CV_{(K)} = \\sum_{k=1}^K\\frac{n_k}{n}MSE_k\\)\n\\(MSE_k = \\sum_{i \\in C_k} (y_i - \\hat{y}_i)^2/n_k\\)\n\\(n_k\\) is the number of observations in group \\(k\\)\n\\(\\hat{y}_i\\) is the fit for observation \\(i\\) obtained from the data with the part \\(k\\) removed\nIf we set \\(K = n\\), we’d have \\(n-fold\\) cross validation which is the same as leave-one-out cross validation (LOOCV)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#estimating-prediction-error-quantitative-outcome-1",
    "href": "slides/05-cv-tidymodels.html#estimating-prediction-error-quantitative-outcome-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Estimating prediction error (quantitative outcome)",
    "text": "Estimating prediction error (quantitative outcome)\n\n\nSplit the data into K parts, where \\(C_1, C_2, \\dots, C_k\\) indicate the indices of observations in part \\(k\\)\n\\(CV_{(K)} = \\sum_{k=1}^K\\frac{n_k}{n}MSE_k\\)\n\\(MSE_k = \\sum_{i \\in C_k} (y_i - \\hat{y}_i)^2/n_k\\)\n\\(n_k\\) is the number of observations in group \\(k\\)\n\\(\\hat{y}_i\\) is the fit for observation \\(i\\) obtained from the data with the part \\(k\\) removed\nIf we set \\(K = n\\), we’d have \\(n-fold\\) cross validation which is the same as leave-one-out cross validation (LOOCV)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#leave-one-out-cross-validation",
    "href": "slides/05-cv-tidymodels.html#leave-one-out-cross-validation",
    "title": "Chapter 5 and tidymodels",
    "section": "Leave-one-out cross validation",
    "text": "Leave-one-out cross validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\dots\\)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#special-case",
    "href": "slides/05-cv-tidymodels.html#special-case",
    "title": "Chapter 5 and tidymodels",
    "section": "Special Case!",
    "text": "Special Case!\n\nWith linear regression, you can actually calculate the LOOCV error without having to iterate!\n\\(CV_{(n)} = \\frac{1}{n}\\sum_{i=1}^n\\left(\\frac{y_i-\\hat{y}_i}{1-h_i}\\right)^2\\)\n\\(\\hat{y}_i\\) is the \\(i\\)th fitted value from the linear model\n\\(h_i\\) is the diagonal of the “hat” matrix (remember that! 🎓)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#picking-k",
    "href": "slides/05-cv-tidymodels.html#picking-k",
    "title": "Chapter 5 and tidymodels",
    "section": "Picking \\(K\\)",
    "text": "Picking \\(K\\)\n\n\\(K\\) can vary from 2 (splitting the data in half each time) to \\(n\\) (LOOCV)\nLOOCV is sometimes useful but usually the estimates from each fold are very correlated, so their average can have a high variance\nA better choice tends to be \\(K=5\\) or \\(K=10\\)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#bias-variance-trade-off",
    "href": "slides/05-cv-tidymodels.html#bias-variance-trade-off",
    "title": "Chapter 5 and tidymodels",
    "section": "Bias variance trade-off",
    "text": "Bias variance trade-off\n\nSince each training set is only \\((K - 1)/K\\) as big as the original training set, the estimates of prediction error will typically be biased upward\nThis bias is minimized when \\(K = n\\) (LOOCV), but this estimate has a high variance\n\\(K =5\\) or \\(K=10\\) provides a nice compromise for the bias-variance trade-off"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#approach-2-k-fold-cross-validation-1",
    "href": "slides/05-cv-tidymodels.html#approach-2-k-fold-cross-validation-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Approach #2: K-fold Cross Validation",
    "text": "Approach #2: K-fold Cross Validation\nAuto example:\n\n\nWe have 392 observations.\nTrying to predict mpg from horsepower"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#estimating-prediction-error-qualitative-outcome",
    "href": "slides/05-cv-tidymodels.html#estimating-prediction-error-qualitative-outcome",
    "title": "Chapter 5 and tidymodels",
    "section": "Estimating prediction error (qualitative outcome)",
    "text": "Estimating prediction error (qualitative outcome)\n\nThe premise is the same as cross valiation for quantitative outcomes\nSplit the data into K parts, where \\(C_1, C_2, \\dots, C_k\\) indicate the indices of observations in part \\(k\\)\n\\(CV_K = \\sum_{k=1}^K\\frac{n_k}{n}Err_k\\)\n\\(Err_k = \\sum_{i\\in C_k}I(y_i\\neq\\hat{y}_i)/n_k\\) (misclassification rate)\n\\(n_k\\) is the number of observations in group \\(k\\)\n\\(\\hat{y}_i\\) is the fit for observation \\(i\\) obtained from the data with the part \\(k\\) removed"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#estimating-prediction-error-qualitative-outcome-1",
    "href": "slides/05-cv-tidymodels.html#estimating-prediction-error-qualitative-outcome-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Estimating prediction error (qualitative outcome)",
    "text": "Estimating prediction error (qualitative outcome)\n\n\nThe premise is the same as cross valiation for quantitative outcomes\nSplit the data into K parts, where \\(C_1, C_2, \\dots, C_k\\) indicate the indices of observations in part \\(k\\)\n\\(CV_K = \\sum_{k=1}^K\\frac{n_k}{n}Err_k\\)\n\\(Err_k = \\sum_{i\\in C_k}I(y_i\\neq\\hat{y}_i)/n_k\\) (misclassification rate)\n\\(n_k\\) is the number of observations in group \\(k\\)\n\\(\\hat{y}_i\\) is the fit for observation \\(i\\) obtained from the data with the part \\(k\\) removed"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise-1",
    "href": "slides/05-cv-tidymodels.html#application-exercise-1",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nCreate a new quarto file in your project and add tidymodels in the name.\nLoad the packages by running the top chunk of R code\n\n\n\nlibrary(tidymodels)\nlibrary(broom)\nlibrary(ISLR)\nlibrary(countdown)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#tidymodels-1",
    "href": "slides/05-cv-tidymodels.html#tidymodels-1",
    "title": "Chapter 5 and tidymodels",
    "section": "tidymodels",
    "text": "tidymodels\n\n\n\n\ntidymodels.org\n\ntidymodels is an opinionated collection of R packages designed for modeling and statistical analysis.\nAll packages share an underlying philosophy and a common grammar."
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#step-1-specify-the-model",
    "href": "slides/05-cv-tidymodels.html#step-1-specify-the-model",
    "title": "Chapter 5 and tidymodels",
    "section": "Step 1: Specify the model",
    "text": "Step 1: Specify the model\n\nPick the model\nSet the engine"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#specify-the-model",
    "href": "slides/05-cv-tidymodels.html#specify-the-model",
    "title": "Chapter 5 and tidymodels",
    "section": "Specify the model",
    "text": "Specify the model\n\nlinear_reg() |&gt;\n  set_engine(\"lm\")"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#specify-the-model-1",
    "href": "slides/05-cv-tidymodels.html#specify-the-model-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Specify the model",
    "text": "Specify the model\n\nlinear_reg() |&gt;\n  set_engine(\"glmnet\")"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#specify-the-model-2",
    "href": "slides/05-cv-tidymodels.html#specify-the-model-2",
    "title": "Chapter 5 and tidymodels",
    "section": "Specify the model",
    "text": "Specify the model\n\nlinear_reg() |&gt;\n  set_engine(\"spark\")"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#specify-the-model-3",
    "href": "slides/05-cv-tidymodels.html#specify-the-model-3",
    "title": "Chapter 5 and tidymodels",
    "section": "Specify the model",
    "text": "Specify the model\n\ndecision_tree() |&gt;\n  set_engine(\"rpart\")"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#specify-the-model-4",
    "href": "slides/05-cv-tidymodels.html#specify-the-model-4",
    "title": "Chapter 5 and tidymodels",
    "section": "Specify the model",
    "text": "Specify the model\n\n\nAll available models:\n\ntidymodels.org"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise-2",
    "href": "slides/05-cv-tidymodels.html#application-exercise-2",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nWrite a pipe that creates a model that uses lm() to fit a linear regression using tidymodels. Save it as lm_spec and look at the object. What does it return?\n\n\nHint: you’ll need https://www.tidymodels.org\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#answer",
    "href": "slides/05-cv-tidymodels.html#answer",
    "title": "Chapter 5 and tidymodels",
    "section": "Answer",
    "text": "Answer\n\nlm_spec &lt;- \n  linear_reg() |&gt; # Pick linear regression\n  set_engine(engine = \"lm\") # set engine\nlm_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#fit-the-data",
    "href": "slides/05-cv-tidymodels.html#fit-the-data",
    "title": "Chapter 5 and tidymodels",
    "section": "Fit the data",
    "text": "Fit the data\n\n\nYou can train your model using the fit() function\n\n\nfit(lm_spec,\n    mpg ~ horsepower,\n    data = Auto)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = mpg ~ horsepower, data = data)\n\nCoefficients:\n(Intercept)   horsepower  \n    39.9359      -0.1578"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise-3",
    "href": "slides/05-cv-tidymodels.html#application-exercise-3",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nFit the model:\n\n\nlibrary(ISLR)\nlm_fit &lt;- fit(lm_spec,\n              mpg ~ horsepower,\n              data = Auto)\nlm_fit\n\nDoes this give the same results as\n\nlm(mpg ~ horsepower, data = Auto)\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#answer-1",
    "href": "slides/05-cv-tidymodels.html#answer-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Answer",
    "text": "Answer\n\nlm_fit &lt;- fit(lm_spec,\n              mpg ~ horsepower,\n              data = Auto)\nlm_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = mpg ~ horsepower, data = data)\n\nCoefficients:\n(Intercept)   horsepower  \n    39.9359      -0.1578  \n\nlm(mpg ~ horsepower, data = Auto)\n\n\nCall:\nlm(formula = mpg ~ horsepower, data = Auto)\n\nCoefficients:\n(Intercept)   horsepower  \n    39.9359      -0.1578"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#get-predictions",
    "href": "slides/05-cv-tidymodels.html#get-predictions",
    "title": "Chapter 5 and tidymodels",
    "section": "Get predictions",
    "text": "Get predictions\n\nlm_fit |&gt;\n  predict(new_data = Auto)\n\n\nUses the predict() function\n‼️ new_data has an underscore\n😄 This automagically creates a data frame"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#get-predictions-1",
    "href": "slides/05-cv-tidymodels.html#get-predictions-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Get predictions",
    "text": "Get predictions\n\nlm_fit |&gt;\n  predict(new_data = Auto) |&gt;\n  bind_cols(Auto)\n\n# A tibble: 392 × 10\n   .pred   mpg cylinders displacement horsepower weight acceleration  year origin name    \n * &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;   \n 1 19.4     18         8          307        130   3504         12      70      1 chevrol…\n 2 13.9     15         8          350        165   3693         11.5    70      1 buick s…\n 3 16.3     18         8          318        150   3436         11      70      1 plymout…\n 4 16.3     16         8          304        150   3433         12      70      1 amc reb…\n 5 17.8     17         8          302        140   3449         10.5    70      1 ford to…\n 6  8.68    15         8          429        198   4341         10      70      1 ford ga…\n 7  5.21    14         8          454        220   4354          9      70      1 chevrol…\n 8  6.00    14         8          440        215   4312          8.5    70      1 plymout…\n 9  4.42    14         8          455        225   4425         10      70      1 pontiac…\n10  9.95    15         8          390        190   3850          8.5    70      1 amc amb…\n# ℹ 382 more rows\n\n\n\n\nWhat does bind_cols do?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#get-predictions-2",
    "href": "slides/05-cv-tidymodels.html#get-predictions-2",
    "title": "Chapter 5 and tidymodels",
    "section": "Get predictions",
    "text": "Get predictions\n\nlm_fit |&gt;\n  predict(new_data = Auto) |&gt;\n  bind_cols(Auto)\n\n# A tibble: 392 × 10\n   .pred   mpg cylinders displacement horsepower weight acceleration  year origin name    \n * &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;   \n 1 19.4     18         8          307        130   3504         12      70      1 chevrol…\n 2 13.9     15         8          350        165   3693         11.5    70      1 buick s…\n 3 16.3     18         8          318        150   3436         11      70      1 plymout…\n 4 16.3     16         8          304        150   3433         12      70      1 amc reb…\n 5 17.8     17         8          302        140   3449         10.5    70      1 ford to…\n 6  8.68    15         8          429        198   4341         10      70      1 ford ga…\n 7  5.21    14         8          454        220   4354          9      70      1 chevrol…\n 8  6.00    14         8          440        215   4312          8.5    70      1 plymout…\n 9  4.42    14         8          455        225   4425         10      70      1 pontiac…\n10  9.95    15         8          390        190   3850          8.5    70      1 amc amb…\n# ℹ 382 more rows\n\n\n\nWhich column has the predicted values?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise-4",
    "href": "slides/05-cv-tidymodels.html#application-exercise-4",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\n\n−+\n03:00\n\n\n\n\n\nEdit the code below to add the original data to the predicted data.\n\n\n\nmpg_pred &lt;- lm_fit |&gt; \n  predict(new_data = Auto) |&gt; \n  ---"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#get-predictions-3",
    "href": "slides/05-cv-tidymodels.html#get-predictions-3",
    "title": "Chapter 5 and tidymodels",
    "section": "Get predictions",
    "text": "Get predictions\n\nmpg_pred &lt;- lm_fit |&gt;\n  predict(new_data = Auto) |&gt;\n  bind_cols(Auto)\n\nmpg_pred\n\n# A tibble: 392 × 10\n   .pred   mpg cylinders displacement horsepower weight acceleration  year origin name    \n * &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;   \n 1 19.4     18         8          307        130   3504         12      70      1 chevrol…\n 2 13.9     15         8          350        165   3693         11.5    70      1 buick s…\n 3 16.3     18         8          318        150   3436         11      70      1 plymout…\n 4 16.3     16         8          304        150   3433         12      70      1 amc reb…\n 5 17.8     17         8          302        140   3449         10.5    70      1 ford to…\n 6  8.68    15         8          429        198   4341         10      70      1 ford ga…\n 7  5.21    14         8          454        220   4354          9      70      1 chevrol…\n 8  6.00    14         8          440        215   4312          8.5    70      1 plymout…\n 9  4.42    14         8          455        225   4425         10      70      1 pontiac…\n10  9.95    15         8          390        190   3850          8.5    70      1 amc amb…\n# ℹ 382 more rows"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#calculate-the-error",
    "href": "slides/05-cv-tidymodels.html#calculate-the-error",
    "title": "Chapter 5 and tidymodels",
    "section": "Calculate the error",
    "text": "Calculate the error\n\n\nRoot mean square error\n\n\n\nmpg_pred |&gt;\n  rmse(truth = mpg, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        4.89\n\n\n\n\nWhat is this estimate? (training error? testing error?)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#validation-set-approach",
    "href": "slides/05-cv-tidymodels.html#validation-set-approach",
    "title": "Chapter 5 and tidymodels",
    "section": "Validation set approach",
    "text": "Validation set approach\n\nAuto_split &lt;- initial_split(Auto, prop = 0.5)\nAuto_split\n\n&lt;Training/Testing/Total&gt;\n&lt;196/196/392&gt;\n\n\n\n\nHow many observations are in the training set?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#validation-set-approach-1",
    "href": "slides/05-cv-tidymodels.html#validation-set-approach-1",
    "title": "Chapter 5 and tidymodels",
    "section": "Validation set approach",
    "text": "Validation set approach\n\nAuto_split &lt;- initial_split(Auto, prop = 0.5)\nAuto_split\n\n&lt;Training/Testing/Total&gt;\n&lt;196/196/392&gt;\n\n\n\nHow many observations are in the test set?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#validation-set-approach-2",
    "href": "slides/05-cv-tidymodels.html#validation-set-approach-2",
    "title": "Chapter 5 and tidymodels",
    "section": "Validation set approach",
    "text": "Validation set approach\n\nAuto_split &lt;- initial_split(Auto, prop = 0.5)\nAuto_split\n\n&lt;Training/Testing/Total&gt;\n&lt;196/196/392&gt;\n\n\n\nHow many observations are there in total?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#validation-set-approach-3",
    "href": "slides/05-cv-tidymodels.html#validation-set-approach-3",
    "title": "Chapter 5 and tidymodels",
    "section": "Validation set approach",
    "text": "Validation set approach\n\nAuto_split &lt;- initial_split(Auto, prop = 0.5)\nAuto_split\n\n&lt;Training/Testing/Total&gt;\n&lt;196/196/392&gt;\n\n\n\n\nExtract the training and testing data\n\n\n\ntraining(Auto_split)\ntesting(Auto_split)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#validation-set-approach-4",
    "href": "slides/05-cv-tidymodels.html#validation-set-approach-4",
    "title": "Chapter 5 and tidymodels",
    "section": "Validation set approach",
    "text": "Validation set approach\n\nAuto_train &lt;- training(Auto_split)\n\n\nAuto_train\n\n\n\n# A tibble: 196 × 9\n     mpg cylinders displacement horsepower weight acceleration  year origin name          \n   &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;         \n 1  37.7         4           89         62   2050         17.3    81      3 toyota tercel \n 2  27           4           97         60   1834         19      71      2 volkswagen mo…\n 3  22           6          232        112   2835         14.7    82      1 ford granada l\n 4  16           6          250        100   3781         17      74      1 chevrolet che…\n 5  25           4           90         71   2223         16.5    75      2 volkswagen da…\n 6  18           6          232        100   2945         16      73      1 amc hornet    \n 7  38.1         4           89         60   1968         18.8    80      3 toyota coroll…\n 8  23           4           97         54   2254         23.5    72      2 volkswagen ty…\n 9  15           8          302        130   4295         14.9    77      1 mercury couga…\n10  34           4          108         70   2245         16.9    82      3 toyota corolla\n# ℹ 186 more rows"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise-5",
    "href": "slides/05-cv-tidymodels.html#application-exercise-5",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nCopy the code below, fill in the blanks to fit a model on the training data then calculate the test RMSE.\n\n\nset.seed(100)\nAuto_split  &lt;- ________\nAuto_train  &lt;- ________\nAuto_test   &lt;- ________\nlm_fit      &lt;- fit(lm_spec, \n                   mpg ~ horsepower, \n                   data = ________)\nmpg_pred  &lt;- ________ |&gt; \n  predict(new_data = ________) |&gt; \n  bind_cols(________)\nrmse(________, truth = ________, estimate = ________)\n\n\n\n\n−+\n06:00"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#a-faster-way",
    "href": "slides/05-cv-tidymodels.html#a-faster-way",
    "title": "Chapter 5 and tidymodels",
    "section": "A faster way!",
    "text": "A faster way!\n\nYou can use last_fit() and specify the split\nThis will automatically train the data on the train data from the split\nInstead of specifying which metric to calculate (with rmse as before) you can just use collect_metrics() and it will automatically calculate the metrics on the test data from the split"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#a-faster-way-1",
    "href": "slides/05-cv-tidymodels.html#a-faster-way-1",
    "title": "Chapter 5 and tidymodels",
    "section": "A faster way!",
    "text": "A faster way!\n\nset.seed(100)\n\nAuto_split &lt;- initial_split(Auto, prop = 0.5)\nlm_fit &lt;- last_fit(lm_spec,\n                   mpg ~ horsepower,\n                   split = Auto_split) \n\nlm_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       4.96  Preprocessor1_Model1\n2 rsq     standard       0.613 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#a-faster-way-2",
    "href": "slides/05-cv-tidymodels.html#a-faster-way-2",
    "title": "Chapter 5 and tidymodels",
    "section": "A faster way!",
    "text": "A faster way!\n\nset.seed(100)\n\nAuto_split &lt;- initial_split(Auto, prop = 0.5)\nlm_fit &lt;- last_fit(lm_spec,\n                   mpg ~ horsepower,\n                   split = Auto_split) \n\nlm_fit |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       4.96  Preprocessor1_Model1\n2 rsq     standard       0.613 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#what-about-cross-validation",
    "href": "slides/05-cv-tidymodels.html#what-about-cross-validation",
    "title": "Chapter 5 and tidymodels",
    "section": "What about cross validation?",
    "text": "What about cross validation?\n\nAuto_cv &lt;- vfold_cv(Auto, v = 5)\nAuto_cv\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits           id   \n  &lt;list&gt;           &lt;chr&gt;\n1 &lt;split [313/79]&gt; Fold1\n2 &lt;split [313/79]&gt; Fold2\n3 &lt;split [314/78]&gt; Fold3\n4 &lt;split [314/78]&gt; Fold4\n5 &lt;split [314/78]&gt; Fold5"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#what-about-cross-validation-1",
    "href": "slides/05-cv-tidymodels.html#what-about-cross-validation-1",
    "title": "Chapter 5 and tidymodels",
    "section": "What about cross validation?",
    "text": "What about cross validation?\n\nInstead of fit we will use fit_resamples\n\n\n\nfit_resamples(lm_spec, \n              mpg ~ horsepower,\n              resamples = Auto_cv)"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#what-about-cross-validation-2",
    "href": "slides/05-cv-tidymodels.html#what-about-cross-validation-2",
    "title": "Chapter 5 and tidymodels",
    "section": "What about cross validation?",
    "text": "What about cross validation?\n\nHow do we get the metrics out? With collect_metrics() again!\n\n\n\nresults &lt;- fit_resamples(lm_spec,\n                         mpg ~ horsepower,\n                         resamples = Auto_cv)\n\nresults |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4.88      5  0.385  Preprocessor1_Model1\n2 rsq     standard   0.616     5  0.0220 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise-6",
    "href": "slides/05-cv-tidymodels.html#application-exercise-6",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\n\n\n−+\n05:00\n\n\n\n\nEdit the code below to get the 5-fold cross validation error rate for the following model:\n\n\\(mpg = \\beta_0 + \\beta_1 horsepower + \\beta_2 horsepower^2+ \\epsilon\\)\n\nAuto_cv &lt;- vfold_cv(Auto, v = 5)\n\nresults &lt;- fit_resamples(lm_spec,\n                         ----,\n                         resamples = ---)\n\nresults |&gt;\n  collect_metrics()\n\n\nWhat do you think rsq is?"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#answer-2",
    "href": "slides/05-cv-tidymodels.html#answer-2",
    "title": "Chapter 5 and tidymodels",
    "section": "Answer",
    "text": "Answer\n\nAuto_cv &lt;- vfold_cv(Auto, v = 5)\n\nresults &lt;- fit_resamples(lm_spec,\n                         mpg ~ horsepower + I(horsepower^2),\n                         resamples = Auto_cv)\n\nresults |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4.38      5  0.110  Preprocessor1_Model1\n2 rsq     standard   0.688     5  0.0177 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/05-cv-tidymodels.html#application-exercise-7",
    "href": "slides/05-cv-tidymodels.html#application-exercise-7",
    "title": "Chapter 5 and tidymodels",
    "section": " Application Exercise",
    "text": "Application Exercise\n\n\nFit 3 models on the data using 5 fold cross validation:\n\n\\(mpg = \\beta_0 + \\beta_1 horsepower + \\epsilon\\)\n\\(mpg = \\beta_0 + \\beta_1 horsepower + \\beta_2 horsepower^2+ \\epsilon\\)\n\\(mpg = \\beta_0 + \\beta_1 horsepower + \\beta_2 horsepower^2+ \\beta_3 horsepower^3 +\\epsilon\\)\n\nCollect the metrics from each model, saving the results as results_1, results_2, results_3\nWhich model is “best”?\n\n\n\n\n\n−+\n08:00\n\n\n\n\n\n\n\n🔗 https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "slides/04_2-lda_qda.html#recap",
    "href": "slides/04_2-lda_qda.html#recap",
    "title": "Chapter 4 Part 2",
    "section": "Recap",
    "text": "Recap\n\nWe had a logistic regression refresher\n\nNow…\n\nWhat if our response has more than two levels?\nWhat if logistic regression is a poor fit?"
  },
  {
    "objectID": "slides/04_2-lda_qda.html#setup",
    "href": "slides/04_2-lda_qda.html#setup",
    "title": "Chapter 4 Part 2",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR)"
  },
  {
    "objectID": "slides/04_2-lda_qda.html#multinomial-logistic",
    "href": "slides/04_2-lda_qda.html#multinomial-logistic",
    "title": "Chapter 4 Part 2",
    "section": "Multinomial Logistic",
    "text": "Multinomial Logistic\n\nSo far we have discussed logistic regression with two classes.\nIt is easily generalized to more than two classes."
  },
  {
    "objectID": "slides/04_2-lda_qda.html#confounding",
    "href": "slides/04_2-lda_qda.html#confounding",
    "title": "Chapter 4 Part 2",
    "section": "Confounding",
    "text": "Confounding\nRecall our defaults data with variable default, student, and balance\n\n\nWhat is going on here?"
  },
  {
    "objectID": "slides/04_2-lda_qda.html#confounding-1",
    "href": "slides/04_2-lda_qda.html#confounding-1",
    "title": "Chapter 4 Part 2",
    "section": "Confounding",
    "text": "Confounding\n\n\nStudents tend to have higher balances than non-students\nTheir marginal default rate is higher\nFor each level of balance, students default less\nTheir conditional default rate is lower"
  },
  {
    "objectID": "slides/04_2-lda_qda.html#logistic-regression-for-more-than-two-classes",
    "href": "slides/04_2-lda_qda.html#logistic-regression-for-more-than-two-classes",
    "title": "Chapter 4 Part 2",
    "section": "Logistic regression for more than two classes",
    "text": "Logistic regression for more than two classes\n\\[P(Y=k|X) = \\frac{e ^{\\beta_{0k}+\\beta_{1k}X_1+\\dots+\\beta_{pk}X_p}}{\\sum_{l=1}^Ke^{\\beta_{0l}+\\beta_{1l}X_1+\\dots+\\beta_{pl}X_p}}\\]\n\nWe generalize this to situations with multiple classes\nHere we have a linear function for each of the \\(K\\) classes\nThis is known as multinomial logistic regression"
  },
  {
    "objectID": "slides/04_2-lda_qda.html#multiple-logistic-regression",
    "href": "slides/04_2-lda_qda.html#multiple-logistic-regression",
    "title": "Chapter 4 Part 2",
    "section": "Multiple logistic regression",
    "text": "Multiple logistic regression\n\\[\\log\\left(\\frac{p(X)}{1-p(X)}\\right)=\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p\\] \\[p(X) = \\frac{e^{\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p}}{1+e^{\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p}}\\]\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.8690452\n0.4922555\n-22.080088\n0.0000000\n\n\nbalance\n0.0057365\n0.0002319\n24.737563\n0.0000000\n\n\nincome\n0.0000030\n0.0000082\n0.369815\n0.7115203\n\n\nstudentYes\n-0.6467758\n0.2362525\n-2.737646\n0.0061881\n\n\n\n\n\n\n\n\n\nWhy is the coefficient for student negative now when it was positive before?"
  },
  {
    "objectID": "slides/04_2-lda_qda.html#the-unforuntate-truth-logistic-regression",
    "href": "slides/04_2-lda_qda.html#the-unforuntate-truth-logistic-regression",
    "title": "Chapter 4 Part 2",
    "section": "The Unforuntate truth logistic regression",
    "text": "The Unforuntate truth logistic regression"
  },
  {
    "objectID": "slides/04_2-lda_qda.html#bayes-rule",
    "href": "slides/04_2-lda_qda.html#bayes-rule",
    "title": "Chapter 4 Part 2",
    "section": "Bayes Rule",
    "text": "Bayes Rule"
  },
  {
    "objectID": "slides/04_2-lda_qda.html#linear-descriminant-analysis",
    "href": "slides/04_2-lda_qda.html#linear-descriminant-analysis",
    "title": "Chapter 4 Part 2",
    "section": "Linear Descriminant Analysis",
    "text": "Linear Descriminant Analysis"
  },
  {
    "objectID": "slides/04-logistic.html#potential-confounding",
    "href": "slides/04-logistic.html#potential-confounding",
    "title": "Chapter 4 Part 1",
    "section": "Potential Confounding",
    "text": "Potential Confounding\n\n\nWhat is going on here?"
  },
  {
    "objectID": "slides/04_2-lda_qda.html",
    "href": "slides/04_2-lda_qda.html",
    "title": "Chapter 4 Part 2",
    "section": "",
    "text": "We had a logistic regression refresher\n\n\n\n\nWhat if our response has more than two levels?\nWhat if logistic regression is a poor fit?"
  },
  {
    "objectID": "slides/04_2-lda_qda.html#lda-warmup",
    "href": "slides/04_2-lda_qda.html#lda-warmup",
    "title": "Chapter 4 Part 2",
    "section": "LDA Warmup",
    "text": "LDA Warmup\nTo give us a general overview, we are going to watch the StatQuest video on the topic: https://www.youtube.com/watch?v=azXCzI57Yfc"
  },
  {
    "objectID": "slides/04_2-lda_qda.html#discriminant-analysis",
    "href": "slides/04_2-lda_qda.html#discriminant-analysis",
    "title": "Chapter 4 Part 2",
    "section": "Discriminant Analysis",
    "text": "Discriminant Analysis\n\nHere the approach is to model the distribution of X in each of the classes separately, and then use Bayes theorem to flip things around and obtain \\(P(Y jX)\\).\nWhen we use normal (Gaussian) distributions for each class, this leads to linear or quadratic discriminant analysis.\nHowever, this approach is quite general, and other distributions can be used as well. We will focus on normal distributions."
  },
  {
    "objectID": "slides/04_2-lda_qda.html#why",
    "href": "slides/04_2-lda_qda.html#why",
    "title": "Chapter 4 Part 2",
    "section": "Why?",
    "text": "Why?\n\nWhen the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.\nIf n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\nLinear discriminant analysis is popular when we have more than two response classes, because it also provides low-dimensional views of the data."
  },
  {
    "objectID": "slides/04_2-lda_qda.html#bayes-theorem-classification",
    "href": "slides/04_2-lda_qda.html#bayes-theorem-classification",
    "title": "Chapter 4 Part 2",
    "section": "Bayes Theorem (classification)",
    "text": "Bayes Theorem (classification)\nThomas Bayes was a famous mathematician whose name represents a big subfield of statistical and probabilistic modeling. Here we focus on a simple result, known as Bayes theorem:\n\\[P(Y=k|X=x) = \\frac{P(X=x|Y=k)\\cdot P(Y=k)}{P(X=x)}\\]"
  },
  {
    "objectID": "slides/04_2-lda_qda.html#bayes-for-discriminant-analysis",
    "href": "slides/04_2-lda_qda.html#bayes-for-discriminant-analysis",
    "title": "Chapter 4 Part 2",
    "section": "Bayes for Discriminant Analysis",
    "text": "Bayes for Discriminant Analysis\n\\[P(Y=k|X=x) = \\frac{\\pi_kf_k(x)}{\\sum_{l=1}^K\\pi_lf_l(x)}\\], where\n\n\\(f_k(x)=P(X=x|Y=k)\\) is the density for \\(X\\) in class \\(k\\). Here we use normal’s but they could be other distributions (such as \\(\\Chi^2\\))\n\\(\\pi_k = P(Y=k)\\) is the marginal or prior probability for class \\(k\\)."
  },
  {
    "objectID": "slides/04_2-lda_qda.html#classify-to-the-highest-density",
    "href": "slides/04_2-lda_qda.html#classify-to-the-highest-density",
    "title": "Chapter 4 Part 2",
    "section": "Classify to the highest density",
    "text": "Classify to the highest density\n\n\n\\[\\pi_1=.5, \\pi_2=.5\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe classify a new point according to which density is highest.\nWhen the priors are different, we take them into account as well, and compare \\(\\pi_kf_k(x)\\).\nOn the right, we favor the pink class - the decision boundary has shifted to the left."
  },
  {
    "objectID": "slides/04_2-lda_qda.html#linear-descriminant-analysis-when-p1",
    "href": "slides/04_2-lda_qda.html#linear-descriminant-analysis-when-p1",
    "title": "Chapter 4 Part 2",
    "section": "Linear Descriminant Analysis (when \\(p=1\\))",
    "text": "Linear Descriminant Analysis (when \\(p=1\\))\nThe Gaussian (normal) density has the form\n\\[f_k(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k}e^{-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma_k})^2}\\]\n\n\\(\\mu_k\\) is the mean, \\(\\sigma_k^2\\) the variance (in class \\(k\\))\nFor now, we assume \\(\\sigma_k=\\sigma\\) for all groups (we will need to check this with real data)"
  },
  {
    "objectID": "slides/04_2-lda_qda.html#linear-descriminant-analysis-when-p1-1",
    "href": "slides/04_2-lda_qda.html#linear-descriminant-analysis-when-p1-1",
    "title": "Chapter 4 Part 2",
    "section": "Linear Descriminant Analysis (when \\(p=1\\))",
    "text": "Linear Descriminant Analysis (when \\(p=1\\))\nWe plug this \\(f_k(x)\\) into Bayes formula and after some simplifying we get:\n\\[p_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma_k})^2}}{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma_k})^2}}\\]"
  },
  {
    "objectID": "slides/04_2-lda_qda.html#discriminant-function-when-p1",
    "href": "slides/04_2-lda_qda.html#discriminant-function-when-p1",
    "title": "Chapter 4 Part 2",
    "section": "Discriminant Function (when \\(p=1\\))",
    "text": "Discriminant Function (when \\(p=1\\))\nTo classify at the value X = x, we need to see which of the \\(p_k(x)\\) is largest. Taking logs, and discarding terms that do not depend on \\(k\\), we see that this is equivalent to assigning x to the class with the largest discriminant score:\n\\[\\delta_k(x) = x\\cdot \\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+log(\\pi_k)\\]\n\nImportantly, \\(\\delta_k(x)\\) is a linear function of \\(x\\).\nIf there are \\(K=2\\) classes and \\(\\pi_1=\\pi_2=.5\\), then the decision boundry is at\n\n\\[x=\\frac{\\mu_1+\\mu_2}{2}\\]"
  },
  {
    "objectID": "slides/04_2-lda_qda.html#example",
    "href": "slides/04_2-lda_qda.html#example",
    "title": "Chapter 4 Part 2",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\n\n\n\n\n\n\n\n🔗 https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#recap",
    "href": "slides/04-2-lda-qda.html#recap",
    "title": "Chapter 4 Part 2",
    "section": "Recap",
    "text": "Recap\n\nWe had a logistic regression refresher\nNow…\n\nWhat if our response has more than two levels?\nWhat if logistic regression is a poor fit?"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#setup",
    "href": "slides/04-2-lda-qda.html#setup",
    "title": "Chapter 4 Part 2",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(ISLR)\nlibrary(Stat2Data)\n#install.packages(\"discrim\")"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#multinomial-logistic",
    "href": "slides/04-2-lda-qda.html#multinomial-logistic",
    "title": "Chapter 4 Part 2",
    "section": "Multinomial Logistic",
    "text": "Multinomial Logistic\n\nSo far we have discussed logistic regression with two classes.\nIt is easily generalized to more than two classes."
  },
  {
    "objectID": "slides/04-2-lda-qda.html#confounding",
    "href": "slides/04-2-lda-qda.html#confounding",
    "title": "Chapter 4 Part 2",
    "section": "Confounding",
    "text": "Confounding\nRecall our defaults data with variable default, student, and balance\n\n\nWhat is going on here?"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#confounding-1",
    "href": "slides/04-2-lda-qda.html#confounding-1",
    "title": "Chapter 4 Part 2",
    "section": "Confounding",
    "text": "Confounding\n\n\nStudents tend to have higher balances than non-students\nTheir marginal default rate is higher\nFor each level of balance, students default less\nTheir conditional default rate is lower"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#logistic-regression-for-more-than-two-classes",
    "href": "slides/04-2-lda-qda.html#logistic-regression-for-more-than-two-classes",
    "title": "Chapter 4 Part 2",
    "section": "Logistic regression for more than two classes",
    "text": "Logistic regression for more than two classes\n\\[P(Y=k|X) = \\frac{e ^{\\beta_{0k}+\\beta_{1k}X_1+\\dots+\\beta_{pk}X_p}}{\\sum_{l=1}^Ke^{\\beta_{0l}+\\beta_{1l}X_1+\\dots+\\beta_{pl}X_p}}\\]\n\nWe generalize this to situations with multiple classes\nHere we have a linear function for each of the \\(K\\) classes\nThis is known as multinomial logistic regression"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#multiple-logistic-regression",
    "href": "slides/04-2-lda-qda.html#multiple-logistic-regression",
    "title": "Chapter 4 Part 2",
    "section": "Multiple logistic regression",
    "text": "Multiple logistic regression\n\\[\\log\\left(\\frac{p(X)}{1-p(X)}\\right)=\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p\\] \\[p(X) = \\frac{e^{\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p}}{1+e^{\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p}}\\]\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.8690452\n0.4922555\n-22.080088\n0.0000000\n\n\nbalance\n0.0057365\n0.0002319\n24.737563\n0.0000000\n\n\nincome\n0.0000030\n0.0000082\n0.369815\n0.7115203\n\n\nstudentYes\n-0.6467758\n0.2362525\n-2.737646\n0.0061881\n\n\n\n\n\n\n\n\n\nWhy is the coefficient for student negative now when it was positive before?"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#lda-warmup",
    "href": "slides/04-2-lda-qda.html#lda-warmup",
    "title": "Chapter 4 Part 2",
    "section": "LDA Warmup",
    "text": "LDA Warmup\nTo give us a general overview, we are going to watch the StatQuest video on the topic: https://www.youtube.com/watch?v=azXCzI57Yfc"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#discriminant-analysis",
    "href": "slides/04-2-lda-qda.html#discriminant-analysis",
    "title": "Chapter 4 Part 2",
    "section": "Discriminant Analysis",
    "text": "Discriminant Analysis\n\nHere the approach is to model the distribution of X in each of the classes separately, and then use Bayes theorem to flip things around and obtain \\(P(Y|X)\\).\nWhen we use normal (Gaussian) distributions for each class, this leads to linear or quadratic discriminant analysis.\nHowever, this approach is quite general, and other distributions can be used as well. We will focus on normal distributions."
  },
  {
    "objectID": "slides/04-2-lda-qda.html#why",
    "href": "slides/04-2-lda-qda.html#why",
    "title": "Chapter 4 Part 2",
    "section": "Why?",
    "text": "Why?\n\nWhen the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.\nIf n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\nLinear discriminant analysis is popular when we have more than two response classes, because it also provides low-dimensional views of the data."
  },
  {
    "objectID": "slides/04-2-lda-qda.html#bayes-theorem-classification",
    "href": "slides/04-2-lda-qda.html#bayes-theorem-classification",
    "title": "Chapter 4 Part 2",
    "section": "Bayes Theorem (classification)",
    "text": "Bayes Theorem (classification)\nThomas Bayes was a famous mathematician whose name represents a big subfield of statistical and probabilistic modeling. Here we focus on a simple result, known as Bayes theorem:\n\\[P(Y=k|X=x) = \\frac{P(X=x|Y=k)\\cdot P(Y=k)}{P(X=x)}\\]"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#bayes-for-discriminant-analysis",
    "href": "slides/04-2-lda-qda.html#bayes-for-discriminant-analysis",
    "title": "Chapter 4 Part 2",
    "section": "Bayes for Discriminant Analysis",
    "text": "Bayes for Discriminant Analysis\n\\[P(Y=k|X=x) = \\frac{\\pi_kf_k(x)}{\\sum_{l=1}^K\\pi_lf_l(x)} \\text{, where}\\]\n\n\\(f_k(x)=P(X=x|Y=k)\\) is the density for \\(X\\) in class \\(k\\). Here we use normal’s but they could be other distributions (such as \\(\\chi^2\\))\n\\(\\pi_k = P(Y=k)\\) is the marginal or prior probability for class \\(k\\)."
  },
  {
    "objectID": "slides/04-2-lda-qda.html#classify-to-the-highest-density",
    "href": "slides/04-2-lda-qda.html#classify-to-the-highest-density",
    "title": "Chapter 4 Part 2",
    "section": "Classify to the highest density",
    "text": "Classify to the highest density\n\\[\\pi_1=.5, \\pi_2=.5\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe classify a new point according to which density is highest.\nWhen the priors are different, we take them into account as well, and compare \\(\\pi_kf_k(x)\\).\nOn the right, we favor the pink class - the decision boundary has shifted to the left."
  },
  {
    "objectID": "slides/04-2-lda-qda.html#linear-descriminant-analysis-when-p1",
    "href": "slides/04-2-lda-qda.html#linear-descriminant-analysis-when-p1",
    "title": "Chapter 4 Part 2",
    "section": "Linear Descriminant Analysis (when \\(p=1\\))",
    "text": "Linear Descriminant Analysis (when \\(p=1\\))\nThe Gaussian (normal) density has the form\n\\[f_k(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k}e^{-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma_k})^2}\\]\n\n\\(\\mu_k\\) is the mean, \\(\\sigma_k^2\\) the variance (in class \\(k\\))\nFor now, we assume \\(\\sigma_k=\\sigma\\) for all groups (we will need to check this with real data)"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#linear-descriminant-analysis-when-p1-1",
    "href": "slides/04-2-lda-qda.html#linear-descriminant-analysis-when-p1-1",
    "title": "Chapter 4 Part 2",
    "section": "Linear Descriminant Analysis (when \\(p=1\\))",
    "text": "Linear Descriminant Analysis (when \\(p=1\\))\nWe plug this \\(f_k(x)\\) into Bayes formula and after some simplifying we get:\n\\[p_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma_k})^2}}{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma_k})^2}}\\]"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#discriminant-function-when-p1",
    "href": "slides/04-2-lda-qda.html#discriminant-function-when-p1",
    "title": "Chapter 4 Part 2",
    "section": "Discriminant Function (when \\(p=1\\))",
    "text": "Discriminant Function (when \\(p=1\\))\nTo classify at the value X = x, we need to see which of the \\(p_k(x)\\) is largest. Taking logs, and discarding terms that do not depend on \\(k\\), we see that this is equivalent to assigning x to the class with the largest discriminant score:\n\\[\\delta_k(x) = x\\cdot \\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+log(\\pi_k)\\]\n\nImportantly, \\(\\delta_k(x)\\) is a linear function of \\(x\\).\nIf there are \\(K=2\\) classes and \\(\\pi_1=\\pi_2=.5\\), then the decision boundry is at\n\n\\[x=\\frac{\\mu_1+\\mu_2}{2}\\]"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#maximizing-delta_kx",
    "href": "slides/04-2-lda-qda.html#maximizing-delta_kx",
    "title": "Chapter 4 Part 2",
    "section": "Maximizing \\(\\delta_k(x)\\)",
    "text": "Maximizing \\(\\delta_k(x)\\)\n\nIn order to maximize this, we need estimates for all the parameters\n\n\nWhat should we estimate \\(\\hat{\\pi_k}\\), \\(\\mu_k\\), and \\(\\sigma^2\\) with?"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#maximizing-delta_kx-1",
    "href": "slides/04-2-lda-qda.html#maximizing-delta_kx-1",
    "title": "Chapter 4 Part 2",
    "section": "Maximizing \\(\\delta_k(x)\\)",
    "text": "Maximizing \\(\\delta_k(x)\\)\n\\[\\hat{\\pi}_k = \\frac{n_k}{n}\\]\n\\[\\hat{\\mu}_k = \\frac{1}{n_k}\\sum_{i:y_k=k}x_i\\]\n\\[\\hat{\\sigma}^2 = \\frac{1}{n-K}\\sum_{k=1}^K\\sum_{i:y_i=k}(x_i-\\hat{\\mu}_k)^2 = \\sum_{k=1}^K\\frac{n_k-1}{n-K}\\cdot \\hat{\\sigma}_k^2\\]\nWhere \\[\\hat{\\sigma}_k^2 = \\frac{1}{n_k-1}\\sum_{i:y_i=k}(x_i-\\hat{\\mu}_k)^2\\]"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#lda-in-r-example-p1",
    "href": "slides/04-2-lda-qda.html#lda-in-r-example-p1",
    "title": "Chapter 4 Part 2",
    "section": "LDA In R Example (\\(p=1\\))",
    "text": "LDA In R Example (\\(p=1\\))\n\ndata(\"BlueJays\") # Bring data into environment\nlibrary(Stat2Data)\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(discrim)\n\n\nCan we determine the sex of a blue jay by measuring the distance from the tip of the bill to the back of the head (Head)?"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#lda-bluejays-eda",
    "href": "slides/04-2-lda-qda.html#lda-bluejays-eda",
    "title": "Chapter 4 Part 2",
    "section": "LDA Bluejays EDA",
    "text": "LDA Bluejays EDA\n\ntt_split &lt;- initial_split(BlueJays,prop=.7)\n\nBJ_train &lt;- training(tt_split)\n\nBJ_train |&gt; ggplot(aes(x = Head,y=KnownSex)) +\n  geom_boxplot() +\n  theme_bw()"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#lda-bluejays-eda-1",
    "href": "slides/04-2-lda-qda.html#lda-bluejays-eda-1",
    "title": "Chapter 4 Part 2",
    "section": "LDA Bluejays EDA",
    "text": "LDA Bluejays EDA\n\nWhat assumptions should we check?\n\n\nNormality of our predictor\nConstant variance between groups."
  },
  {
    "objectID": "slides/04-2-lda-qda.html#lda-bluejays-eda---normality",
    "href": "slides/04-2-lda-qda.html#lda-bluejays-eda---normality",
    "title": "Chapter 4 Part 2",
    "section": "LDA Bluejays EDA - Normality",
    "text": "LDA Bluejays EDA - Normality\n\nlibrary(patchwork)\n\n(BJ_train |&gt; ggplot(aes(sample = Head)) + geom_qq())+\n  (BJ_train |&gt; ggplot(aes(x = Head)) + geom_histogram())"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#lda-bluejays-eda---variance",
    "href": "slides/04-2-lda-qda.html#lda-bluejays-eda---variance",
    "title": "Chapter 4 Part 2",
    "section": "LDA Bluejays EDA - Variance",
    "text": "LDA Bluejays EDA - Variance\n\nBJ_train |&gt; group_by(KnownSex)|&gt;\n  summarize(Head_sd = sd(Head))\n\n# A tibble: 2 × 2\n  KnownSex Head_sd\n  &lt;fct&gt;      &lt;dbl&gt;\n1 F           1.14\n2 M           1.27\n\n\n\nVery similar standard deviations (and thus variances)"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#application-exercise",
    "href": "slides/04-2-lda-qda.html#application-exercise",
    "title": "Chapter 4 Part 2",
    "section": " Application Exercise",
    "text": "Application Exercise\nWe are going to use the penguins data from the palmerpeguins package.\n\nConduct basic EDA to see if penguin bill length is different by sex and if LDA is an appropriate model choice.\nUse LDA to predict penguin sex based on their bill length using training and testing data. Get the accuracy on the testing set.\nFit a logistic regression model and get it’s accuracy to see which did better."
  },
  {
    "objectID": "slides/04-2-lda-qda.html#fit-lda",
    "href": "slides/04-2-lda-qda.html#fit-lda",
    "title": "Chapter 4 Part 2",
    "section": "Fit LDA",
    "text": "Fit LDA\n\nlda_spec &lt;- discrim_linear() |&gt;\n  set_mode(\"classification\")|&gt;\n  set_engine(\"MASS\")\n\nlda_fit&lt;-  lda_spec |&gt; \n  fit(KnownSex ~ Head,\n      data = BJ_train)"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#check-the-fit",
    "href": "slides/04-2-lda-qda.html#check-the-fit",
    "title": "Chapter 4 Part 2",
    "section": "Check The Fit",
    "text": "Check The Fit\n\nBJTest &lt;- testing(tt_split)\n\nlda_fit |&gt; \n  augment(new_data = BJTest) %&gt;%\n  conf_mat(truth = KnownSex, estimate = .pred_class) \n\n          Truth\nPrediction  F  M\n         F 12  0\n         M 10 15\n\n\n\nlda_fit|&gt; \n  augment(new_data = BJTest) |&gt; \n  accuracy(truth = KnownSex,estimate=.pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.730"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#compare-to-logistic",
    "href": "slides/04-2-lda-qda.html#compare-to-logistic",
    "title": "Chapter 4 Part 2",
    "section": "Compare to Logistic",
    "text": "Compare to Logistic\n\nlogistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(KnownSex ~ Head,\n      data = BJ_train) |&gt;\n  augment(new_data = BJTest)|&gt;\n    accuracy(truth = KnownSex,estimate=.pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.730"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#lda-with-p1",
    "href": "slides/04-2-lda-qda.html#lda-with-p1",
    "title": "Chapter 4 Part 2",
    "section": "LDA with \\(p>1\\)",
    "text": "LDA with \\(p&gt;1\\)\n\nWhen we have 2 or more predictors, the distribution becomes multivariate.\nIf the covariance between predictors is 0 within each class of the response, LDA is still appropriate."
  },
  {
    "objectID": "slides/04-2-lda-qda.html#qda",
    "href": "slides/04-2-lda-qda.html#qda",
    "title": "Chapter 4 Part 2",
    "section": "QDA",
    "text": "QDA\n\nQDA arises when \\(p&gt;1\\) and the is a covariance structure between the predictors within the same level of the response.\nThis introduces a squared term into the maximization problem of the \\(\\delta_k(x)\\), thus the name."
  },
  {
    "objectID": "slides/04-2-lda-qda.html#probabilities",
    "href": "slides/04-2-lda-qda.html#probabilities",
    "title": "Chapter 4 Part 2",
    "section": "Probabilities",
    "text": "Probabilities\n\nOnce the estimates for the \\(\\hat{\\delta}_k(x)\\) have been found, we can plug them in and get:\n\\[\\hat{P}(Y=k|X=x)=\\frac{e^{\\hat{\\delta}_k(x)}}{\\sum_{l=1}^Ke^{\\hat{\\delta}^l(x)}}\\]\nClassifying to the largest \\(\\hat{\\delta}_k(x)\\) amounts to classifying to the class for which \\(\\hat{P}(Y = k|X = x)\\) is largest.\nWhen \\(K = 2\\), we classify to class 2 if \\(\\hat{P}(Y = 2|X = x)\\geq 0.5\\), else to class 1."
  },
  {
    "objectID": "slides/04-2-lda-qda.html#example---credit-card-fraud",
    "href": "slides/04-2-lda-qda.html#example---credit-card-fraud",
    "title": "Chapter 4 Part 2",
    "section": "Example - Credit Card Fraud",
    "text": "Example - Credit Card Fraud\n\n\n          Truth\nPrediction   No  Yes\n       No  2889   83\n       Yes    2   26\n\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.972\n\n\n\nAccuracy of 97.7% on a testing set!\nWhat about the different types of errors?"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#errors",
    "href": "slides/04-2-lda-qda.html#errors",
    "title": "Chapter 4 Part 2",
    "section": "Errors",
    "text": "Errors\n\nOf the true yes, we made errors at the rate of 83/(83+26), 76%\nOf the true no, we made errors at the rate of 2/(2889+2), .069%"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#types-of-errors",
    "href": "slides/04-2-lda-qda.html#types-of-errors",
    "title": "Chapter 4 Part 2",
    "section": "Types of Errors",
    "text": "Types of Errors\n\nRecall:\n\nFalse positive rate: The fraction of negative examples that are classified as positive.\nFalse negative rate: The fraction of positive examples that are classified as negative.\n\nRemember the model gave probabilities, the final yes or no is decided by\n\\[\\hat{P}(Default=Yes|Balance,Student) \\geq .5\\]\n\nWe can adjust our error rates by changing that threshold"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#varying-the-threshold",
    "href": "slides/04-2-lda-qda.html#varying-the-threshold",
    "title": "Chapter 4 Part 2",
    "section": "Varying the threshold",
    "text": "Varying the threshold\n\nIn order to determine the best threshold, we want to maximize the specificty and sensitivity.\nOR - maximize the sensitivty and minimize 1-specificity.\nIn order to to do this we use an \\(ROC\\) curve which stands for receiver operating characteristic curve."
  },
  {
    "objectID": "slides/04-2-lda-qda.html#sensitivity-and-specificity",
    "href": "slides/04-2-lda-qda.html#sensitivity-and-specificity",
    "title": "Chapter 4 Part 2",
    "section": "Sensitivity and Specificity",
    "text": "Sensitivity and Specificity\n\n\n          Truth\nPrediction   No  Yes\n       No  2889   83\n       Yes    2   26\n\n\n\nThe sensitivity is the true positive rate.\n\nThe rate at which we correctly predict a person will default.\n26/(83+26) = .24 (24%)\n\nThe specificity is the true negative rate.\n\nThe rate at which we correctly predict a person will not default.\n2889/(2889+2) =.999 (99.9%)"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#roc-curve",
    "href": "slides/04-2-lda-qda.html#roc-curve",
    "title": "Chapter 4 Part 2",
    "section": "ROC Curve",
    "text": "ROC Curve\nWe want to maximize the area under this curve, called ROC AUC\n\nlda_roc&lt;- lda_fit_2 |&gt;\n  augment(new_data = testing(def_splits)) |&gt;\n  roc_curve(truth = default,.pred_No) \n\nhead(lda_roc)\n\n# A tibble: 6 × 3\n  .threshold specificity sensitivity\n       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1  -Inf          0                 1\n2     0.0819     0                 1\n3     0.129      0.00917           1\n4     0.151      0.0183            1\n5     0.154      0.0275            1\n6     0.201      0.0367            1"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#roc-auc",
    "href": "slides/04-2-lda-qda.html#roc-auc",
    "title": "Chapter 4 Part 2",
    "section": "ROC AUC",
    "text": "ROC AUC\n\nlda_fit_2 |&gt;\n  augment(new_data = testing(def_splits)) |&gt;\n  roc_auc(truth = default,.pred_No) \n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.957\n\n\nIf we want to tune our model better, we can optimize the ROC AUC by changing the threshold.\n\nI did a train/test approach here. What should I do different if I want to tune for threshold?"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#qda-in-r",
    "href": "slides/04-2-lda-qda.html#qda-in-r",
    "title": "Chapter 4 Part 2",
    "section": "QDA In R",
    "text": "QDA In R\n\nqda_fit&lt;-discrim_quad() |&gt;\n  set_mode(\"classification\")|&gt;\n  set_engine(\"MASS\")|&gt; \n  fit(default ~ balance + student,\n      data = training(def_splits))\n\nqda_fit |&gt;\n  augment(new_data = testing(def_splits)) %&gt;%\n  conf_mat(truth = default, estimate = .pred_class) \n\n          Truth\nPrediction   No  Yes\n       No  2888   80\n       Yes    3   29\n\nqda_fit |&gt;\n  augment(new_data = testing(def_splits)) %&gt;%\n  roc_auc(truth = default,.pred_No) \n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.957"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#tuning-by-threhold",
    "href": "slides/04-2-lda-qda.html#tuning-by-threhold",
    "title": "Chapter 4 Part 2",
    "section": "Tuning By Threhold",
    "text": "Tuning By Threhold\nhttps://www.tidymodels.org/start/case-study/\n\n\n\n\n🔗 https://sta362-sb8-24.github.io/STA362StatLearning/"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#roc-curve-1",
    "href": "slides/04-2-lda-qda.html#roc-curve-1",
    "title": "Chapter 4 Part 2",
    "section": "ROC Curve",
    "text": "ROC Curve\n\nautoplot(lda_roc)"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#why-another-approach",
    "href": "slides/04-2-lda-qda.html#why-another-approach",
    "title": "Chapter 4 Part 2",
    "section": "Why Another Approach?",
    "text": "Why Another Approach?\n\nWhen the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.\nIf n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\nLinear discriminant analysis is popular when we have more than two response classes, because it also provides low-dimensional views of the data."
  },
  {
    "objectID": "slides/04-2-lda-qda.html#lda-when-p1",
    "href": "slides/04-2-lda-qda.html#lda-when-p1",
    "title": "Chapter 4 Part 2",
    "section": "LDA (when \\(p=1\\))",
    "text": "LDA (when \\(p=1\\))\nThe Gaussian (normal) density has the form\n\\[f_k(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k}e^{-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma_k})^2}\\]\n\n\\(\\mu_k\\) is the mean, \\(\\sigma_k^2\\) the variance (in class \\(k\\))\nFor now, we assume \\(\\sigma_k=\\sigma\\) for all groups (we will need to check this with real data)"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#lda-when-p1-1",
    "href": "slides/04-2-lda-qda.html#lda-when-p1-1",
    "title": "Chapter 4 Part 2",
    "section": "LDA (when \\(p=1\\))",
    "text": "LDA (when \\(p=1\\))\nWe plug this \\(f_k(x)\\) into Bayes formula and after some simplifying we get:\n\\[p_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma_k})^2}}{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x-\\mu_k}{\\sigma_k})^2}}\\]"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#discriminant-function",
    "href": "slides/04-2-lda-qda.html#discriminant-function",
    "title": "Chapter 4 Part 2",
    "section": "Discriminant Function",
    "text": "Discriminant Function\nTo classify at the value X = x, we need to see which of the \\(p_k(x)\\) is largest. Taking logs, and discarding terms that do not depend on \\(k\\), we see that this is equivalent to assigning x to the class with the largest discriminant score:\n\\[\\delta_k(x) = x\\cdot \\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+log(\\pi_k)\\]\n\nImportantly, \\(\\delta_k(x)\\) is a linear function of \\(x\\).\nIf there are \\(K=2\\) classes and \\(\\pi_1=\\pi_2=.5\\), then the decision boundry is at\n\n\\[x=\\frac{\\mu_1+\\mu_2}{2}\\]"
  },
  {
    "objectID": "labs/03-lda_qda.html",
    "href": "labs/03-lda_qda.html",
    "title": "Lab 03",
    "section": "",
    "text": "Go to our RStudio and create a new R project inside your class folder.\n\n\nCreate a .qmd file for your lab, make sure the author is your name, and Render the document."
  },
  {
    "objectID": "labs/03-lda_qda.html#yaml",
    "href": "labs/03-lda_qda.html#yaml",
    "title": "Lab 03",
    "section": "",
    "text": "Create a .qmd file for your lab, make sure the author is your name, and Render the document."
  },
  {
    "objectID": "labs/03-lda_qda.html#conceptual-questions",
    "href": "labs/03-lda_qda.html#conceptual-questions",
    "title": "Lab 03",
    "section": "Conceptual questions",
    "text": "Conceptual questions\n\nSuppose that an individual has a 23% chance of defaulting on their credit card payment. What are the odds that they will default?"
  },
  {
    "objectID": "labs/03-lda_qda.html#logistic-regression",
    "href": "labs/03-lda_qda.html#logistic-regression",
    "title": "Lab 03",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nFor this lab we are using the Smarket data. Examine this data set - how many observations are there? How many columns? What are the variables?\nLet’s look at the correlation between all of the variables. Add the code below to your .qmd file. What can you learn from this visualization? Which pair of variables have the highest correlation?\n\n\nggpairs(Smarket, \n        lower = list(combo = wrap(ggally_facethist, binwidth = 0.5)), \n        progress = FALSE)\n\n\nInference Fit a logistic regression model to predict Direction using Lag1, Lag2, Lag3, Lag4, Lag5, and Volume. Show a table that contains the coefficients and p-values along with the confidence intervals for each of the 6 predictors. Which predictor has the smallest p-value? Interpret the coefficient, confidence interval, and p-value for this predictor.\nInference Exponentiate the results from Exercise 4. Interpret the odds ratio for the same predictor you selected in Exercise 4.\nPrediction Using 5-fold cross validation, fit the same logistic regression model as Exercise 4. What is the test Accuracy for this model? Interpret this result.\nInference Fit a logistic regression model to predict Direction using only Lag1 and Lag2. Show a table that contains the coefficients and p-values along with the confidence intervals for each of the 2 predictors. Which predictor has the smallest p-value? Interpret the coefficient, confidence interval, and p-value for this predictor.\nInference Exponentiate the results from Exercise 7. Interpret the odds ratio for the same predictor you selected in Exercise 7.\nPrediction Using 5-fold cross validation, fit the same logistic regression model as Exercise 7. What is the test Accuracy for this model? Interpret this result.\nIf you had to choose between the model fit in Exercise 4 and the one fit in Exercise 7, which would you choose? Why?"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#lda-with-p1-1",
    "href": "slides/04-2-lda-qda.html#lda-with-p1-1",
    "title": "Chapter 4 Part 2",
    "section": "LDA with \\(p>1\\)",
    "text": "LDA with \\(p&gt;1\\)\n\nFor example, with 2 normal predictors, their distributions in 3d would like like this:\n\n\n\nLuckily, the discriminate function remains linear\n\n\\[\\delta_k(x) = c_{k0} + c_{k1}x_1+...c_{kp}x_p\\]"
  },
  {
    "objectID": "slides/04-2-lda-qda.html#example-p2k3",
    "href": "slides/04-2-lda-qda.html#example-p2k3",
    "title": "Chapter 4 Part 2",
    "section": "Example: \\(p=2,K=3\\)",
    "text": "Example: \\(p=2,K=3\\)\n\nThere is no limit on the number of levels of the categorical response for LDA\nSuppose \\(\\pi_1=\\pi_2\\pi_3=1/3\\)\n\n - The dashed lines are known as the Bayes decision boundaries"
  }
]